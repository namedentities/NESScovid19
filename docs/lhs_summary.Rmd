---
title: "The Machine Learning for Social Science Global Subnational Covid Counts Datset"
output:
  html_notebook:
    toc: yes
    toc_float: true
    code_folding: hide
date: 
author: 
affiliation: Director, Machine Learning for Social Science Lab, Center for Peace and Security Studies, University of California San Diego
editor_options: 
  chunk_output_type: inline
---

Rex W. Douglass

<style type="text/css">
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
</style>

# Introduction

## Library Loads

```{r,warning=FALSE,message=FALSE,error=FALSE, results='hide'}
#libraries
library(lubridate)
library(tidyverse)

#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(scales)
library(gghighlight)
library(lubridate)
library(R0)  # consider moving all library commands to top -- this one was in a loop below

library(WikidataR)
library(countrycode)

library(usmap) ; # install.packages('usmap')
data(statepop)
#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(sf)

library(jsonlite)

#This is too slow it's downloading each
library(GADMTools)
library(strucchange) ; #install.packages('strucchange')
library(tsibble)

```
## Data Loading and Cleaning

```{r, fig.width=10, fig.height=8,warning=FALSE,message=FALSE,error=FALSE, results='hide'}

lhs_long <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long.Rds")
#dim(lhs_long) #187,305

lhs_long_clean <- lhs_long %>%
  filter(!is.na(date_asdate)) %>%
  
  #First drop anything that's negative. Shouldn't be any negatives.
  filter(is.na(confirmed) | confirmed>=0) %>%
  filter(is.na(deaths) | deaths>=0) %>%
  filter(is.na(tested_people) | tested_people>=0) %>%
  filter(is.na(tested_samples) | tested_samples>=0) %>%
  
  #Then set 0s to NA, we don't trust NAs
  mutate(confirmed=ifelse(confirmed==0, NA, confirmed)) %>%
  mutate(deaths=ifelse(deaths==0, NA, deaths)) %>%
  mutate(tested_people=ifelse(tested_people==0, NA, tested_people)) %>%
  mutate(tested_samples=ifelse(tested_samples==0, NA, tested_samples)) %>%

  #Then check first differences and drop anything that doesn't weakly increase monotonically
  group_by(dataset,gid ,  geonameid ,wikidata_id) %>% 
    arrange(date_asdate) %>%
    mutate(confirmed_fd=confirmed, deaths_fd=deaths, tested_people_fd=tested_people, tested_samples_fd=tested_samples) %>%
    tidyr::fill( ends_with("_fd")) %>%
    mutate_at(vars(ends_with("_fd")), difference) %>%
  ungroup() %>%
  #About 4k of these observations show a decrease from one time step to the next which suggests either an original error or a joining error
  #We're going to straight drop those observations as a cleaning step
  filter( (is.na(confirmed_fd) | confirmed_fd>=0) &  #never allow for a reversal
          (is.na(deaths_fd) | deaths_fd>=0) &        #never allow for a reversal
          (is.na(tested_people_fd) | tested_people_fd>=0) &  #never allow for a reversal
          (is.na(tested_samples_fd) | tested_samples_fd>=0)  #never allow for a reversal
          ) %>%
  
  
  #we're going to go one step further and require either confirmed or tested to be strictly higher
  #In words, during an episode we don't believe reports mean "no new cases" just no new good reporting
  #metabiota is the worst offender here so restricting it to just that
  filter( !(dataset=="metabiota" & (is.na(confirmed_fd) | confirmed_fd<=0 ))    ) %>% #metabiota is really problematic 90% of what we're doing is trying to account for it
  
  #Also reject any where deaths are greater than confirmed
  filter(is.na(confirmed) | is.na(deaths) | confirmed>=deaths) %>%
  
  #also reject any where confirmed doesn't vary 
  group_by(gid ,  geonameid ,wikidata_id) %>% #chose not to do by dataset because testing datasets might not have confirmed
    filter(var(confirmed, na.rm=T)>0) %>%
  ungroup()
  
#dim(lhs_long_clean) #281,464 #292,580 #299213 #181,563

saveRDS(lhs_long_clean, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long_cleand.Rds")
```

Summary Statistics of Our Data

```{r}
print("Number of observations")
dim(lhs_long_clean)

print("Number of locations")
lhs_long_clean$wikidata_id %>% unique() %>% length() #3,373

print("Number of Days")
lhs_long_clean$date_asdate %>% unique() %>% length() #113

library(DT) #https://rstudio.github.io/DT/
lhs_long_clean %>% count(dataset) %>% arrange(-n) %>% DT::datatable(options = list(pageLength = 20, autoWidth = TRUE))

#library(gt)
#lhs_long_clean %>% count(dataset) %>% arrange(-n) %>% gt() %>% 
# tab_header(
#    title = md("Location-Days by Dataset")#,
#    #subtitle = "Number of Days with Data by Dataset"
#  ) %>%
#  fmt_missing( columns=everything(), rows=NULL,missing_text = 0)

```


# Spatial Variation in Data Availability

## Country Level Data Availability

```{r, fig.width=12, fig.height=12,warning=FALSE,message=FALSE,error=FALSE, results='hide'}

#gadm36 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESSgadm/data_in/gadm36_gpkg/gadm36.gpkg")
#st_layers("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg")
gadm36_levels_0 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg", layer="level0")  %>%
                  st_simplify(preserveTopology = FALSE, dTolerance =0.1) #  0.025 this is supposedly broken up by 6 levels and so should have u.s. 

#plot(gadm36_levels_0)
#dim(gadm36_levels_0_sf$sf)
#gadm_plot(gadm36_levels_0_sf)
lhs_long_place_sources <- lhs_long  %>% dplyr::select(gid,   geonameid, wikidata_id, dataset) %>% 
                           group_by(gid,  geonameid, wikidata_id) %>% count(dataset) %>%
                           group_by(gid,  geonameid, wikidata_id) %>%
                           summarise(datasets_n=n()) 

p0 <- gadm36_levels_0 %>% 
      left_join(lhs_long_place_sources %>% dplyr::select(gid=gid, datasets_n) %>%  left_join( gadm36_levels_0 %>% as.data.frame() %>% dplyr::select(gid=GID_0, NAME_0)  %>% distinct()  )
                ) %>%
      #replace_na(list(datasets_n = 0)) %>% 
      ggplot() + geom_sf(aes(fill = datasets_n)) +
      scale_fill_gradient(low="blue", high="red") +
      theme_bw() 
```



```{r, fig.width=12, fig.height=6,warning=FALSE,message=FALSE,error=FALSE}
p0 + ggtitle("Covid Count Data Availability at the Country Level")
```

```{r}
temp <- gadm36_levels_0 %>%
   left_join(
   lhs_long  %>% count(gid, dataset) %>% rename(GID_0=gid) #there are still dupe geonames wikidata to gid matches that need to be fixed geonameid, wikidata_id, 
) 

temp_wide <- temp %>% as.data.frame() %>% dplyr::select(Country=NAME_0, dataset, n) %>% distinct() %>% pivot_wider( id_cols = Country, names_from = dataset,
  values_from = n, values_fill = NULL, values_fn = NULL) 

temp_wide %>% DT::datatable(options = list(pageLength = 20, autoWidth = TRUE))

#install.packages('gt')
#library(gt)
#temp_wide %>% gt() %>% 
# tab_header(
#    title = md("Data Availability by Country"),
#    subtitle = "Number of Days with Data by Dataset"
#  ) %>%
#  fmt_missing( columns=everything(), rows=NULL,missing_text = 0)

```

## State/Province Level Data Availability


```{r, fig.width=12, fig.height=12,warning=FALSE,message=FALSE,error=FALSE, results='hide'}
gadm36_levels_1 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg", layer="level1")  %>%
  st_simplify(preserveTopology = FALSE, dTolerance =0.1) #  0.025 this is supposedly broken up by 6 levels and so should have u.s. 


p1 <- gadm36_levels_1 %>% 
  left_join(lhs_long_place_sources %>% dplyr::select(gid=gid, datasets_n) %>%  left_join( gadm36_levels_1 %>% as.data.frame() %>% dplyr::select(gid=GID_1, NAME_1) %>% distinct()  )
  ) %>%
  #replace_na(list(datasets_n = 0)) %>% 
  ggplot() + geom_sf(aes(fill = datasets_n)) +
  scale_fill_gradient(low="blue", high="red") +
  theme_bw() 
```

```{r, fig.width=12, fig.height=6,warning=FALSE,message=FALSE,error=FALSE}
p1
```

```{r}
temp <- gadm36_levels_1 %>%
   left_join(
   lhs_long  %>% count(gid, dataset) %>% rename(GID_1=gid) #there are still dupe geonames wikidata to gid matches that need to be fixed geonameid, wikidata_id, 
) 

temp_wide <- temp %>% as.data.frame() %>% dplyr::select(Country=NAME_0, Admin1=NAME_1, dataset, n) %>% distinct() %>% pivot_wider( id_cols = Country:Admin1, names_from = dataset,
  values_from = n, values_fill = NULL, values_fn = NULL) %>% mutate(total = rowSums(.[-c(1,2)], na.rm = T))

temp_wide %>% filter(total>0) %>% DT::datatable(options = list(pageLength = 10, autoWidth = TRUE))

```

## County District Level Data Availability

```{r, fig.width=12, fig.height=12,warning=FALSE,message=FALSE,error=FALSE, results='hide'}
gadm36_levels_2 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg", layer="level2")  %>%
  st_simplify(preserveTopology = FALSE, dTolerance = 0.001) #  0.025 this is supposedly broken up by 6 levels and so should have u.s. I have to keep shrinking it so misisng goes to zero 

p2 <- gadm36_levels_2 %>% 
  left_join(lhs_long_place_sources %>% dplyr::select(gid=gid, datasets_n) %>%  left_join( gadm36_levels_2 %>% as.data.frame() %>% dplyr::select(gid=GID_2, NAME_2) %>% distinct()  )
  ) %>%
  #replace_na(list(datasets_n = 0)) %>% 
  ggplot() + geom_sf(aes(fill = datasets_n),lwd = 0) +
  scale_fill_gradient(low="blue", high="red") +
  theme_bw() 
```

```{r, fig.width=12, fig.height=6,warning=FALSE,message=FALSE,error=FALSE}
p2
```


```{r}
temp <- gadm36_levels_2 %>%
   left_join(
   lhs_long  %>% count(gid, dataset) %>% rename(GID_2=gid) #there are still dupe geonames wikidata to gid matches that need to be fixed geonameid, wikidata_id, 
) 

temp_wide <- temp %>% as.data.frame() %>% dplyr::select(Country=NAME_0, Admin1=NAME_1, Admin2=NAME_2, dataset, n) %>% distinct() %>% pivot_wider( id_cols = Country:Admin2, names_from = dataset,
  values_from = n, values_fill = NULL, values_fn = NULL) %>% mutate(total = rowSums(.[-c(1,2,3)], na.rm = T))

temp_wide %>% filter(total>0) %>% DT::datatable(options = list(pageLength = 10, autoWidth = TRUE))

```

# Data Availability and Interpolation over Time

```{r,warning=FALSE,message=FALSE,error=FALSE, results='hide'}

#test <- lhs_long %>% group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>% summarize(n=n())

all_na <- function(x) any(!is.na(x)) #https://intellipaat.com/community/12999/remove-columns-from-dataframe-where-all-values-are-na

#bing looks off by a day from the other ones
lhs_wide_qcode <- lhs_long %>% 
                  filter(!is.na(wikidata_id) & !is.na(date_asdate)) %>%
                  group_by(gid, geonameid, wikidata_id, date_asdate) %>%  mutate(confirmed_var=var(confirmed, na.rm=T), deaths_var=var(deaths, na.rm=T)) %>% ungroup() %>%
                  dplyr::select(dataset, gid, geonameid, wikidata_id, date_asdate,confirmed, deaths, tested_samples, tested_people) %>%
                  group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>%  summarise_if(is.numeric,max,na.rm=T) %>% ungroup() %>% #this is a hack, we should have dupes within datasets
                  pivot_wider(names_from = dataset, values_from = c(confirmed, deaths, tested_samples, tested_people) ) %>% 
                  mutate_if(is.numeric, list(~na_if(abs(.), Inf))) %>% #https://stackoverflow.com/questions/12188509/cleaning-inf-values-from-an-r-dataframe
                  select_if(all_na) 

dim(lhs_wide_qcode) #82,157 82k place/day observations

length(unique(lhs_wide_qcode$wikidata_id)) #3697 #almost 4k 

test <- lhs_wide_qcode %>% dplyr::select(starts_with("confirmed")) %>% distinct() 
#cor(test, use="pairwise.complete.obs")

lhs_long_median <- lhs_long %>% group_by(gid, geonameid, wikidata_id, date_asdate) %>% summarize_if(is.numeric, median, na.rm=T) %>% dplyr::select(-ends_with("_fd")) %>% #we take the median across observations
                   mutate(CFR=deaths/confirmed)
#dim(lhs_long_median)
#summary(lhs_long_median)
```


```{r, eval=F, echo=F,warning=FALSE,message=FALSE,error=FALSE}

#Here is where I worked out the idea of fitting piecewise linear functions to each dataset independently. It's now replaced with a fully automated pipline based on trees.

count_datasets <- lhs_long_clean %>% dplyr::select(wikidata_id, dataset) %>% distinct() %>% count(wikidata_id)

formula = confirmed_log ~ 1 + date_rank #+ I(date_rank^2) + I(date_rank^3)
h=5
#bexar county
library(strucchange) ; #install.packages('strucchange')
temp <- lhs_long_clean %>% arrange(date_asdate) %>% filter(wikidata_id %in% "Q16861") %>%  mutate(confirmed_log=log(confirmed+1)) %>%
        mutate(date_rank= as.numeric(date_asdate) - min(as.numeric(date_asdate)))  

table(temp$dataset)

temp_usafacts <- temp %>% filter(dataset=="usafacts")
bp_usafacts <- breakpoints(formula, data=temp_usafacts,h=h)
temp_usafacts$y_hat <- fitted.values(bp_usafacts)
temp_usafacts$groups <- 0; temp_usafacts$groups[bp_usafacts$breakpoints+1] <- 1 ; temp_usafacts$groups <- cumsum(temp_usafacts$groups)+1

temp_nyt <- temp %>% filter(dataset=="nyt")
bp_nyt <- breakpoints(formula, data=temp_nyt,h=h)
temp_nyt$y_hat <- fitted.values(bp_nyt)
temp_nyt$groups <- 0; temp_nyt$groups[bp_nyt$breakpoints+1] <- 1 ; temp_nyt$groups <- cumsum(temp_nyt$groups)+1


temp_CSSE <- temp %>% filter(dataset=="CSSE")
bp_CSSE <- breakpoints(formula, data=temp_CSSE,h=h)
temp_CSSE$y_hat <- fitted.values(bp_CSSE)
temp_CSSE$groups <- 0; temp_CSSE$groups[bp_CSSE$breakpoints+1] <- 1 ; temp_CSSE$groups <- cumsum(temp_CSSE$groups)+1

temp_bing <- temp %>% filter(dataset=="bing")
bp_bing <- breakpoints(formula, data=temp_bing,h=h) #this fails bc too few
lm_bing <-lm(formula, data=temp_bing)
temp_bing$y_hat <- fitted.values(lm_bing)
temp_bing$groups <-1

temp_df <- bind_rows(temp_nyt, temp_CSSE, temp_bing, temp_usafacts)  %>% 
            arrange(dataset,date_asdate) %>% 
           mutate(groups=paste(dataset,groups))

temp_df2 <- bind_rows(temp_nyt, temp_CSSE, temp_bing, temp_usafacts) %>% 
            arrange(date_asdate) %>% 
            group_by(date_asdate) %>%
              summarise(y_hat_mean=mean(y_hat, na.rm=T)) %>%
            mutate(groups=99)

ggplot() + 
   geom_point(data=temp_df, aes(x=date_asdate, y=confirmed_log, color=dataset)) +
   geom_line(data=temp_df, aes(x=date_asdate, y=y_hat, color=dataset, group=groups)) +
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("Bexar County (Q16861) Confirmed")



#####
#China Q148
temp <- lhs_long_clean %>% arrange(date_asdate) %>% 
        filter(wikidata_id %in% "Q148") %>% 
        group_by(dataset, date_asdate) %>%
          summarise(confirmed=max(confirmed, na.rm=T) ) %>% #this is a problem dupes with the same date  #this fixes a lot of things but we need to figure out the origin of this problem
        ungroup() %>%
        mutate(confirmed_log=log(confirmed+1)) %>% 
        mutate(date_rank= as.numeric(date_asdate) - min(as.numeric(date_asdate)))  


temp_wikipedia <- temp %>% filter(dataset=="wikipedia")
bp_wikipedia <- breakpoints(formula, data=temp_wikipedia,h=h)
temp_wikipedia$y_hat <- fitted.values(bp_wikipedia)
temp_wikipedia$groups <- 0; temp_wikipedia$groups[bp_wikipedia$breakpoints+1] <- 1 ; temp_wikipedia$groups <- cumsum(temp_wikipedia$groups)+1


temp_ecdc <- temp %>% filter(dataset=="ecdc")
bp_ecdc <- breakpoints(formula, data=temp_ecdc,h=h)
temp_ecdc$y_hat <- fitted.values(bp_ecdc)
temp_ecdc$groups <- 0; temp_ecdc$groups[bp_ecdc$breakpoints+1] <- 1 ; temp_ecdc$groups <- cumsum(temp_ecdc$groups)+1

temp_who <- temp %>% filter(dataset=="who")
bp_who <- breakpoints(formula, data=temp_who,h=h)
temp_who$y_hat <- fitted.values(bp_who)
temp_who$groups <- 0; temp_who$groups[bp_who$breakpoints+1] <- 1 ; temp_who$groups <- cumsum(temp_who$groups)+1


temp_metabiota <- temp %>% filter(dataset=="metabiota")
bp_metabiota <- breakpoints(formula, data=temp_metabiota,h=h)
temp_metabiota$y_hat <- fitted.values(bp_metabiota)
temp_metabiota$groups <- 0; temp_metabiota$groups[bp_metabiota$breakpoints+1] <- 1 ; temp_metabiota$groups <- cumsum(temp_metabiota$groups)+1

temp_bing <- temp %>% filter(dataset=="bing")
bp_bing <- breakpoints(formula, data=temp_bing,h=h) #this fails bc too few
lm_bing <-lm(formula, data=temp_bing)
temp_bing$y_hat <- fitted.values(lm_bing)
temp_bing$groups <-1

temp_df <- bind_rows(temp_ecdc, temp_metabiota, temp_bing, temp_who, temp_wikipedia) %>% 
            arrange(dataset,date_asdate) %>% 
           mutate(groups=paste(dataset,groups))

temp_df2 <- bind_rows(temp_ecdc, temp_metabiota, temp_bing, temp_who, temp_wikipedia) %>% 
            arrange(date_asdate) %>% 
            group_by(date_asdate) %>%
              summarise(y_hat_mean=mean(y_hat, na.rm=T)) %>%
            mutate(groups=99)

ggplot() + 
   geom_point(data=temp_df, aes(x=date_asdate, y=confirmed_log, color=dataset)) +
   geom_line(data=temp_df, aes(x=date_asdate, y=y_hat, color=dataset, group=groups)) +
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("China (Q148) Confirmed")
   #geom_vline(xintercept=bp$breakpoints)


#India
#"Q668"
temp <- lhs_long_clean %>% arrange(date_asdate) %>% 
        filter(wikidata_id %in% "Q668") %>% 
        group_by(dataset, date_asdate) %>%
          summarise(confirmed=max(confirmed, na.rm=T) ) %>% #this is a problem dupes with the same date  #this fixes a lot of things but we need to figure out the origin of this problem
        ungroup() %>%
        mutate(confirmed_log=log(confirmed+1)) %>% 
        mutate(date_rank= as.numeric(date_asdate) - min(as.numeric(date_asdate)))  
table(temp$dataset)

temp_covid19india <- temp %>% filter(dataset=="covid19india")
bp_covid19india <- breakpoints(formula, data=temp_covid19india,h=h)
temp_covid19india$y_hat <- fitted.values(bp_covid19india)
temp_covid19india$groups <- 0; temp_covid19india$groups[bp_covid19india$breakpoints+1] <- 1 ; temp_covid19india$groups <- cumsum(temp_covid19india$groups)+1

temp_wikipedia <- temp %>% filter(dataset=="wikipedia")
bp_wikipedia <- breakpoints(formula, data=temp_wikipedia,h=h)
temp_wikipedia$y_hat <- fitted.values(bp_wikipedia)
temp_wikipedia$groups <- 0; temp_wikipedia$groups[bp_wikipedia$breakpoints+1] <- 1 ; temp_wikipedia$groups <- cumsum(temp_wikipedia$groups)+1

temp_CSSE <- temp %>% filter(dataset=="CSSE")
bp_CSSE <- breakpoints(formula, data=temp_CSSE,h=h)
temp_CSSE$y_hat <- fitted.values(bp_CSSE)
temp_CSSE$groups <- 0; temp_CSSE$groups[bp_CSSE$breakpoints+1] <- 1 ; temp_CSSE$groups <- cumsum(temp_CSSE$groups)+1

temp_ecdc <- temp %>% filter(dataset=="ecdc")
bp_ecdc <- breakpoints(formula, data=temp_ecdc,h=h)
temp_ecdc$y_hat <- fitted.values(bp_ecdc)
temp_ecdc$groups <- 0; temp_ecdc$groups[bp_ecdc$breakpoints+1] <- 1 ; temp_ecdc$groups <- cumsum(temp_ecdc$groups)+1

temp_who <- temp %>% filter(dataset=="who")
bp_who <- breakpoints(formula, data=temp_who,h=h)
temp_who$y_hat <- fitted.values(bp_who)
temp_who$groups <- 0; temp_who$groups[bp_who$breakpoints+1] <- 1 ; temp_who$groups <- cumsum(temp_who$groups)+1

temp_metabiota <- temp %>% filter(dataset=="metabiota")
bp_metabiota <- breakpoints(formula, data=temp_metabiota,h=h)
temp_metabiota$y_hat <- fitted.values(bp_metabiota)
temp_metabiota$groups <- 0; temp_metabiota$groups[bp_metabiota$breakpoints+1] <- 1 ; temp_metabiota$groups <- cumsum(temp_metabiota$groups)+1

temp_bing <- temp %>% filter(dataset=="bing")
bp_bing <- breakpoints(formula, data=temp_bing,h=h) #this fails bc too few
lm_bing <-lm(formula, data=temp_bing)
temp_bing$y_hat <- fitted.values(lm_bing)
temp_bing$groups <-1

temp_df <- bind_rows(temp_covid19india, temp_wikipedia, temp_CSSE, temp_ecdc, temp_who, temp_metabiota, temp_bing) %>% 
            arrange(dataset,date_asdate) %>% 
           mutate(groups=paste(dataset,groups))

temp_df2 <- bind_rows(temp_covid19india, temp_wikipedia, temp_CSSE, temp_ecdc, temp_who, temp_metabiota, temp_bing) %>% 
            arrange(date_asdate) %>% 
            group_by(date_asdate) %>%
              summarise(y_hat_mean=mean(y_hat, na.rm=T)) %>%
            mutate(groups=99)

ggplot() + 
   geom_point(data=temp_df, aes(x=date_asdate, y=confirmed_log, color=dataset), alpha=.3) +
   geom_line(data=temp_df, aes(x=date_asdate, y=y_hat, color=dataset, group=groups)) +
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("India (Q668) Confirmed")


#Florida
#Q812
temp <- lhs_long_clean %>% arrange(date_asdate) %>% 
        filter(wikidata_id %in% "Q812") %>% 
        group_by(dataset, date_asdate) %>%
          summarise(confirmed=max(confirmed, na.rm=T) ) %>% #this is a problem dupes with the same date  #this fixes a lot of things but we need to figure out the origin of this problem
        ungroup() %>%
        mutate(confirmed_log=log(confirmed+1)) %>%
        mutate(date_rank= as.numeric(date_asdate) - min(as.numeric(date_asdate)))  
table(temp$dataset)

temp_covidtracking <- temp %>% filter(dataset=="covidtracking")
bp_covidtracking <- breakpoints(formula, data=temp_covidtracking,h=h)
temp_covidtracking$y_hat <- fitted.values(bp_covidtracking)
temp_covidtracking$groups <- 0; temp_covidtracking$groups[bp_covidtracking$breakpoints+1] <- 1 ; temp_covidtracking$groups <- cumsum(temp_covidtracking$groups)+1

temp_wikipedia <- temp %>% filter(dataset=="wikipedia")
bp_wikipedia <- breakpoints(formula, data=temp_wikipedia,h=h)
temp_wikipedia$y_hat <- fitted.values(bp_wikipedia)
temp_wikipedia$groups <- 0; temp_wikipedia$groups[bp_wikipedia$breakpoints+1] <- 1 ; temp_wikipedia$groups <- cumsum(temp_wikipedia$groups)+1

temp_metabiota <- temp %>% filter(dataset=="metabiota")
bp_metabiota <- breakpoints(formula, data=temp_metabiota,h=h)
temp_metabiota$y_hat <- fitted.values(bp_metabiota)
temp_metabiota$groups <- 0; temp_metabiota$groups[bp_metabiota$breakpoints+1] <- 1 ; temp_metabiota$groups <- cumsum(temp_metabiota$groups)+1

temp_bing <- temp %>% filter(dataset=="bing")
bp_bing <- breakpoints(formula, data=temp_bing,h=h) #this fails bc too few
lm_bing <-lm(formula, data=temp_bing)
temp_bing$y_hat <- fitted.values(lm_bing)
temp_bing$groups <-1

temp_df <- bind_rows(temp_covidtracking, temp_wikipedia, temp_metabiota, temp_bing) %>% 
            arrange(dataset,date_asdate) %>% 
           mutate(groups=paste(dataset,groups))

temp_df2 <- bind_rows(temp_covidtracking, temp_wikipedia, temp_metabiota, temp_bing) %>% 
            arrange(date_asdate) %>% 
            group_by(date_asdate) %>%
              summarise(y_hat_mean=mean(y_hat, na.rm=T)) %>%
            mutate(groups=99)

ggplot() + 
   geom_point(data=temp_df, aes(x=date_asdate, y=confirmed_log, color=dataset), alpha=.3) +
   geom_line(data=temp_df, aes(x=date_asdate, y=y_hat, color=dataset, group=groups)) +
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("Florida (Q812) Confirmed")

```



```{r, eval=F, echo=F,warning=FALSE,message=FALSE,error=FALSE}
#As was this part
# # Q668  Q16861  Q148  Q30 Q148 Q16861 Q30
temp <- temp_df %>% filter(wikidata_id=="Q148") %>% mutate(date_asnumeric=as.numeric(date_asdate))
temp %>% ggplot() + geom_point(aes(date_asdate,confirmed_log))  + facet_wrap(~dataset) #+ geom_point(aes(date_asdate,confirmed_log_y_hat), color="blue")

#library(rpart)
#rt <- rpart(confirmed_log ~ 1 + date_asdate , data=temp)
#temp$y_hat <- predict( rt, newdata=temp )

#temp$dataset <- as.factor(temp$dataset)
#fmBH <- mob(confirmed_log ~ 1 + t + I(t^2) + I(t^3) | t + dataset, control = mob_control(minsplit = 3), data = temp,  model = linearModel) #this fits a piecewise linear to each dataset #have to use t, doesn't like data objects
#plot(fmBH)
#coef(fmBH)
#temp$y_hat <- predict( fmBH, newdata=temp )

#The trick is fitting only to one dataset at a time
temp_list <- list()
for( d in temp$dataset %>% unique() ){
  temptemp <- temp %>% filter(dataset==d)
  fmBH <- mob(confirmed_log ~ 1 + date_asnumeric    | date_asnumeric , control = mob_control(minsplit = 3, alpha=.2), data = temptemp,  model = linearModel)  # + I(date_asnumeric^2)     + I(t^3)  # + I(t^2) + I(t^3) + I(t^4) + I(t^2) 
  temptemp$y_hat <- predict( fmBH, newdata=temptemp , type = c("response") )
  temptemp$group_number <-  predict( fmBH, newdata=temptemp , type = c("node"))
  temptemp <- temptemp %>% group_by(group_number) %>% arrange(date_asnumeric) %>% mutate(slope=difference(y_hat)) %>% fill(slope, .direction="up") %>% ungroup() %>%
                      mutate(percent_change = round((exp(slope)-1)*100,2)) 

  temp_list[[d]] <- temptemp
}

combined <- bind_rows(temp_list) 
p0 <- combined %>%
     mutate(group=paste(wikidata_id, dataset, group_number)) %>%
     ggplot() + geom_point(aes(date_asdate,confirmed_log, color=dataset), alpha=.5) + 
                                    geom_line(aes(date_asdate,y_hat, group=group), color="blue", alpha=1) + facet_wrap(~dataset) + theme_bw() #it shits the bed in that big gap in wikipedia

fmBH_slope <- mob(confirmed_log ~ 1   | date_asnumeric , control = mob_control(minsplit = 3, alpha=.1), data = combined,  model = linearModel)  #it doesn't want to do 
rt1 <- rpart(formula = percent_change ~ date_asnumeric, data=combined)
combined$percent_change_y_hat <-  predict(rt1, newdata=combined)
#combined$slope_y_hat <- predict( fmBH_slope, newdata=combined  , type = c("response") )
#combined$group_number <- predict( fmBH_slope, newdata=combined  , type = c("node") )

p1 <- combined %>%
     mutate(group=paste(wikidata_id, dataset, group_number)) %>%
     ggplot() + geom_point(aes(date_asdate,confirmed_log, color=dataset), alpha=.5) + 
                                    geom_line(aes(date_asdate,y_hat, group=group, color=dataset), alpha=.1) + theme_bw() + facet_wrap(~dataset)#  #it shits the bed in that big gap in wikipedia


p2 <- combined %>% ggplot() + geom_point(aes(date_asdate,percent_change_y_hat, color=dataset), alpha=.5) + 
             geom_line(aes(date_asdate,percent_change_y_hat), color="black", alpha=1) + theme_bw() + facet_wrap(~dataset) #+ facet_wrap(~dataset) #it shits the bed in that big gap in wikipedia

p1 / p2


```

Here we do the actual interpolation. For each individual location-dataset, we fit a piecewise linear model over time, and then use that to average over day to day noise and interpolate between observations. Because this is in log space, the slope of that line is the day on day percent increase in the count. We finish by fitting a piecewise intercept model to those slopes to get a single daily estimate of how much the count is increasing. Datasets are able to disagree in lots of different ways and we still recover a correct rate of change.

```{r,warning=FALSE,message=FALSE,error=FALSE}

places <- lhs_long_clean$wikidata_id %>% unique() %>% na.omit() ; length(places)
datasets <- lhs_long_clean$dataset %>% unique() %>% na.omit() ; table(datasets)

temp_list <- list()
for(p in places){
    print(p)
    for(d in datasets){
      try({
        temp <- NULL
        temp <- lhs_long_clean %>%
                filter(dataset %in% d) %>% 
                filter(wikidata_id %in% p) %>% 
                arrange(date_asdate) %>% 
                mutate(confirmed_log=log(confirmed+1)) %>%
                mutate(deaths_log=log(deaths+1)) %>%
                mutate(tested_people_log=log(tested_people+1)) 
        if(nrow(temp)==0){next()}
        
        temp <- temp %>% expand(dataset,gid, geonameid,wikidata_id,  date_asdate=min(date_asdate):max(date_asdate) %>% as.Date() ) %>% full_join(temp) %>% mutate(date_asnumeric= as.numeric(date_asdate) ) 
                
        tryCatch({  
          tree_confirmed <- mob(confirmed_log ~ 1 + date_asnumeric    | date_asnumeric , control = mob_control(minsplit = 3, alpha=.2), data = temp,  model = linearModel)  # + I(date_asnumeric^2)     + I(t^3)  # + I(t^2) + I(t^3) + I(t^4) + I(t^2) 
          temp$confirmed_log_y_hat <- predict( tree_confirmed, newdata=temp , type = c("response") )
          temp$confirmed_log_group_number <-  predict( tree_confirmed, newdata=temp , type = c("node"))
          temp <- temp %>% group_by(confirmed_log_group_number) %>% arrange(date_asnumeric) %>% mutate(confirmed_log_y_hat_slope=difference(confirmed_log_y_hat)) %>% fill(confirmed_log_y_hat_slope, .direction="up") %>% ungroup() %>%
                              mutate(confirmed_log_y_hat_percent_change = round((exp(confirmed_log_y_hat_slope)-1)*100,2)) 
        }, error = function(e) {})
        
        tryCatch({  
          tree_deaths <- mob(deaths_log ~ 1 + date_asnumeric    | date_asnumeric , control = mob_control(minsplit = 3, alpha=.2), data = temp,  model = linearModel)  # + I(date_asnumeric^2)     + I(t^3)  # + I(t^2) + I(t^3) + I(t^4) + I(t^2) 
          temp$deaths_log_y_hat <- predict( tree_deaths, newdata=temp , type = c("response") )
          temp$deaths_log_group_number <-  predict( tree_deaths, newdata=temp , type = c("node"))
          temp <- temp %>% group_by(deaths_log_group_number) %>% arrange(date_asnumeric) %>% mutate(deaths_log_y_hat_slope=difference(deaths_log_y_hat)) %>% fill(deaths_log_y_hat_slope, .direction="up") %>% ungroup() %>%
                              mutate(deaths_log_y_hat_percent_change = round((exp(deaths_log_y_hat_slope)-1)*100,2)) 
        }, error = function(e) {})
        
        tryCatch({  
          tree_tested_people <- mob(tested_people_log ~ 1 + date_asnumeric    | date_asnumeric , control = mob_control(minsplit = 3, alpha=.2), data = temp,  model = linearModel)  # + I(date_asnumeric^2)     + I(t^3)  # + I(t^2) + I(t^3) + I(t^4) + I(t^2) 
          temp$tested_people_log_y_hat <- predict( tree_tested_people, newdata=temp , type = c("response") )
          temp$tested_people_log_group_number <-  predict( tree_tested_people, newdata=temp , type = c("node"))
          temp <- temp %>% group_by(tested_people_log_group_number) %>% arrange(date_asnumeric) %>% mutate(tested_people_log_y_hat_slope=difference(tested_people_log_y_hat)) %>% fill(tested_people_log_y_hat_slope, .direction="up") %>% ungroup() %>%
                              mutate(tested_people_log_y_hat_percent_change = round((exp(tested_people_log_y_hat_slope)-1)*100,2)) 
        }, error = function(e) {})
        
        temp_list[[paste(p,d) ]] <- temp
  
      })
    }
}

lhs_long_clean_imputed <- bind_rows(temp_list)
dim(lhs_long_clean_imputed) #bigger because we're interpolating now

#rt1 <- rpart(formula = confirmed_log_y_hat_percent_change ~ date_asnumeric, data=test)
#interpolated = data.frame(date_asnumeric=min(test$date_asnumeric):max(test$date_asnumeric) )
#interpolated$confirmed_log_y_hat_percent_change_y_hat <-  predict(rt1, newdata=interpolated)
#interpolated$date_asdate <- as.Date(interpolated$date_asnumeric)


minsplit=6
minbucket=3
temp_list <- list()
for(p in places){
  print(p)
  temp <- lhs_long_clean_imputed %>% filter(wikidata_id==p) %>% arrange(date_asnumeric)
          
  tryCatch({  
    rt1 <- rpart(formula = confirmed_log_y_hat_percent_change ~ date_asnumeric, data=temp, control = rpart.control(minsplit = minsplit, minbucket=minbucket))
    temp$confirmed_log_y_hat_percent_change_y_hat <-  predict(rt1, newdata=temp)
  }, error = function(e) {})
  
  tryCatch({  
    rt2 <- rpart(formula = deaths_log_y_hat_percent_change ~ date_asnumeric, data=temp, control = rpart.control(minsplit = minsplit, minbucket=minbucket))
    temp$deaths_log_y_hat_percent_change_y_hat <-  predict(rt2, newdata=temp)
  }, error = function(e) {})

  tryCatch({  
    rt3 <- rpart(formula = tested_people_log ~ date_asnumeric, data=temp)
    temp$tested_people_log_y_hat_percent_change_y_hat <-  predict(rt3, newdata=temp, control = rpart.control(minsplit = minsplit, minbucket=minbucket))
  }, error = function(e) {})
  
  temp_list[[paste(p) ]] <- temp
}
lhs_long_clean_imputed2 <- bind_rows(temp_list)

lhs_long_clean_imputed <- lhs_long_clean_imputed2
saveRDS(lhs_long_clean_imputed, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long_clean_imputed.Rds")

```

# Plot Confirmed Curves for Select Locations

```{r, fig.width=16, fig.height=12,warning=FALSE,message=FALSE,error=FALSE, results='hide'}

lhs_long_clean_imputed <- readRDS( "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long_clean_imputed.Rds")
dim(lhs_long_clean_imputed) #337043

#we want the colors to be consistent by dataset too btw

#####
#USA Q30

#bp_temp <- breakpoints(formula=confirmed_log_y_hat_percent_change ~ 1, data=temp_df %>% mutate(group=paste(dataset,wikidata_id,confirmed_log_y_hat_percent_change)) %>% filter(wikidata_id=="Q30"))
#confirmed_log_y_hat_percent_change=fitted.values(bp_temp)

test <- lhs_long_clean_imputed %>%
        mutate(group=paste(dataset,wikidata_id,confirmed_log_group_number)) %>%
        filter(wikidata_id=="Q30") %>% 
        arrange(date_asdate)  %>%
        mutate(date_asnumeric = date_asdate %>% as.numeric() )

#rt1 <- rpart(formula = confirmed_log_y_hat_percent_change ~ date_asnumeric, data=test)
#interpolated = data.frame(date_asnumeric=min(test$date_asnumeric):max(test$date_asnumeric) )
#interpolated$confirmed_log_y_hat_percent_change_y_hat <-  predict(rt1, newdata=interpolated)
#interpolated$date_asdate <- as.Date(interpolated$date_asnumeric)

p0a <- test %>%
        ggplot() + 
           geom_point( aes(x=date_asdate, y=confirmed_log, color=dataset), alpha=.3) +
           geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
           #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
           theme_bw() +
           ggtitle("USA Q30 Confirmed") # + facet_wrap(~dataset) 

p0b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change, color=dataset), alpha=.3 ) +
         geom_line(aes(x=date_asdate , y=confirmed_log_y_hat_percent_change_y_hat ), alpha=1, color="black"  ) +
         #geom_smooth( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
         ggtitle("USA Q30 Confirmed") # + facet_wrap(~dataset)+ ylab("Daily Percent Change Confirmed")

#p0a / p0b



#####
#China Q148

test <- lhs_long_clean_imputed %>% mutate(group=paste(dataset,wikidata_id,confirmed_log_group_number)) %>% filter(wikidata_id=="Q148") %>% 
        mutate(group=paste(dataset, wikidata_id,confirmed_log_group_number)) %>%
        arrange(date_asdate) #%>%

p1a <- test %>% 
ggplot() + 
   geom_point( aes(x=date_asdate, y=confirmed_log, color=dataset), alpha=.3) +
   geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("China Q148 Confirmed")

p1b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change, color=dataset), alpha=.3) +
         geom_line(aes(x=date_asdate , y=confirmed_log_y_hat_percent_change_y_hat ), alpha=1, color="black") +
         #geom_smooth( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
   ggtitle("China Q148 Confirmed")+ ylab("Daily Percent Change Confirmed")
   

#p1a / p1b

#India
#"Q668"

test <- lhs_long_clean_imputed %>% mutate(group=paste(dataset,wikidata_id,confirmed_log_y_hat_percent_change)) %>% filter(wikidata_id=="Q668") %>% 
        mutate(group=paste(dataset, wikidata_id,confirmed_log_group_number)) %>%
        arrange(date_asdate) #%>%
        #mutate( confirmed_log_y_hat_percent_change = breakpoints(formula=confirmed_log_y_hat_percent_change ~ 1, data=. ) %>% fitted.values() )


p2a <- test %>%
ggplot() + 
   geom_point( aes(x=date_asdate, y=confirmed_log, color=dataset), alpha=.3) +
   geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("India Q668 Confirmed")

p2b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change, color=dataset), alpha=.3) +
         geom_line(aes(x=date_asdate , y=confirmed_log_y_hat_percent_change_y_hat ), alpha=1, color="black") +
         #geom_smooth( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
   ggtitle("India Q668 Confirmed")+ ylab("Daily Percent Change Confirmed")
 
  
#p2a / p2b

#Bexar
#"Q16861"

test <- lhs_long_clean_imputed %>% mutate(group=paste(dataset,wikidata_id,confirmed_log_y_hat_percent_change)) %>% filter(wikidata_id=="Q16861") %>% 
        mutate(group=paste(dataset, wikidata_id,confirmed_log_group_number)) %>%
        arrange(date_asdate) #%>%
        #mutate( confirmed_log_y_hat_percent_change = breakpoints(formula=confirmed_log_y_hat_percent_change ~ 1, data=. ) %>% fitted.values() )


p3a <- test %>%
      ggplot() + 
         geom_point( aes(x=date_asdate, y=confirmed_log, color=dataset), alpha=.3) +
         geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
         ggtitle("Bexar County Q16861 Confirmed")

p3b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change, color=dataset), alpha=.3) +
         geom_line(aes(x=date_asdate , y=confirmed_log_y_hat_percent_change_y_hat ), alpha=1, color="black") +
         #geom_smooth( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
   ggtitle("Bexar County Q16861 Confirmed") + ylim(0,100)+ ylab("Daily Percent Change Confirmed")
   
#p3a / p3b



#Italy
#"Q38"

test <- lhs_long_clean_imputed %>% mutate(group=paste(dataset,wikidata_id,confirmed_log_y_hat_percent_change)) %>% filter(wikidata_id=="Q38") %>% 
        mutate(group=paste(dataset, wikidata_id,confirmed_log_group_number)) %>%
        arrange(date_asdate) #%>%
        #mutate( confirmed_log_y_hat_percent_change = breakpoints(formula=confirmed_log_y_hat_percent_change ~ 1, data=. ) %>% fitted.values() )


p4a <- test %>%
ggplot() + 
   geom_point( aes(x=date_asdate, y=confirmed_log, color=dataset), alpha=.3) +
   geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("Italy Q38 Confirmed")

p4b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change, color=dataset), alpha=.3) +
         geom_line(aes(x=date_asdate , y=confirmed_log_y_hat_percent_change_y_hat ), alpha=1, color="black") +
         #geom_smooth( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
   ggtitle("Italy Q38 Confirmed")+ ylab("Daily Percent Change Confirmed")
   
    
#p4a / p4b



#South Korea
#"Q884"

test <- lhs_long_clean_imputed %>% mutate(group=paste(dataset,wikidata_id,confirmed_log_y_hat_percent_change)) %>% filter(wikidata_id=="Q884") %>% 
        mutate(group=paste(dataset, wikidata_id,confirmed_log_group_number)) %>%
        arrange(date_asdate) #%>%
        #mutate( confirmed_log_y_hat_percent_change = breakpoints(formula=confirmed_log_y_hat_percent_change ~ 1, data=. ) %>% fitted.values() )


p5a <- test %>%
ggplot() + 
   geom_point( aes(x=date_asdate, y=confirmed_log, color=dataset), alpha=.3) +
   geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("South Korea Q884  Confirmed")

p5b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change, color=dataset), alpha=.3) +
         geom_line(aes(x=date_asdate , y=confirmed_log_y_hat_percent_change_y_hat ), alpha=1, color="black") +
         #geom_smooth( aes(x=date_asdate , y=confirmed_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=confirmed_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
   ggtitle("South Korea Q884 Confirmed") + ylab("Daily Percent Change Confirmed")
   
    
#p5a / p5b
```



```{r, fig.width=12, fig.height=6 ,warning=FALSE,message=FALSE,error=FALSE}
(p0a + p1a + p2a ) / (p0b + p1b + p2b )
```

```{r, fig.width=12, fig.height=6 ,warning=FALSE,message=FALSE,error=FALSE}
(p3a + p4a + p5a) / (p3b + p4b + p5b)
```

# Plot Death Curves for Select Locations

Show how interpolation works on deaths

```{r, fig.width=16, fig.height=12,warning=FALSE,message=FALSE,error=FALSE, results='hide'}

test <- lhs_long_clean_imputed %>%
        mutate(group=paste(dataset,wikidata_id,deaths_log_group_number)) %>%
        filter(wikidata_id=="Q30") %>% 
        arrange(date_asdate)  %>%
        mutate(date_asnumeric = date_asdate %>% as.numeric() )

p0a <- test %>%
        ggplot() + 
           geom_point( aes(x=date_asdate, y=deaths_log, color=dataset), alpha=.3) +
           geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
           #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
           theme_bw() +
           ggtitle("USA Q30 Deaths") # + facet_wrap(~dataset) 

p0b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=deaths_log_y_hat_percent_change, color=dataset), alpha=.3 ) +
         geom_line(aes(x=date_asdate , y=deaths_log_y_hat_percent_change_y_hat ), alpha=1, color="black"  ) +
         #geom_smooth( aes(x=date_asdate , y=deaths_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
         ggtitle("USA Q30 Deaths") + ylab("Daily Percent Change Deaths") # + facet_wrap(~dataset)

#p0a / p0b


#####
#China Q148

test <- lhs_long_clean_imputed %>% mutate(group=paste(dataset,wikidata_id,deaths_log_group_number)) %>% filter(wikidata_id=="Q148") %>% 
        mutate(group=paste(dataset, wikidata_id,deaths_log_group_number)) %>%
        arrange(date_asdate) #%>%

p1a <- test %>% 
ggplot() + 
   geom_point( aes(x=date_asdate, y=deaths_log, color=dataset), alpha=.3) +
   geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("China Q148 Deaths")

p1b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=deaths_log_y_hat_percent_change, color=dataset), alpha=.3) +
         geom_line(aes(x=date_asdate , y=deaths_log_y_hat_percent_change_y_hat ), alpha=1, color="black") +
         #geom_smooth( aes(x=date_asdate , y=deaths_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
   ggtitle("China Q148 Deaths")+ ylab("Daily Percent Change Deaths")
   

#p1a / p1b

#India
#"Q668"

test <- lhs_long_clean_imputed %>% mutate(group=paste(dataset,wikidata_id,deaths_log_y_hat_percent_change)) %>% filter(wikidata_id=="Q668") %>% 
        mutate(group=paste(dataset, wikidata_id,deaths_log_group_number)) %>%
        arrange(date_asdate) #%>%
        #mutate( deaths_log_y_hat_percent_change = breakpoints(formula=deaths_log_y_hat_percent_change ~ 1, data=. ) %>% fitted.values() )


p2a <- test %>%
ggplot() + 
   geom_point( aes(x=date_asdate, y=deaths_log, color=dataset), alpha=.3) +
   geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("India Q668 Deaths")

p2b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=deaths_log_y_hat_percent_change, color=dataset), alpha=.3) +
         geom_line(aes(x=date_asdate , y=deaths_log_y_hat_percent_change_y_hat ), alpha=1, color="black") +
         #geom_smooth( aes(x=date_asdate , y=deaths_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
   ggtitle("India Q668 Deaths")+ ylab("Daily Percent Change Deaths")
 
  
#p2a / p2b

#Bexar
#"Q16861"

test <- lhs_long_clean_imputed %>% mutate(group=paste(dataset,wikidata_id,deaths_log_y_hat_percent_change)) %>% filter(wikidata_id=="Q16861") %>% 
        mutate(group=paste(dataset, wikidata_id,deaths_log_group_number)) %>%
        arrange(date_asdate) #%>%
        #mutate( deaths_log_y_hat_percent_change = breakpoints(formula=deaths_log_y_hat_percent_change ~ 1, data=. ) %>% fitted.values() )


p3a <- test %>%
      ggplot() + 
         geom_point( aes(x=date_asdate, y=deaths_log, color=dataset), alpha=.3) +
         geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
         ggtitle("Bexar County Q16861 Deaths")

p3b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=deaths_log_y_hat_percent_change, color=dataset), alpha=.3) +
         geom_line(aes(x=date_asdate , y=deaths_log_y_hat_percent_change_y_hat ), alpha=1, color="black") +
         #geom_smooth( aes(x=date_asdate , y=deaths_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
   ggtitle("Bexar County Q16861 Deaths") + ylim(0,100)+ ylab("Daily Percent Change Deaths")
   
#p3a / p3b



#Italy
#"Q38"

test <- lhs_long_clean_imputed %>% mutate(group=paste(dataset,wikidata_id,deaths_log_y_hat_percent_change)) %>% filter(wikidata_id=="Q38") %>% 
        mutate(group=paste(dataset, wikidata_id,deaths_log_group_number)) %>%
        arrange(date_asdate) #%>%
        #mutate( deaths_log_y_hat_percent_change = breakpoints(formula=deaths_log_y_hat_percent_change ~ 1, data=. ) %>% fitted.values() )


p4a <- test %>%
ggplot() + 
   geom_point( aes(x=date_asdate, y=deaths_log, color=dataset), alpha=.3) +
   geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("Italy Q38 Deaths")

p4b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=deaths_log_y_hat_percent_change, color=dataset), alpha=.3) +
         geom_line(aes(x=date_asdate , y=deaths_log_y_hat_percent_change_y_hat ), alpha=1, color="black") +
         #geom_smooth( aes(x=date_asdate , y=deaths_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
   ggtitle("Italy Q38 Deaths")+ ylab("Daily Percent Change Deaths")
   
    
#p4a / p4b



#South Korea
#"Q884"

test <- lhs_long_clean_imputed %>% mutate(group=paste(dataset,wikidata_id,deaths_log_y_hat_percent_change)) %>% filter(wikidata_id=="Q884") %>% 
        mutate(group=paste(dataset, wikidata_id,deaths_log_group_number)) %>%
        arrange(date_asdate) #%>%
        #mutate( deaths_log_y_hat_percent_change = breakpoints(formula=deaths_log_y_hat_percent_change ~ 1, data=. ) %>% fitted.values() )


p5a <- test %>%
ggplot() + 
   geom_point( aes(x=date_asdate, y=deaths_log, color=dataset), alpha=.3) +
   geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
   #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
   theme_bw() +
   ggtitle("South Korea Q884  Deaths")

p5b <- test %>%
       ggplot() + 
         geom_point( aes(x=date_asdate , y=deaths_log_y_hat_percent_change, color=dataset), alpha=.3) +
         geom_line(aes(x=date_asdate , y=deaths_log_y_hat_percent_change_y_hat ), alpha=1, color="black") +
         #geom_smooth( aes(x=date_asdate , y=deaths_log_y_hat_percent_change), alpha=.3, span=.05) + #, span=.05
         #geom_line( aes(x=date_asdate, y=deaths_log_y_hat, color=dataset, group=group)) + #
         #geom_line(data=temp_df2, aes(x=date_asdate, y=y_hat_mean), color="black", linetype = "dashed") +
         theme_bw() +
   ggtitle("South Korea Q884 Deaths") + ylab("Daily Percent Change Deaths")
   
    
#p5a / p5b
```

I need to fix this, USA has a fraction of a death

```{r, fig.width=12, fig.height=6 ,warning=FALSE,message=FALSE,error=FALSE}
(p0a + p1a + p2a ) / (p0b + p1b + p2b )
```

```{r, fig.width=12, fig.height=6 ,warning=FALSE,message=FALSE,error=FALSE}
(p3a + p4a + p5a) / (p3b + p4b + p5b)
```

# Aligning Curves by Takeoff

```{r, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide'}

df_slopes <- lhs_long_clean_imputed %>% 
             group_by(gid, geonameid, wikidata_id, date_asdate) %>% 
             mutate(
                    confirmed=median(confirmed, na.rm=T),
                    deaths=median(deaths, na.rm=T),
                    tested_people=median(tested_people, na.rm=T),
                    
                    confirmed_log_y_hat=median(confirmed_log_y_hat, na.rm=T),
                    deaths_log_y_hat=median(deaths_log_y_hat, na.rm=T),
                    tested_people_log_y_hat=median(tested_people_log_y_hat, na.rm=T),
                    ) %>%
             ungroup() %>%
             group_by(gid, geonameid, wikidata_id) %>% 
             arrange(date_asdate) %>% 
             filter(!duplicated(date_asdate)) %>% 
             mutate(t = row_number()) %>%
             mutate(confirmed_log_y_hat_percent_change_y_hat_fd=difference(confirmed_log_y_hat_percent_change_y_hat)) %>%
  
             #So there are ones that start low and build up
             mutate(t_alligned_maxfd =  confirmed_log_y_hat_percent_change_y_hat_fd==max(confirmed_log_y_hat_percent_change_y_hat_fd,na.rm=T) ) %>% 
             #but there are other ones that go from zero to high and those aren't getting caught by the top above
             mutate(t_alligned_max_at_0 =  confirmed_log_y_hat_percent_change_y_hat==max(confirmed_log_y_hat_percent_change_y_hat,na.rm=T) & t==1 ) %>% 

             mutate(t_alligned = ifelse(t_alligned_max_at_0 | (t_alligned_maxfd & max(t_alligned_max_at_0)==F) , t, NA) ) %>% 
             mutate(t_alligned = max(t_alligned, na.rm=T) ) %>%
             mutate(t_alligned = t-max(t_alligned, na.rm=T) ) %>%

             mutate(t_alligned_firstconfirmed = ifelse(confirmed==min(confirmed, na.rm=T) , t, NA) ) %>% 
             mutate(t_alligned_firstconfirmed = max(t_alligned_firstconfirmed, na.rm=T) ) %>%
             mutate(t_alligned_firstconfirmed = t-max(t_alligned_firstconfirmed, na.rm=T) ) %>%

             #this throws a bunch of infinite missing because of countries that haven't yet hit 100
             #mutate(t_alligned_100confirmed_100 = ifelse(confirmed>=100  , t, NA) ) %>% 
             #mutate(t_alligned_100confirmed_100 = ifelse(t_alligned_100confirmed_100==min(t_alligned_100confirmed_100, na.rm=T) , t, NA) ) %>% 
             #mutate(t_alligned_100confirmed = t-max(t_alligned_100confirmed_100, na.rm=T) ) %>%
             #dplyr::select(-t_alligned_100confirmed_100) %>%
             ungroup()
```

```{r, eval=F}

library(infotheo)
#Once we removed the no variance places my alignment has more mutual information than either first confirmed or 100 confirmed
mutinformation(X=df_slopes %>% dplyr::select(t,t_alligned,t_alligned_firstconfirmed,confirmed_log_y_hat_percent_change_y_hat) %>% discretize(), method="emp") #we outperformed t_alligned_100confirmed but it wasn't avail for most

#df_slopes %>% filter(abs(t_alligned)<20,abs(t_alligned_100confirmed_100)<20) %>% janitor::tabyl(t_alligned, t_alligned_100confirmed_100)
#hist(df_slopes$t_alligned - df_slopes$t_alligned_100confirmed_100, breaks=50)
  
```

Plot of curves raw and then curves alligned by takeoff start

```{r, fig.width=12, fig.height=12, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide'}

#df_slopes %>% mutate(group=paste(wikidata_id)) %>%
#              ggplot() + 
#              geom_line( aes(x=t_alligned , y=confirmed_log_y_hat_percent_change_y_hat, group=wikidata_id), alpha=.05, color="blue") +
#              theme_bw() +
#              ggtitle("Interpolated log count by Alligned T") + ylim(0,100)


p_t_alligned_firstconfirmed <-  df_slopes %>% mutate(group=paste(wikidata_id)) %>% 
                               ggplot() + 
                               geom_line( aes(x=t_alligned_firstconfirmed , y=confirmed_log_y_hat_percent_change_y_hat, group=group, color=wikidata_id), alpha=1) +
                               theme_bw() +
                               ggtitle("Interpolated log count by T") + ylim(0,100) +
                               gghighlight(wikidata_id %in% c('Q30','Q1439','Q668','Q884','Q16861','Q159','Q38'), unhighlighted_params=list(alpha=.1)) +
                                 geom_smooth( aes(x=t_alligned_firstconfirmed , y=confirmed_log_y_hat_percent_change_y_hat), span = 0.05)  + xlim(-50,100)

#p_t_alligned_firstconfirmed


p_t_alligned <- df_slopes %>% mutate(group=paste(wikidata_id)) %>% 
               ggplot() + 
               geom_line( aes(x=t_alligned , y=confirmed_log_y_hat_percent_change_y_hat, group=group, color=wikidata_id), alpha=1) +
               theme_bw() +
               ggtitle("Interpolated log count by Alligned T") + ylim(0,100) +
               gghighlight(wikidata_id %in% c('Q30','Q1439','Q668','Q884','Q16861','Q159','Q38'), unhighlighted_params=list(alpha=.1)) +
               geom_smooth( aes(x=t_alligned , y=confirmed_log_y_hat_percent_change_y_hat), span = 0.05) + xlim(-50,100)

#p_t_alligned
#df_slopes %>% mutate(group=paste(wikidata_id)) %>% 
#ggplot() + 
#   geom_line( aes(x=t_alligned , y=confirmed_log_y_hat_percent_change_y_hat, group=group, color=wikidata_id), alpha=1) +
#   theme_bw() +
#   ggtitle("Interpolated log count by Alligned T") + ylim(0,100)  + xlim(0,100) +
#   gghighlight(wikidata_id %in% c('Q30','Q1439','Q668','Q884','Q16861','Q159','Q38'), unhighlighted_params=list(alpha=.1)) +
#   geom_smooth( aes(x=t_alligned , y=confirmed_log_y_hat_percent_change_y_hat), span = 0.05) 



```

The idea here is that completely unconstrained, COVID-19 growth should follow a logistic curve, flat, upswing, constant growth, downswing, and flat again. The assumption of no unconstrained growth no longer holds because the world is reacting, but there's still a pretty uniformly characteristic upswing across geographic units. We look for this signal of the largest first difference in the daily percent change, call that the takeoff date, and then allign all the time series with that date as 0.

```{r, fig.width=12, fig.height=12,warning=FALSE,message=FALSE,error=FALSE}
p_t_alligned_firstconfirmed  /p_t_alligned 
```


What this gives us is a nice normalized dataset where the task is to predict the shape of that distribution. We could try to predict time until the takeoff, the intensity of the growth at the takeoff point, the time until it returns to zero, the area under the curve, or every nook and change.


```{r, eval=F}
lhs_long_median %>% ggplot(aes(x=date_asdate, y=confirmed, color=wikidata_id)) + geom_line() + theme(legend.position = "none")
```

```{r, eval=F}
lhs_long_median %>% ggplot(aes(x=date_asdate, y=deaths, color=wikidata_id)) + geom_line() + theme(legend.position = "none")

lhs_long_median %>% ggplot(aes(x=date_asdate, y=deaths, color=wikidata_id)) + geom_line() + theme(legend.position = "none") + scale_y_log10()

```
```{r, eval=F}
library(tsibble)
lhs_long_median %>% arrange(wikidata_id,date_asdate) %>%
  group_by(wikidata_id) %>%
  mutate(deaths_fd=difference(deaths)) %>% 
  ungroup() %>%
  filter(deaths_fd>0) %>%
  ggplot(aes(x=date_asdate, y=deaths_fd, color=wikidata_id)) + 
  geom_line() +
  #stat_smooth(method="loess", se = F) +
  theme(legend.position = "none") # + scale_y_log10()
```




```{r, eval=F}

lm1 <- lm(deaths ~  confirmed,data=lhs_long_median %>% filter(CFR<1))
lm2 <- lm(deaths ~ tested_people + confirmed,data=lhs_long_median %>% filter(CFR<1))

library(ggRandomForests); #install.packages('ggRandomForests')
rf1 <- rfsrc(deaths~ confirmed,
                      data=lhs_long_median %>% filter(CFR<1) %>% as.data.frame() )
gg_e <- gg_error(rf1)
gg_v <- gg_variable(rf1)
plot(gg_v, panel=TRUE, se=.95, span=1.2, alpha=.4) 

rf1 <- rfsrc(deaths~ confirmed + tested_people,
                      data=lhs_long_median %>% filter(CFR<1) %>% as.data.frame() )
gg_e <- gg_error(rf1)
gg_v <- gg_variable(rf1)
plot(gg_v, panel=TRUE, se=.95, span=1.2, alpha=.4)


rf2 <- rfsrc(CFR ~  tested_people_log,
                      data=lhs_long_median %>% 
                      mutate(CFR=deaths/confirmed) %>%
                      mutate(tested_people_log=log(tested_people+1)) %>%
                      filter(CFR<1) %>% as.data.frame()
             )
gg_v <- gg_variable(rf2)
plot(gg_v, panel=TRUE, se=.95, span=1.2, alpha=.4)

rf2 <- rfsrc(CFR ~  tested_people_log + confirmed_log,
                      data=lhs_long_median %>% 
                      mutate(CFR=deaths/confirmed) %>%
                      mutate(tested_people_log=log(tested_people+1)) %>%
                      mutate(confirmed_log=log(confirmed+1)) %>%
                      filter(CFR<1) %>% as.data.frame()
             )
gg_v <- gg_variable(rf2)
plot(gg_v, panel=TRUE, se=.95, span=1.2, alpha=.4)


#Throw in time
rf3 <- rfsrc(CFR ~  positive_perc + tested_people_log,
                                    data=lhs_long_median %>% 
                                    mutate(CFR=deaths/confirmed) %>%
                                    mutate(positive_perc=confirmed/tested_people) %>%
                                    mutate(tested_people_log=log(tested_people+1)) %>%
                                    mutate(confirmed_log=log(confirmed+1)) %>%
                                    filter(CFR<1) %>% as.data.frame()
             )
gg_v <- gg_variable(rf3)
plot(gg_v, panel=TRUE, se=.95, span=1.2, alpha=.4)

partial_Boston <- plot.variable(rf3,
partial=TRUE, sorted=FALSE,
show.plots = FALSE )

gg_p <- gg_partial(partial_Boston)
plot(gg_p, panel=TRUE, points = F )


copper_cts <-quantile_pts(lhs_long_median$tested_people_log, groups = 6, intervals = TRUE)
partial_coplot_Boston <- gg_partial_coplot(rf2, xvar="confirmed_log",
                                         groups=rm_grp,
                                         show.plots=FALSE)



summary(lhs_long_median$CFR)

lhs_long_median %>% filter(confirmed>10) %>% filter(CFR<1) %>% pull(CFR) %>% summary() #0.0297297  that's a median CFR of about 3%
lhs_long_median %>% filter(confirmed>10) %>% filter(CFR<1) %>% pull(CFR) %>% hist(breaks=50)

lhs_long_median %>% filter(confirmed>10) %>% filter(CFR<1) %>% ggplot(aes(x=CFR)) + geom_density()


```



```{r, eval=F}
#This code is now depricated in favor of the tree base method above

places <- lhs_long_clean$wikidata_id %>% unique() %>% na.omit() ; length(places)
datasets <- lhs_long_clean$dataset %>% unique() %>% na.omit() ; table(dataset)

temp_list <- list()
for(p in places){
    print(p)
    for(d in datasets){
      temp <- NULL
      temp <- lhs_long_clean %>% 
              arrange(date_asdate) %>% 
              filter(dataset %in% d) %>% 
              filter(wikidata_id %in% p) %>% 
              mutate(confirmed_log=log(confirmed+1)) %>%
              mutate(deaths_log=log(deaths+1)) %>%
              mutate(tested_people_log=log(tested_people+1)) %>%
              mutate(t= as.numeric(date_asdate) - min(as.numeric(date_asdate)) +1  ) %>% #start at 1 to make indexing easier
              mutate(i= row_number()  ) # actually need this
        
      i_original <- temp$i
      
      if( nrow(temp)==0 ) { next} #print("error");
      print(p)
      
      #bp <- breakpoints(confirmed_log ~ 1, data=temp)
      bp <- NULL
      lm1 <- NULL
      y_hat <- NA
      cdf <- NULL
      try({
            #if it fails fall back to just a lm
        lm1 <- lm(confirmed_log ~ 1 + t, data=temp)
    
        cdf <- data.frame(
                          t= 1, 
                          confirmed_log_intercept= coef(lm1)[1],
                          confirmed_log_slope= coef(lm1)[2],
                          confirmed_log_slope_break=0
                          )
        confirmed_log_y_hat=fitted.values(lm1)
      })
      
      try({
        bp <- breakpoints(confirmed_log ~ 1 + t, data=temp, h=3)
        
        cdf <- data.frame(
                          i= c(1,bp$breakpoints), 
                          confirmed_log_intercept= coef(bp)[,1],
                          confirmed_log_slope= coef(bp)[,2],
                          confirmed_log_slope_break=1
                          )
        confirmed_log_y_hat=fitted.values(bp)
      })
      
      try({
        temp2 <- temp
        temp2$confirmed_log_y_hat <- NA
        temp2$confirmed_log_y_hat <- confirmed_log_y_hat
        
        temp2 <- temp2 %>% 
                left_join(cdf) 
        q=paste0(p,"_",d)
        temp2 <- temp2 %>% 
                  expand(dataset, gid,geonameid, wikidata_id,t=min(temp$t):max(temp$t)) %>% #expand this to include all the t 
                  left_join(temp2) %>% 
                  mutate(date_asdate=min(date_asdate, na.rm=T)-1+t) %>% #go back interp date again
                  mutate(confirmed_log_slope_break = confirmed_log_slope_break %>% replace_na(0) %>% cumsum() ) %>%
                  fill(confirmed_log_intercept) %>%
                  fill(confirmed_log_slope) %>%
                  mutate(confirmed_log_y_hat= confirmed_log_intercept + confirmed_log_slope*t ) %>% 
                  mutate(confirmed_log_slope_percent_change = round((exp(confirmed_log_slope)-1)*100,2)) 

        temp_list[[as.character(q)]] <- temp2
      })
      #if( is.na( temp_list[[as.character(q)]]$y_hat) ) {print("error"); break}
    }
}
#"13055"
temp_df <- bind_rows(temp_list)
dim(temp_df) #bigger because we're interpolating now

# install.packages("devtools")
#devtools::install_github("tidyverse/multidplyr")

library(multidplyr)
library(dplyr, warn.conflicts = FALSE)
#cluster <- new_cluster(4)

#need to supress messages
rex_function <- function(x){
  temp=data.frame(confirmed_log_slope=x)
  tryCatch({
    #if there's an NA in x we get an error because breakpoints is one short
    prediction <- breakpoints(formula=confirmed_log_slope ~ 1, data=temp, h=3) %>% fitted.values()
    result=rep(NA, length(x) )
    result[!is.na(x)] <-prediction
    return(result)
    
  }, error=function(e){})
  
  return( rep(NA, length(x) ) )
         
}
#didn't take too long now

#library(multidplyr)
#library(dplyr, warn.conflicts = FALSE)
#cluster <- new_cluster(4)

library(tictoc)
tic()
library(strucchange)
test <- temp_df %>% head(20000)
test3 <- test %>% 
                group_by(wikidata_id) %>% 
                #partition(cluster) %>%
                arrange(date_asdate) %>%
                #mutate( confirmed_log_slope_y_hat = breakpoints(formula=confirmed_log_slope ~ 1, data=. ) %>% fitted.values() ) %>% 
                mutate( confirmed_log_slope_y_hat = rex_function(confirmed_log_slope) ) %>%  #fails on the cluster
                ungroup() #%>%
                #collect()
toc() #48 seconds for 10k #122 seconds for 20k

saveRDS(temp_df, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long_interpolated.Rds")

```



