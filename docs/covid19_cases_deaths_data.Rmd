---
title: "How to be Curious instead of Contrarian About Covid 19 Part 2: Cases, Tests, and Deaths"
output:
  html_notebook:
    toc: yes
date: 
author: 
affiliation: Director, Machine Learning for Social Science Lab, Center for Peace and Security Studies, University of California San Diego
editor_options: 
  chunk_output_type: inline
---

Rex W. Douglass

<style type="text/css">
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
</style>

The good news brigade is back at it, looking for angles with which to interpret count data that support a happy or underdog story.

```{r}
#libraries
library(lubridate)
library(tidyverse)

#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(scales)
library(gghighlight)
library(lubridate)
library(R0)  # consider moving all library commands to top -- this one was in a loop below

library(WikidataR)
library(countrycode)

library(usmap) ; # install.packages('usmap')
data(statepop)
#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(sf)

library(jsonlite)


```

Can start back from here

```{r}

admin0 <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/admin0.Rds")
admin1 <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/admin1.Rds")
admin2 <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/admin2.Rds")

rex_clean <- function(x){ x %>% stringi::stri_trans_general("latin-ascii") %>% stringi::stri_replace_all(regex="[^A-Za-z0-9]","") %>% tolower() } #so individually, each string should be devoid of spaces,noncharacters, and nonlatin


rex_admin_function <- function(x) {
  x %>% 
  left_join(admin0 ) %>%
  left_join(admin1 )  %>%
  left_join(admin2 ) %>%
  mutate(gid=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',gid2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', gid1, gid0))) %>%
  mutate(wikidata_id=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',wikidata_id2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', wikidata_id1, wikidata_id0))) %>%
  mutate(geonameid=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',geonameid2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', geonameid1, geonameid0))) %>%
  mutate(admin_level=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', 1, 0)))
}

```

## Google maps

```{r}
#install.packages("mapsapi")
#library(mapsapi)
#doc = mp_geocode(addresses = "Tel-Aviv", key=) #couldn't get this to work with my api key. intentionally leaving it out here so I don't push it accidentally

```

# LHS

## Mexico

https://www.reddit.com/r/datasets/comments/fykr5h/json_dataset_about_covid19_in_mexico/

## California Hosptilizations

https://github.com/CALmatters/covid-19-california-hospitalizations-data
https://public.tableau.com/profile/ca.open.data#!/vizhome/COVID-19PublicDashboard/Covid-19Hospitals

## ILILNet

#Can download state-week data here
https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html

https://wwwn.cdc.gov/ILINet/

https://www.medrxiv.org/content/10.1101/2020.04.01.20050542v1

https://www.economist.com/graphic-detail/2020/04/11/why-a-study-showing-that-covid-19-is-everywhere-is-good-news?fsrc=scn%2Ftw%2Fte%2Fbl%2Fed%2Ffootprintsoftheinvisibleenemywhyastudyshowingthatcovid19iseverywhereisgoodnewsgraphicdetail&%3Ffsrc%3Dscn%2F=tw%2Fdc

## ECDC
https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide

Country level

```{r}

#these libraries need to be loaded
library(utils)
#read the Dataset sheet into “R”. The dataset will be called "data".
ecdc <- read.csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv", na.strings = "", fileEncoding = "UTF-8-BOM")

ecdc_log <- ecdc %>% 
  mutate(date_asdate = dmy(dateRep)) %>%
  mutate(dataset="ecdc") %>% 
  
  mutate(admin0_name_clean=countriesAndTerritories %>% rex_clean()) %>%
  mutate(admin1_name_clean='') %>%
  mutate(admin2_name_clean='') %>%
  rex_admin_function() %>%
  group_by(gid, geonameid,wikidata_id) %>% arrange(date_asdate) %>%
  mutate(confirmed=cumsum(cases),
         deaths=cumsum(deaths)) %>%
  ungroup()

forjoining_ecdc <- ecdc_log %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate, confirmed, deaths)


```


## India
https://api.covid19india.org/
https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#
https://github.com/covid19india/api/blob/master/state_test_data.json


https://github.com/amodm/api-covid19-in


```{r}
#Couldn't get any of this to work
#library(googlesheets4)
#temp <- read_csv("http://api.covid19india.org/states_daily_csv/confirmed.csv")
#temp <- read_sheet("https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#")
#install.packages('gsheet')
#library(gsheet)

#The test data is pushed as a json file here but it's not historical
#https://github.com/covid19india/api/blob/master/state_test_data.json
covid19india_test <- fromJSON("https://raw.githubusercontent.com/covid19india/api/master/state_test_data.json")$states_tested_data
saveRDS(covid19india_test, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/covid19india_test_latest_" %>% paste0(Sys.Date(),".Rds")   ) #start saving dayly snapshots that we can add to the pile

covid19india_test_long <- covid19india_test %>% 
                          mutate(date_asdate = dmy(updatedon )) %>%
                          mutate(admin0_name_clean="India" %>% rex_clean()) %>%
                          mutate(admin1_name_clean= state %>% rex_clean()) %>%
                          mutate(admin2_name_clean= "" %>% rex_clean()) %>%
                          mutate(totaltested=as.numeric(totaltested)) %>%
                          dplyr::select(date_asdate, admin0_name_clean, admin1_name_clean, admin2_name_clean, tested_people=totaltested)
                          
  
#construct_download_url(url="https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#", format = "csv", sheetid = 1)
#temp < gsheet2tbl(url="https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#", sheetid = 1)
library(jsonlite)
rootnet_case_json <- fromJSON("https://api.rootnet.in/covid19-in/stats/history")
for(i in 1:length(rootnet_case_json$data$regional)){
  rootnet_case_json$data$regional[[i]]$day <- rootnet_case_json$data$day[i]
}
rootnet_cases_regional <- bind_rows(rootnet_case_json$data$regional)
rootnet_cases_regional <- rootnet_cases_regional  %>%
                           mutate(admin0_name_clean="India" %>% rex_clean()) %>%
                           mutate(admin1_name_clean= loc %>% rex_clean()) %>%
                           mutate(admin2_name_clean= "" %>% rex_clean())

rootnet_cases_national <- rootnet_case_json$data$summary
rootnet_cases_national$day <- rootnet_case_json$data$day

#This is just national testing
rootnet_testing <- fromJSON("https://api.rootnet.in/covid19-in/stats/testing/history")$data

rootnet_national <- rootnet_testing %>%
                     full_join(rootnet_cases_national) %>%
                     mutate(admin0_name_clean="India" %>% rex_clean()) %>%
                     mutate(admin1_name_clean= "" %>% rex_clean()) %>%
                     mutate(admin2_name_clean= "" %>% rex_clean()) 

covid19india_long <- bind_rows(
                rootnet_national %>% rename(tested_samples=totalSamplesTested, tested_people=totalIndividualsTested) %>% mutate(date_asdate = ymd(day )),
                rootnet_cases_regional %>% mutate(date_asdate = ymd(day )) %>% full_join(covid19india_test_long %>% distinct()) 
                ) %>%
                mutate(dataset="covid19india") %>%
                rex_admin_function()

forjoining_covid19india <- covid19india_long %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate,
                                                     tested_samples,
                                                     tested_people,
                                                     confirmed=total,
                                                     deaths=deaths
                                                     )

dim(forjoining_covid19india) #946


```


## Bing

```{r}

library(jsonlite)
#library(rjson)
#tmp2 <- tempfile()
#curl_download("https://bing.com/covid/data", tmp2)
bing1 <- fromJSON("https://web.archive.org/web/20200407032518if_/https://bing.com/covid/data")
bing2 <-  fromJSON("https://web.archive.org/web/20200414040859if_/https://bing.com/covid/data")
bing3 <- fromJSON("https://web.archive.org/web/20200331030201if_/https://bing.com/covid/data")
bing_latest <- fromJSON("https://bing.com/covid/data")
saveRDS(bing_latest, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/bing_latest_" %>% paste0(Sys.Date(),".Rds")   ) #start saving dayly snapshots that we can add to the pile
areas <- bind_rows(
  bing1$areas,
  bing2$areas,
  bing3$areas,
  bing_latest$areas,
)

temp1 <- bing1$areas %>% dplyr::select(-areas)
temp2 <- bing2$areas %>% dplyr::select(-areas)
temp3 <- bing3$areas %>% dplyr::select(-areas)
temp4 <- bing_latest$areas %>% dplyr::select(-areas)

bing_df0 <- bind_rows(areas) %>% mutate(admin0_name_original=id)
dim(bing_df0) #835 14

bing_df1 <- bind_rows(bing_df0$areas) %>% separate(id, c("admin1_name_original", "admin0_name_original")) 
dim(bing_df1) #753  13

bing_df2 <- bind_rows(bing_df1$areas) %>% separate(id, c("admin2_name_original","admin1_name_original", "admin0_name_original")) 
dim(bing_df2) #2872   13

bing_long <- bind_rows(
          bing_df0 %>% dplyr::select(-areas),
          bing_df1 %>% dplyr::select(-areas),
          bing_df2 %>% dplyr::select(-areas)
) %>% 
  mutate(date_asdate = ymd(substring(lastUpdated,1,10))) %>%
  mutate(date_asdate = date_asdate-1 ) %>% #BING data are one day behind other data, subtract a day off
  
  mutate(dataset="bing") %>% 
  
  mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
  mutate(admin0_name_clean=str_replace(admin0_name_clean,"chinamainland","china")) %>%
  mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
  mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
  rex_admin_function()

forjoining_bing <- bing_long %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate, confirmed=totalConfirmed, deaths=totalDeaths)
dim(forjoining_bing)

#Some of these are wrong
#russia khanty mansi  is a second level administrative division, it and a bunch of other ones from russia are listed as first order

#other are ambigious like
#Russia Voronezh might either be the city or the Oblast

```

## WHO

```{r}
WHO <- read_csv(url("https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv")) 

who_long <- WHO %>%
            mutate(dataset="who") %>%
            mutate(admin0_name_original=Country) %>%
            mutate(date_asdate = ymd(Date)) %>% 
            rename(confirmed=Confirmed  , deaths=Deaths)  %>%
  
             mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
             mutate(admin1_name_clean='') %>%
             mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
             rex_admin_function()

forjoining_who <- who_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths)

```

## CSSEGISandData
Johns Hopkins

```{r, echo=F, message=FALSE, results = FALSE, warning=FALSE}

#https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/
CSSE_confirmed_us      <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv"))
CSSE_confirmed_global  <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"))

CSSE_deaths_us      <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv"))
CSSE_deaths_global  <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv"))

CSSE_confirmed_us_long <- pivot_longer(CSSE_confirmed_us, names_to = "date", cols = ends_with("20"), values_to = "confirmed")
CSSE_confirmed_global_long <- pivot_longer(CSSE_confirmed_global, names_to = "date", cols = ends_with("20"), values_to = "confirmed")

CSSE_deaths_us_long <- pivot_longer(CSSE_deaths_us, names_to = "date", cols = ends_with("20"), values_to = "deaths")
CSSE_deaths_global_long <- pivot_longer(CSSE_deaths_global, names_to = "date", cols = ends_with("20"), values_to = "deaths")

CSSE_us_long <- CSSE_confirmed_us_long %>% full_join(CSSE_deaths_us_long) %>% mutate(admin0_name_original="United States") %>% mutate(admin1_name_original=Province_State) %>% mutate(admin2_name_original=Admin2)
CSSE_global_long <- CSSE_confirmed_global_long %>% full_join(CSSE_deaths_global_long) %>% mutate(admin0_name_original=`Country/Region`) %>% mutate(admin1_name_original=`Province/State`)

CSSE_long <- bind_rows(CSSE_us_long, CSSE_global_long) %>% 
              mutate(dataset="CSSE") %>%
              mutate(date_asdate = mdy(str_replace(date,"20$","2020"))) %>%

              mutate(admin0_name_clean=admin0_name_original %>% rex_clean())  %>%
              mutate(admin1_name_clean=admin1_name_original %>% rex_clean())  %>%
              mutate(admin2_name_clean=admin2_name_original %>% rex_clean())  %>%
              rex_admin_function() %>%
              distinct()

test <- CSSE_long %>% group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>% mutate(n=n()) %>% ungroup() #the dupes are coming either from NA location/date info or deaths and confirmed appearing as two seperate records


forjoining_CSSE <- CSSE_long %>% 
                   dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths) %>%
                   group_by(dataset, gid, geonameid, wikidata_id,date_asdate) %>% summarize(confirmed=max(confirmed, na.rm=T), deaths=max(deaths, na.rm=T))

test <- forjoining_CSSE %>% group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>% mutate(n=n()) %>% ungroup() #the dupes are coming either from NA location/date info or deaths and confirmed appearing as two seperate records


```


## NYT


```{r}
#https://www.nytimes.com/article/coronavirus-county-data-us.html
#https://github.com/nytimes/covid-19-data
nytimes <- read_csv(url("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")) #

nytimes_long <- nytimes %>%
                mutate(date_asdate = ymd(date)) %>% 
                rename(confirmed=cases) %>%
                mutate(dataset="nyt") %>%
                mutate(admin0_name_original="United States") %>%
                mutate(admin1_name_original=state) %>%
                mutate(admin2_name_original=county)  %>%

                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                mutate(admin0_name_clean=str_replace(admin0_name_clean,"chinamainland","china")) %>%
              
                mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%

                mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
               rex_admin_function()

forjoining_nytimes <- nytimes_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths)

```

### Covidtracking.com

```{r}
#https://covidtracking.com/about-data
"If you are calculating positive rates, it should only be with states that have an A grade. And be careful going back in time because almost all the states have changed their level of reporting at different times."

#The data contain the u.s. and the states too
#FIPS the first two digits are the state
#All 0 is the U.S.
#
#https://covidtracking.com/api
covidtracking <- read_csv(url("https://covidtracking.com/api/states/daily.csv"))
dim(covidtracking) #1653   25
covidtracking_t <- t(covidtracking)

covidtracking_long <- covidtracking %>%

                      mutate(date_asdate = ymd(date)) %>% 
                      rename(state_abbr=state) %>%
                      left_join(state_codes) %>% 
                      dplyr::select(date_asdate, state_name, state_abbr, positive, negative, death) %>%
                      mutate(dataset="covidtracking") %>%
                      mutate(admin0_name_original="United States") %>%
                      mutate(admin1_name_original=state_name)  %>%
                      mutate(admin2_name_original='')  %>%
  
                      mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                      mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                      mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                      rex_admin_function()

forjoining_covidtracking <- covidtracking_long %>%
                            mutate(tested_people=negative+positive) %>%
                            dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=positive, deaths=death, tested_people)

```

metabiota.com

```{r}

metabiota <- read_csv("/home/skynet2/Downloads/data_ncov2019.csv")
dim(metabiota) #605848

table(metabiota$OUTCOME)
table(metabiota$CUMULATIVE_FLAG)

#cities look broken with just a flat number of 1 or 2 "Ho Chi Minh City"

metabiota_confirmed <- metabiota %>% filter(CUMULATIVE_FLAG==T) %>% filter(OUTCOME=="CASE") %>% filter(CONFIRM_STATUS=="CONFIRMED") %>% 
                       filter( DATE_LOW==DATE_HIGH) %>%
                       mutate(composite_source = SOURCE=="Metabiota Composite Source") %>%
                       distinct() %>%
                       arrange(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW, -composite_source) %>%
                       group_by(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW) %>%
                       filter(row_number()==1) %>%
                       ungroup() %>%
                       dplyr::select(date=DATE_REPORT,
                                          AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME,LOCALITY_NAME,confirmed=VALUE) 

#test <- metabiota_confirmed %>% group_by(AL0_NAME,AL1_CODE, AL1_NAME, AL2_NAME, AL3_NAME, LOCALITY_NAME, DATE_LOW) %>% mutate(n=n()) %>% ungroup() #the entire problem is that we're not handling cities correctly.


metabiota_deaths <- metabiota %>% filter(CUMULATIVE_FLAG==T) %>% filter(OUTCOME=="DEATH") %>% filter(CONFIRM_STATUS=="CONFIRMED") %>% 
                       filter( DATE_LOW==DATE_HIGH) %>%
                       mutate(composite_source = SOURCE=="Metabiota Composite Source") %>%
                       distinct() %>%
                       arrange(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW, -composite_source) %>%
                       group_by(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW) %>%
                       filter(row_number()==1) %>%
                       ungroup() %>%
                       dplyr::select(date=DATE_REPORT, #DATE_LOW #changing this to date of the report to align it with other 
                                     AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME,LOCALITY_NAME,deaths=VALUE) 

metabiota_long <- metabiota_confirmed %>% 
                  full_join(metabiota_deaths) %>%
                  group_by(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME) %>%
                  filter(min(confirmed)!=max(confirmed)) %>% #require some variation
                  ungroup() %>%
                  rename(admin0_name_original=AL0_NAME,
                         admin1_name_original=AL1_NAME,
                         admin2_name_original=AL2_NAME,
                         admin3_name_original=LOCALITY_NAME
                         ) %>%
                  dplyr::select(-AL3_NAME) %>%
                  mutate(dataset="metabiota") %>%
                  mutate(date_asdate = ymd(date)) %>% 
                  mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                  mutate(admin0_name_clean=str_replace(admin0_name_clean,"chinamainland","china")) %>%
                
                  mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%

                  mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%

                  mutate(admin3_name_clean=admin3_name_original %>% rex_clean()) %>%
                  rex_admin_function()

forjoining_metabiota <- metabiota_long %>% 
                        filter(is.na(admin3_name_original) | admin3_name_original=='') %>% #we don't handle cities correctly so we have to exclude them here for now %>%
                        dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths) %>% 
                        distinct()

test <- forjoining_metabiota %>% group_by(dataset,gid, geonameid, wikidata_id,date_asdate) %>% mutate(n=n()) %>% ungroup() #the entire problem is that we're not handling cities correctly.

#The problem is ISO31662 is merging up multiple times
#ISO31662


#Drop any with no variation

#https://data.humdata.org/dataset/2019-novel-coronavirus-cases
#Spatiotemporal data for 2019-Novel Coronavirus Covid-19 Cases and deaths
#This dataset is part of COVID-19 Pandemic
#Data Overview
#This repository contains spatiotemporal data from many official sources for 2019-Novel Coronavirus beginning 2019 in Hubei, China ("nCoV_2019")
#You may not use this data for commercial purposes. If there is a need for commercial use of the data, please contact Metabiota at info@metabiota.com to obtain a commercial use license.

```

All the relevant data used in this paper are publicy available and accessible at
https://lab.gedidigital.it/gedi-visual/2020/coronavirus-i-contagi-in-italia/

## Wikipedia national testing page


```{r}

#https://webapps.stackexchange.com/questions/35822/get-the-version-of-a-wikipedia-page-from-a-specific-date
#Go to the history page of your desired page, and then add &YYYYMMDDHHMMSS to the URL.
#http://en.wikipedia.org/w/index.php?title=COVID-19_testing&action=history&offset=20200415000000
#&limit=1&action=history  #limits it further to just one link
library(rvest)
url <- "http://en.wikipedia.org/w/index.php?title=COVID-19_testing&action=history&offset=20200415000000&limit=1&action=history"
wikipedia_history_page <- read_html(url)

#https://en.wikipedia.org/w/index.php?title=COVID-19_testing&oldid=950235147

#Aim at the template not the page
#https://en.wikipedia.org/w/index.php?title=Template:COVID-19_testing&action=history

#The first appearence is feb 25 2020
month_start=3
month_end=4
dates_to_grab <- apply(expand.grid(str_pad(month_start:month_end, 2, pad = "0") , str_pad(1:31, 2, pad = "0") ), 1, paste, collapse="")
histor_links_list <- list()
for(q in dates_to_grab){
  url <- paste0("https://en.wikipedia.org/w/index.php?title=Template:COVID-19_testing&action=history&offset=2020",q,"000000&limit=1&action=history")
  print(url)
  wikipedia_history_page <- read_html(url)
  links <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('href'))
  titles <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('title')) 
  text <- wikipedia_history_page %>% html_nodes("a") %>% html_text() 
  wikipedia_history_page_links <- data.frame(link=paste0("https://en.wikipedia.org/",links) , 
                                             title=titles,
                                             text=text) %>%
                                  filter(str_detect(link,"oldid") & text!="cur" & text!="prev")
  histor_links_list[[q]] <- wikipedia_history_page_links
}

history_links_df <- bind_rows(histor_links_list)

#Ah it's a fixed table that just gets included so scraping the whole page doesn't show any variation

tables_list <- list()
for(q in history_links_df$link %>% unique() ){
  print(q)
  thepage = readLines(q)
  #temp_html <- read_html(q)
  #temp_tables <- temp_html  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=F) 

  library(htmltab) ; #install.packages('htmltab')
  temp_tables <- read_html(q)  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=T) 
  try({
    for(h in temp_tables){
        #table_temp <- htmltab(doc = thepage, which = i, complementary=F, headerSep="_") #,header=1 htmltab doesn't just fail it crashes R which is sad because I liked it better
        if( 
          names(h) %>% tolower() %>% str_detect( c("country","tests","positive")) %>% any() #had to take out date because it triggers on "update"
          ){
          tables_list[[q]] <- h
          print("Table Found")
          break
        }
    }
  })
}
length(tables_list)
lapply(tables_list, names) #41 snapshots all with the same names god bless them

library(janitor) #install.packages("janitor")

wikipedia_national_test <- bind_rows(lapply(tables_list, clean_names))

wikipedia_national_test_long <- wikipedia_national_test %>% rowwise() %>% 
                                mutate(dataset="wikipedia") %>%
                                mutate(tested_people = coalesce(tests,total_tests,totaltests) ) %>%
                                mutate(date = coalesce(as_of,date) ) %>%
                                mutate(place = coalesce(country_or_region,country) ) %>% 
                                separate(place, c("admin0_name_original", "admin1_name_original"),sep=":")  %>%

                                mutate(date_asdate1 = dmy(paste0(date,", 2020"))) %>% 
                                mutate(date_asdate2 = mdy(paste0(date,", 2020"))) %>% 
                                mutate(date_asdate = coalesce(date_asdate1,date_asdate2)  ) %>% 
  
                                mutate(admin0_name_clean = admin0_name_original %>% rex_clean()) %>%
                                mutate(admin1_name_clean = admin1_name_original %>% rex_clean()) %>%
                                mutate(admin2_name_clean = '' %>% rex_clean()) %>%
                                rex_admin_function()

forjoining_wikipedia_national_test <- wikipedia_national_test_long %>%   
                                      dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, tested_people) %>% 
                                      mutate(tested_people = str_replace_all(tested_people,",|>| ","") %>% as.numeric()) %>%
                                      distinct()

dim(forjoining_wikipedia_national_test) #

```



```{r}

library(tsibble)

lhs_long <- bind_rows(
  forjoining_CSSE,
  forjoining_who,
  forjoining_bing,
  forjoining_ecdc,
  forjoining_nytimes,
  forjoining_covidtracking,
  forjoining_metabiota,
  forjoining_covid19india,
  forjoining_wikipedia_national_test
)   %>% 
  mutate(confirmed=ifelse(confirmed==0,NA,confirmed)) %>% #we don't believe zeros
  mutate(deaths=ifelse(deaths==0,NA,deaths)) %>%
  filter(!(is.na(confirmed & is.na(deaths) & is.na(tested_people)))) %>%
  distinct() %>%
  arrange(dataset,gid ,  geonameid ,wikidata_id, date_asdate) %>%
  group_by(dataset,gid ,  geonameid ,wikidata_id) %>% 
    mutate(confirmed_fd=confirmed, deaths_fd=deaths, tested_people_fd=tested_people, tested_samples_fd=tested_samples) %>%
    tidyr::fill( ends_with("_fd")) %>%
    mutate_at(vars(ends_with("_fd")), difference) %>%
  ungroup() %>%
  #About 4k of these observations show a decrease from one time step to the next which suggests either an original error or a joining error
  #We're going to straight drop those observations as a cleaning step
  filter( (is.na(confirmed_fd) | confirmed_fd>=0) & 
          (is.na(deaths_fd) | deaths_fd>=0) & 
          (is.na(tested_people_fd) | tested_people_fd>=0) & 
          (is.na(tested_samples_fd) | tested_samples_fd>=0)  
          ) %>%
  #Also reject any where deaths are greater than confirmed
  filter(is.na(confirmed) | is.na(deaths) | confirmed>=deaths)


dim(lhs_long) #178447 #174028 #174691  #179,748

saveRDS(lhs_long, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long.Rds")

#test <- lhs_long %>% group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>% summarize(n=n())

all_na <- function(x) any(!is.na(x)) #https://intellipaat.com/community/12999/remove-columns-from-dataframe-where-all-values-are-na

#bing looks off by a day from the other ones
lhs_wide_qcode <- lhs_long %>% 
                  filter(!is.na(wikidata_id) & !is.na(date_asdate)) %>%
                  group_by(gid, geonameid, wikidata_id, date_asdate) %>%  mutate(confirmed_var=var(confirmed, na.rm=T), deaths_var=var(deaths, na.rm=T)) %>% ungroup() %>%
                  dplyr::select(dataset, gid, geonameid, wikidata_id, date_asdate,confirmed, deaths, tested_samples, tested_people) %>%
                  group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>%  summarise_if(is.numeric,max,na.rm=T) %>% ungroup() %>% #this is a hack, we should have dupes within datasets
                  pivot_wider(names_from = dataset, values_from = c(confirmed, deaths, tested_samples, tested_people) ) %>% 
                  mutate_if(is.numeric, list(~na_if(abs(.), Inf))) %>% #https://stackoverflow.com/questions/12188509/cleaning-inf-values-from-an-r-dataframe
                  select_if(all_na) 

dim(lhs_wide_qcode) #82,157 82k place/day observations

length(unique(lhs_wide_qcode$wikidata_id)) #3697 #almost 4k 

test <- lhs_wide_qcode %>% dplyr::select(starts_with("confirmed")) %>% distinct() 
cor(test, use="pairwise.complete.obs")

lhs_long_median <- lhs_long %>% group_by(gid, geonameid, wikidata_id, date_asdate) %>% summarize_if(is.numeric, median, na.rm=T) %>% dplyr::select(-ends_with("_fd")) %>% #we take the median across observations
                   mutate(CFR=deaths/confirmed)
dim(lhs_long_median)
summary(lhs_long_median)

```


