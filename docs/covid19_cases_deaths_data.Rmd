---
title: "How to be Curious instead of Contrarian About Covid 19 Part 2: Cases, Tests, and Deaths"
output:
  html_notebook:
    toc: yes
date: 
author: 
affiliation: Director, Machine Learning for Social Science Lab, Center for Peace and Security Studies, University of California San Diego
editor_options: 
  chunk_output_type: inline
---

Rex W. Douglass

<style type="text/css">
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
</style>

The good news brigade is back at it, looking for angles with which to interpret count data that support a happy or underdog story.

```{r}
#libraries
library(lubridate)
library(tidyverse)

#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(scales)
library(gghighlight)
library(lubridate)
library(R0)  # consider moving all library commands to top -- this one was in a loop below

library(WikidataR)
library(countrycode)

library(usmap) ; # install.packages('usmap')
data(statepop)
#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(sf)

library(jsonlite)

```

Can start back from here

```{r}
admin0 <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_in/admin0.Rds")
admin1 <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_in/admin1.Rds")
admin2 <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_in/admin2.Rds")

rex_clean <- function(x){ x %>% stringi::stri_trans_general("latin-ascii") %>% stringi::stri_replace_all(regex="[^A-Za-z0-9]","") %>% tolower() } #so individually, each string should be devoid of spaces,noncharacters, and nonlatin

rex_admin_function <- function(x) {
  x %>% 
  left_join(admin0 ) %>%
  left_join(admin1 )  %>%
  left_join(admin2 ) %>%
  mutate(gid=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',gid2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', gid1, gid0))) %>%
  mutate(wikidata_id=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',wikidata_id2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', wikidata_id1, wikidata_id0))) %>%
  mutate(geonameid=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',geonameid2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', geonameid1, geonameid0))) %>%
  mutate(admin_level=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', 1, 0)))
}

```

## Google maps

```{r}
#install.packages("mapsapi")
#library(mapsapi)
#doc = mp_geocode(addresses = "Tel-Aviv", key=) #couldn't get this to work with my api key. intentionally leaving it out here so I don't push it accidentally

```

# LHS

## Country Specific

Country level datasets
If you are interested in knowing country level data, please refer to the following Kaggle datasets:

Switzerland - https://www.kaggle.com/daenuprobst/covid19-cases-switzerland
Indonesia - https://www.kaggle.com/ardisragen/indonesia-coronavirus-cases

### United Kingdom

https://github.com/tomwhite/covid-19-uk-data

We don't correctly handle the NHS regions yet

```{r}



covid_19_uk_data1 <- read_csv("https://raw.githubusercontent.com/tomwhite/covid-19-uk-data/master/data/covid-19-cases-uk.csv") %>%
                     mutate(admin0_name_original="United Kingdom") %>% mutate(admin1_name_original=Country)  %>% mutate(admin2_name_original=Area) 

covid_19_uk_data2 <-read_csv("https://raw.githubusercontent.com/tomwhite/covid-19-uk-data/master/data/covid-19-totals-uk.csv") %>% mutate(admin0_name_original="United Kingdom")

covid_19_uk_data3 <-read_csv("https://raw.githubusercontent.com/tomwhite/covid-19-uk-data/master/data/covid-19-totals-england.csv") %>%
                     mutate(admin0_name_original="United Kingdom") %>% mutate(admin1_name_original="England")

covid_19_uk_data4 <-read_csv("https://raw.githubusercontent.com/tomwhite/covid-19-uk-data/master/data/covid-19-totals-northern-ireland.csv") %>%
                     mutate(admin0_name_original="United Kingdom") %>% mutate(admin1_name_original="Norther Ireland")

covid_19_uk_data5 <-read_csv("https://raw.githubusercontent.com/tomwhite/covid-19-uk-data/master/data/covid-19-totals-scotland.csv") %>%
                     mutate(admin0_name_original="United Kingdom") %>% mutate(admin1_name_original="Scotland")

covid_19_uk_data6 <-read_csv("https://raw.githubusercontent.com/tomwhite/covid-19-uk-data/master/data/covid-19-totals-wales.csv") %>%
                     mutate(admin0_name_original="United Kingdom") %>% mutate(admin1_name_original="Wales")

covid_19_uk_data7 <-read_csv("https://raw.githubusercontent.com/tomwhite/covid-19-uk-data/master/data/covid-19-indicators-uk.csv") %>%
                     mutate(admin0_name_original="United Kingdom") %>% mutate(admin1_name_original=Country) %>%
                     pivot_wider( id_cols = NULL, names_from = Indicator, values_from = Value, values_fill = NULL, values_fn = NULL) 

covid_19_uk_data_long <- bind_rows(
  covid_19_uk_data1,
  covid_19_uk_data2,
  covid_19_uk_data3,
  covid_19_uk_data4,
  covid_19_uk_data5,
  covid_19_uk_data6,
  covid_19_uk_data7
) %>% dplyr::select(Date, admin0_name_original , admin1_name_original , admin2_name_original,  TotalCases, Tests, ConfirmedCases, Deaths)  %>%
                    mutate(date_asdate = ymd(Date)) %>% 
                    mutate(dataset="covid_19_uk") %>%
                    mutate(confirmed = dplyr::coalesce(TotalCases %>% as.numeric() ,ConfirmedCases %>% as.numeric() ) ) %>%
      
                    mutate(admin0_name_clean=admin0_name_original %>%  replace_na('') %>% rex_clean()) %>%
                    mutate(admin1_name_clean=admin1_name_original %>%  replace_na('') %>% rex_clean()) %>%
                    mutate(admin2_name_clean=admin2_name_original %>%  replace_na('') %>% rex_clean()) %>%
                    rex_admin_function()

forjoining_covid_19_uk <- covid_19_uk_data_long %>%
                          dplyr::select(dataset,admin0_name_original, admin1_name_original, admin2_name_original, gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=Deaths, tested_people=Tests)

dim(forjoining_covid_19_uk)

```

## North America

### Canada

https://experience.arcgis.com/experience/2f1a13ca0b29422f9b34660f0b705043/

```{r}

```

### U.S.


#### FluView

Influenz and mortality

https://gis.cdc.gov/grasp/fluview/mortality.html

```{r}

fluview <- read_csv("https://gis.cdc.gov/grasp/fluview/Flu7References/Data/USStates.csv")

```

#### California Hosptilizations

https://github.com/CALmatters/covid-19-california-hospitalizations-data
https://public.tableau.com/profile/ca.open.data#!/vizhome/COVID-19PublicDashboard/Covid-19Hospitals

#### ILILNet

Can download state-week data here
https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html

https://wwwn.cdc.gov/ILINet/

https://www.medrxiv.org/content/10.1101/2020.04.01.20050542v1

https://www.economist.com/graphic-detail/2020/04/11/why-a-study-showing-that-covid-19-is-everywhere-is-good-news?fsrc=scn%2Ftw%2Fte%2Fbl%2Fed%2Ffootprintsoftheinvisibleenemywhyastudyshowingthatcovid19iseverywhereisgoodnewsgraphicdetail&%3Ffsrc%3Dscn%2F=tw%2Fdc


#### NYT


```{r}
#https://www.nytimes.com/article/coronavirus-county-data-us.html
#https://github.com/nytimes/covid-19-data
nytimes <- read_csv(url("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")) #

nytimes_long <- nytimes %>%
                mutate(date_asdate = ymd(date)) %>% 
                rename(confirmed=cases) %>%
                mutate(dataset="nyt") %>%
                mutate(admin0_name_original="United States") %>%
                mutate(admin1_name_original=state) %>%
                mutate(admin2_name_original=county)  %>%

                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                mutate(admin0_name_clean=str_replace(admin0_name_clean,"chinamainland","china")) %>%
              
                mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%

                mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                rex_admin_function()

forjoining_nytimes <- nytimes_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths)

```



#### USA Facts

COVID-19 deaths dataset. USA Facts. Accessed on 03-31-2020 at https://www.reuters.
com/article/us-health-coronavirus-who/covid-19-spread-map
https://www.stat.berkeley.edu/~binyu/ps/papers2020/covid19_paper.pdf
https://github.com/Yu-Group/covid19-severity-prediction/tree/master/data
https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/

USA facts has the buroughs of new york, but it adds the word county to each of them which somehow we don't handle!?! No we do, it's in admin 2
Ok figured it out, wikidata has an entity for the county Bronx County (Q855974) and for the exact same damn thing but as the borough The Bronx (Q18426) and it has a link between the two "coextensive with (P3403)
this item has the same boundary as the target item; area associated with (this) entity is identical with the area associated with that entity"

```{r}

usafacts_confirmed <- read_csv("https://usafactsstatic.blob.core.windows.net/public/data/covid-19/covid_confirmed_usafacts.csv")
usafacts_deaths <- read_csv("https://usafactsstatic.blob.core.windows.net/public/data/covid-19/covid_deaths_usafacts.csv")

usafacts_confirmed_long <-  pivot_longer(usafacts_confirmed, names_to = "date", cols = ends_with("20"), values_to = "confirmed")
usafacts_deaths_long <-  pivot_longer(usafacts_deaths, names_to = "date", cols = ends_with("20"), values_to = "deaths")

usafacts_long <- usafacts_confirmed_long %>% full_join(usafacts_deaths_long) %>%
                  mutate(date_asdate = mdy(str_replace(date,"20$","2020"))) %>%  
                  mutate(dataset="usafacts") %>%
                  mutate(admin0_name_original="United States") %>%
                  mutate(admin1_name_original=State) %>%
                  mutate(admin2_name_original=`County Name`)  %>%
  
                  mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                  mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                  mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                  rex_admin_function()

forjoining_usafacts <- usafacts_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths)


```

#### Covidtracking.com

```{r}
#https://covidtracking.com/about-data
"If you are calculating positive rates, it should only be with states that have an A grade. And be careful going back in time because almost all the states have changed their level of reporting at different times."

#The data contain the u.s. and the states too
#FIPS the first two digits are the state
#All 0 is the U.S.
#
#https://covidtracking.com/api
covidtracking <- read_csv(url("https://covidtracking.com/api/states/daily.csv"))
dim(covidtracking) #1653   25
covidtracking_t <- t(covidtracking)

covidtracking_long <- covidtracking %>%

                      mutate(date_asdate = ymd(date)) %>% 
                      rename(state_abbr=state) %>%
                      left_join(state_codes) %>% 
                      dplyr::select(date_asdate, state_name, state_abbr, positive, negative, death) %>%
                      mutate(dataset="covidtracking") %>%
                      mutate(admin0_name_original="United States") %>%
                      mutate(admin1_name_original=state_name)  %>%
                      mutate(admin2_name_original='')  %>%
  
                      mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                      mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                      mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                      rex_admin_function()

forjoining_covidtracking <- covidtracking_long %>%
                            mutate(tested_people=negative+positive) %>%
                            dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=positive, deaths=death, tested_people)

```

#### Wikipedia

State level confirmed and deaths
https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/United_States_medical_cases

As a cross section
https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/United_States_medical_cases_by_state

## Europe

### Germany

https://npgeo-corona-npgeo-de.hub.arcgis.com/datasets/dd4580c810204019a7b8eb3e0b329dd6_0/data?orderBy=Meldedatum&orderByAsc=false&selectedAttribute=Datenstand

```{r}

rki_covid19 <- read_csv("https://opendata.arcgis.com/datasets/dd4580c810204019a7b8eb3e0b329dd6_0.csv")
names(rki_covid19) <- c("IdBundesland "," Federal state "," District "," Age group "," Gender "," Number of cases "," Number of deaths "," ObjectId "," Registration date "," IdLandkreis "," Data status "," New case "," New deaths ","Ref date", "New recoveries", "Number of recoveries") %>% trimws() %>% tolower() 
rki_covid19 <- rki_covid19 %>% clean_names()

rki_covid19_long <- rki_covid19 %>% 
                    group_by(federal_state,district, registration_date) %>% 
                    summarise(number_of_cases=sum(number_of_cases, na.rm=T), number_of_deaths=sum(number_of_deaths, na.rm=T)) %>%
                    ungroup() %>%
                    group_by(federal_state,district) %>%
                    arrange(registration_date) %>%
                    mutate(number_of_cases=cumsum(number_of_cases), number_of_deaths=cumsum(number_of_deaths)) %>%
                    ungroup() %>%
                    mutate(date_asdate = ymd(registration_date)) %>% 
                    mutate(dataset="robert_koch") %>%
                    mutate(admin0_name_original="Germany") %>%
                    mutate(admin1_name_original=federal_state)  %>%
                    mutate(admin2_name_original=district  )  %>%

                    mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                    mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                    mutate(admin2_name_clean=admin2_name_original %>% str_replace("^[A-Za-z]{1,2} ","") %>% rex_clean()) %>%
                    rex_admin_function()

forjoining_rki_covid19<- rki_covid19_long %>%
                            dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=number_of_cases, deaths=number_of_deaths)


```

### Italy

#### Dati COVID-19 Italia DPC
Dati COVID-19 Italia
Italy - https://www.kaggle.com/sudalairajkumar/covid19-in-italy
https://github.com/pcm-dpc/COVID-19


```{r}

dpc_region <- read.csv("https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv", na.strings = "", fileEncoding = "UTF-8-BOM")
dpc_province <- read.csv("https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-province/dpc-covid19-ita-province-latest.csv", na.strings = "", fileEncoding = "UTF-8-BOM")
dpc_national <- read.csv("https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-andamento-nazionale/dpc-covid19-ita-andamento-nazionale.csv", na.strings = "", fileEncoding = "UTF-8-BOM")

dpc_national_long <- dpc_national %>% 
                    mutate(date_asdate = ymd(substring(data,1,10))) %>%
                    mutate(dataset="dpc") %>% 
  
                    mutate(admin0_name_clean=stato %>% rex_clean()) %>%
                    mutate(admin1_name_clean= '') %>%
                    mutate(admin2_name_clean='') %>%
                    rex_admin_function() 

dpc_region_long <- dpc_region %>% 
                    mutate(date_asdate = ymd(substring(data,1,10))) %>%
                    mutate(dataset="dpc") %>% 
  
                    mutate(admin0_name_clean=stato %>% rex_clean()) %>%
                    mutate(admin1_name_clean=denominazione_regione %>% rex_clean()) %>%
                    mutate(admin2_name_clean='') %>%
                    rex_admin_function() 

dpc_province_long <- dpc_province %>% 
                      mutate(date_asdate = ymd(substring(data,1,10))) %>%
                      mutate(dataset="dpc") %>% 
    
                      mutate(admin0_name_clean=stato %>% rex_clean()) %>%
                      mutate(admin1_name_clean=denominazione_regione %>% rex_clean()) %>%
                      mutate(admin2_name_clean=denominazione_provincia %>% rex_clean()) %>%
                      rex_admin_function() 

forjoining_dpc <- bind_rows(dpc_national_long, dpc_region_long, dpc_province_long) %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate, confirmed=totale_casi, deaths=deceduti, tested_people=casi_testati)


```

### India

#### covid19india

India - https://www.kaggle.com/sudalairajkumar/covid19-in-india

https://api.covid19india.org/
https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#
https://github.com/covid19india/api/blob/master/state_test_data.json

https://github.com/amodm/api-covid19-in

```{r}
#Couldn't get any of this to work
#library(googlesheets4)
#temp <- read_csv("http://api.covid19india.org/states_daily_csv/confirmed.csv")
#temp <- read_sheet("https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#")
#install.packages('gsheet')
#library(gsheet)

# "cases_time_series" "statewise"         "tested"  
covid19india_cases_time_series <- fromJSON("https://api.covid19india.org/data.json")$cases_time_series
covid19india_statewise <- fromJSON("https://api.covid19india.org/data.json")$statewise
covid19india_tested <- fromJSON("https://api.covid19india.org/data.json")$tested

tested_numbers_icmr_data <- read_csv("https://api.covid19india.org/csv/latest/tested_numbers_icmr_data.csv")

raw1 <- read_csv("https://api.covid19india.org/csv/latest/statewise_tested_numbers_data.csv") #state level test data

raw2  <- read_csv("https://api.covid19india.org/csv/latest/state_wise_daily.csv") #state level confirmed dead recovered


#raw3 <- read_csv("https://api.covid19india.org/csv/latest/raw_data.csv") #patient data

#https://github.com/covid19india/api/blob/master/state_test_data.json
#covid19india_test <- fromJSON("https://raw.githubusercontent.com/covid19india/api/master/state_test_data.json")$states_tested_data
#saveRDS(covid19india_test, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/covid19india_test_latest_" %>% paste0(Sys.Date(),".Rds")   ) #start saving daily snapshots that we can add to the pile
library(janitor)
raw1_long  <- raw1 %>% clean_names() %>%
              mutate(date_asdate = dmy(updated_on )) %>%
              mutate(admin0_name_clean="India" %>% rex_clean()) %>%
              mutate(admin1_name_clean= state %>% rex_clean()) %>%
              mutate(admin2_name_clean= "" %>% rex_clean()) %>%
              mutate(dataset="covid19india") %>%
              rex_admin_function()



statecodes <- data.frame(
      stringsAsFactors = FALSE,
          state_abbrev = c("AP","AR","AS","BR",
                           "CT","GA","GJ","HR","HP","JH","KA","KL","MP",
                           "MH","MN","ML","MZ","NL","OR","PB","RJ",
                           "SK","TN","TG","TR","UT","UP","WB","AN","CH",
                           "DN","DD","DL","JK","LA","LD","PY"),
  admin1_name_original = c("Andhra Pradesh",
                           "Arunachal Pradesh","Assam","Bihar","Chhattisgarh",
                           "Goa","Gujarat","Haryana","Himachal Pradesh",
                           "Jharkhand","Karnataka","Kerala","Madhya Pradesh",
                           "Maharashtra","Manipur","Meghalaya","Mizoram",
                           "Nagaland","Odisha","Punjab","Rajasthan","Sikkim",
                           "Tamil Nadu","Telangana","Tripura","Uttarakhand",
                           "Uttar Pradesh","West Bengal",
                           "Andaman and Nicobar Islands","Chandigarh","Dadra and Nagar Haveli",
                           "Daman and Diu","Delhi","Jammu and Kashmir","Ladakh",
                           "Lakshadweep","Puducherry")
)

raw2_lomg <- raw2 %>% pivot_longer(cols=TT:WB, names_to="state_abbrev") %>% pivot_wider(id_cols=matches("Date|state_abbrev") , names_from=Status, values_from=value) %>% 
              left_join(statecodes) %>% 
              clean_names() %>%
              mutate(date_asdate = dmy(date )) %>%
              mutate(admin0_name_clean="India" %>% rex_clean()) %>%
              mutate(admin1_name_clean= admin1_name_original %>% rex_clean()) %>%
              mutate(admin2_name_clean= "" %>% rex_clean()) %>%
              mutate(dataset="covid19india") %>%
              rex_admin_function()


forjoining_covid19india1 <- raw1_long %>% 
                           dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate , tested_people=total_tested)
                          
forjoining_covid19india2 <- raw2_lomg %>% 
                           dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate , confirmed=confirmed , deaths=deceased) %>%
                           group_by(gid, geonameid,wikidata_id) %>% arrange(date_asdate) %>%
                           mutate(confirmed = confirmed %>% replace_na(0) %>% cumsum()) %>%
                           mutate(deaths = confirmed %>% replace_na(0) %>% cumsum()) 
  
forjoining_covid19india <- forjoining_covid19india1 %>% full_join(forjoining_covid19india2)

dim(forjoining_covid19india) #1553

```  

#### IMHFW The Ministry of Health and Family Welfare (IMHFW)

https://github.com/amodm/api-covid19-in

Post Mar 15, data is from The Ministry of Health & Family Welfare
Pre Mar 15, data is sourced from datameet/covid19

```{r}
#construct_download_url(url="https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#", format = "csv", sheetid = 1)
#temp < gsheet2tbl(url="https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#", sheetid = 1)
library(jsonlite)
rootnet_case_json <- fromJSON("https://api.rootnet.in/covid19-in/stats/history")
for(i in 1:length(rootnet_case_json$data$regional)){
  rootnet_case_json$data$regional[[i]]$day <- rootnet_case_json$data$day[i]
}
rootnet_cases_regional <- bind_rows(rootnet_case_json$data$regional) %>%
                           mutate(admin0_name_clean="India" %>% rex_clean()) %>%
                           mutate(admin1_name_clean= loc %>% rex_clean()) %>%
                           mutate(admin2_name_clean= "" %>% rex_clean()) %>%
                          rex_admin_function()

rootnet_cases_national <- rootnet_case_json$data$summary
rootnet_cases_national$day <- rootnet_case_json$data$day

#This is just national testing
rootnet_testing <- fromJSON("https://api.rootnet.in/covid19-in/stats/testing/history")$data

rootnet_national <- rootnet_testing %>%
                     full_join(rootnet_cases_national) %>%
                     mutate(admin0_name_clean="India" %>% rex_clean()) %>%
                     mutate(admin1_name_clean= "" %>% rex_clean()) %>%
                     mutate(admin2_name_clean= "" %>% rex_clean()) %>%
                     rex_admin_function()

imhfw_long <- bind_rows(
                rootnet_national %>% 
                                 rename(confirmed=total, tested_samples=totalSamplesTested, tested_people=totalIndividualsTested) %>% mutate(date_asdate = ymd(day )),
                rootnet_cases_regional %>% 
                                 rename(confirmed=totalConfirmed) %>%
                                 mutate(date_asdate = ymd(day ))
                ) %>%
                mutate(dataset="imhfw") 

forjoining_imhfw <- imhfw_long %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate,
                                                         tested_samples,
                                                         tested_people,
                                                         confirmed=confirmed,
                                                         deaths=deaths
                                                         ) %>% distinct()

dim(forjoining_imhfw) #946


```
### Chile

http://www.minciencia.gob.cl/COVID19
https://github.com/MinCiencia/Datos-COVID19


### Brazil

#### 

https://github.com/turicas/covid19-br
"COVID-19
Coronavirus newsletters and cases by municipality per day"


#### covid19br

https://covid19br.wcota.me/#cidadesTempo
https://raw.githubusercontent.com/wcota/covid19br/master/cases-brazil-cities-time.csv

```{r}


covid19br <- read_csv("https://raw.githubusercontent.com/wcota/covid19br/master/cases-brazil-cities-time.csv")

covid19br_long <- covid19br %>%
                  mutate(date_asdate = ymd(date)) %>% 
                  mutate(dataset="covid19br") %>%
                  mutate(admin0_name_original="Brazil") %>%
                  mutate(admin1_name_original= city %>% str_replace("\\/.*$","")) %>%
                  mutate(admin2_name_original= '') %>%
                  mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                  mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                  mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                  rex_admin_function()

forjoining_covid19br_long <- covid19br_long %>%
                             dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=totalCases, deaths=deaths)


```

#### Brazil Health Ministry
Brazil - https://www.kaggle.com/unanimad/corona-virus-brazil

COVID19
Coronavirus panel
https://covid.saude.gov.br/

following the instructions here I was able to get the direct lnk
https://stackoverflow.com/questions/42901942/how-do-we-download-a-blob-url-video

State codes from here https://en.wikipedia.org/wiki/States_of_Brazil


```{r}

library("clipr")
df <- data.frame(
estado=c("AC", "AL", "AP", "AM", "BA", "CE", "DF", "ES", "GO", "MA", "MT", "MS", "MG", "PA", "PB", "PR", "PE", "PI", "RJ", "RN", "RS", "RO", "RR", "SC", "SP", "SE", "TO"),
admin1_name_original=c("Acre", "Alagoas", "Amapá", "Amazonas", "Bahia", "Ceará", "Distrito Federal", "Espírito Santo", "Goiás", "Maranhão", "Mato Grosso", "Mato Grosso do Sul", "Minas Gerais", "Pará", "Paraíba", "Paraná", "Pernambuco", "Piauí", "Rio de Janeiro", "Rio Grande do Norte", "Rio Grande do Sul", "Rondônia", "Roraima", "Santa Catarina", "São Paulo", "Sergipe", "Tocantins")
)

bhm <- read_delim("https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/4f4bc815e56dc052ccd547b877f56647_Download_COVID19_20200420.csv", delim=";")

bhm_long <- bhm %>%
                mutate(date_asdate = ymd(data)) %>% 
                mutate(dataset="bhm") %>%
                mutate(admin0_name_original="Brazil") %>%
                #mutate(admin1_name_original= regiao) %>%
                mutate(admin2_name_original= '') %>%
                left_join(df) %>%
                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                rex_admin_function()

forjoining_bhm <- bhm_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=casosAcumulados, deaths=obitosAcumulados)


```

### China

Qualitative
https://time.com/5813628/china-coronavirus-statistics-wuhan/
The Chinese time series holds off reporting on numbers until a major event like replacing party leadership in Hbei and Wuhan, or removing the Shandong provicial justice chief.
https://www.economist.com/graphic-detail/2020/04/07/chinas-data-reveal-a-puzzling-link-between-covid-19-cases-and-political-events

### South Korea

#### DS4C: Data Science for COVID-19 in South Korea

South Korea - https://www.kaggle.com/kimjihoo/coronavirusdataset
https://github.com/jihoo-kim/Data-Science-for-COVID-19

```{r}
#national
ds4c_national <- read_csv("https://raw.githubusercontent.com/jihoo-kim/Data-Science-for-COVID-19/master/dataset/Time/Time.csv")

#province
ds4c_province <- read_csv("https://raw.githubusercontent.com/jihoo-kim/Data-Science-for-COVID-19/master/dataset/Time/TimeProvince.csv")


ds4c_long <- bind_rows(ds4c_national, ds4c_province) %>%
                mutate(date_asdate = ymd(date)) %>% 
                mutate(dataset="ds4c") %>%
                mutate(admin0_name_original="South Korea") %>%
                mutate(admin1_name_original= province) %>%
                mutate(admin2_name_original='')  %>%
                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                rex_admin_function()

forjoining_ds4c <- ds4c_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deceased, tested_people=test)

```

### Japan

####

https://github.com/kaz-ogiwara/covid19
"from the press release data of the Ministry of Health, Labor and Welfare."

####
https://github.com/reustle/covid19japan
https://github.com/reustle/covid19japan-data/

```{r}

#covid19japan <- fromJSON("https://raw.githubusercontent.com/reustle/covid19japan-data/master/docs/summary/2020-04-21.json")
covid19japan <- fromJSON("https://data.covid19japan.com/summary/latest.json")

covid19japan_national   <- covid19japan$daily
covid19japan_prefecture <- covid19japan$prefectures #it has counts folded up inside them

temp <- covid19japan_prefecture
temp$confirmedByCity <- NULL

rawdata_df <- as.data.frame(temp$dailyConfirmedCount)
colnames(rawdata_df) <- temp$name
rawdata_df$date <- ymd("2020-01-08"):(ymd("2020-01-08")+nrow(rawdata_df)-1) %>% as_date()

library(tidyselect)
covid19japan_prefecture_long <- pivot_longer(rawdata_df, names_to = "admin1_name_original", cols = Tokyo:Tottori, values_to = "confirmed") %>% #-ends_with("date")
                                mutate(date_asdate = ymd(date)) %>% dplyr::select(-date) %>%
                                mutate(admin0_name_original="Japan") %>%
                                mutate(admin2_name_original=admin1_name_original)  %>%
                                mutate(admin2_name_original='')  %>%
                                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                                mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                                mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                                rex_admin_function() %>%
                                group_by(gid, geonameid, wikidata_id) %>% arrange(date_asdate) %>%
                                mutate(confirmedCumulative=confirmed %>% replace_na(0) %>% cumsum())  %>% ungroup()

covid19japan_national_long <- covid19japan_national %>%
                               mutate(date_asdate = ymd(date)) %>% dplyr::select(-date) %>%
                               mutate(admin0_name_original="Japan") %>%
                               mutate(admin1_name_original='')  %>%
                               mutate(admin2_name_original='')  %>%
                               mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                               mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                               mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                               rex_admin_function()

forjoining_covid19japan <- bind_rows(covid19japan_national_long,covid19japan_prefecture_long) %>% 
                           mutate(dataset="covid19japan") %>%
                           dplyr::select(dataset,gid, geonameid, wikidata_id, date_asdate, confirmed=confirmedCumulative, deaths=deceasedCumulative, tested_people=testedCumulative)

```

### Tunesia

https://github.com/kik00/TnCovid-19
"All Data is based On The offcial press release from the tunisian ministry of health"

```{r}

TnCovid <- fromJSON("https://raw.githubusercontent.com/kik00/TnCovid-19/master/data/dailyPerState.json")

TnCovid_long <- TnCovid %>%
                mutate(date_asdate = ymd(date)) %>% 
                 mutate(admin0_name_original="Tunesia") %>%
                 mutate(admin1_name_original=name)  %>%
                 mutate(admin2_name_original='')  %>%
                 mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                 mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                 mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                 rex_admin_function()

forjoining_TnCovid <-TnCovid_long %>% 
                     mutate(dataset="TnCovid") %>%
                     dplyr::select(dataset,gid, geonameid, wikidata_id, date_asdate, confirmed=cases, deaths=deaths, tested_people=tests)


```


### Switzerland

https://github.com/openZH/covid_19

```{r}

openZH <- read_csv("https://raw.githubusercontent.com/openZH/covid_19/master/COVID19_Fallzahlen_CH_total_v2.csv")

openZH_long <- openZH %>%
                mutate(date_asdate = ymd(date)) %>% 
                 mutate(admin0_name_original="Switzerland") %>%
                 mutate(admin1_name_original=abbreviation_canton_and_fl)  %>%
                 mutate(admin2_name_original='')  %>%
                 mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                 mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                 mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                 rex_admin_function()

forjoining_openZH <-openZH_long %>% 
                     mutate(dataset="openZH") %>%
                     dplyr::select(dataset,gid, geonameid, wikidata_id, date_asdate, confirmed=ncumul_conf, deaths=ncumul_deceased, tested_people=ncumul_tested)

```

### South Africa

####


https://github.com/dsfsi/covid19za

```{r}

#https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_provincial_cumulative_timeline_confirmed.csv
#https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_provincial_cumulative_timeline_deaths.csv
#https://raw.githubusercontent.com/dsfsi/covid19za/master/data/covid19za_timeline_testing.csv


openZH <- read_csv("https://raw.githubusercontent.com/openZH/covid_19/master/COVID19_Fallzahlen_CH_total_v2.csv")

openZH_long <- openZH %>%
                mutate(date_asdate = ymd(date)) %>% 
                 mutate(admin0_name_original="Switzerland") %>%
                 mutate(admin1_name_original=abbreviation_canton_and_fl)  %>%
                 mutate(admin2_name_original='')  %>%
                 mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                 mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                 mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                 rex_admin_function()

forjoining_openZH <-openZH_long %>% 
                     mutate(dataset="openZH") %>%
                     dplyr::select(dataset,gid, geonameid, wikidata_id, date_asdate, confirmed=ncumul_conf, deaths=ncumul_deceased, tested_people=ncumul_tested)


```

#### Wikipedia

```{r}

#Example static wikipedia table - cross section

library(rvest)

q ="https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/South_Africa_medical_cases"

library(htmltab) ; #install.packages('htmltab')
temp_tables <- read_html(q)  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=NA) 
tables_list <- NULL
try({
  for(h in temp_tables){
      #table_temp <- htmltab(doc = thepage, which = i, complementary=F, headerSep="_") #,header=1 htmltab doesn't just fail it crashes R which is sad because I liked it better
      if( 
        names(h) %>% tolower() %>% str_detect( pattern=c("confirmed")) %>% any() #it has a stupid extra header that hangs it up, so added search term for the header
        ){
        tables_list[[q]] <- h
        tables_list[[q]]$edit_date <- edit_date
        print("Table Found")
        break
      }
  }
})

wikipedia_subnational_south_africa <-  t(tables_list[[1]])  
provinces <- rownames(wikipedia_subnational_south_africa)
wikipedia_subnational_south_africa <- wikipedia_subnational_south_africa %>%  as.data.frame()
colnames(wikipedia_subnational_south_africa) <- wikipedia_subnational_south_africa[1,] %>% unlist() %>% as.character()
wikipedia_subnational_south_africa$provinces <- provinces
wikipedia_subnational_south_africa$Date <- NULL
wikipedia_subnational_south_africa$`Notes:` <- NULL
names(wikipedia_subnational_south_africa)[names(wikipedia_subnational_south_africa)==''] <- paste0("trash",1:(  (names(wikipedia_subnational_south_africa)=="") %>% sum()  ))
wikipedia_subnational_south_africa$`2020` <- NULL
wikipedia_subnational_south_africa$`trash1` <- NULL
wikipedia_subnational_south_africa$`trash2` <- NULL


df <- data.frame(
  provinces_fullname=c("Eastern Cape", "Free State", "Gauteng", "KwaZulu-Natal", "Limpopo", "Mpumalanga", "Northern Cape", "North-West", "Western Cape"),
  provinces=c("EC", "FS", "GP", "KZN", "LP", "MP", "NC", "NW", "WC")
)


wikipedia_subnational_south_africa_long <- pivot_longer(wikipedia_subnational_south_africa, names_to = "date", cols = starts_with("0"), values_to = "confirmed") %>% 
                                           mutate(confirmed=confirmed %>% as.character() %>% as.numeric()) %>%
                                           filter(provinces!="Date") %>%
                                           left_join(df) %>% filter(!is.na(provinces_fullname)) %>%
                                           mutate_if(is.character, list(~na_if(., ""))) %>% #dplyr::coalesce may not handle '' correctly
                                           mutate(dataset="wikipedia") %>%
                                           mutate(date_asdate = date %>% paste0("-2020") %>%  mdy() ) %>% 
            
                                            mutate(admin0_name_clean = "South Africa"  %>% rex_clean()) %>%
                                            mutate(admin1_name_clean =  provinces_fullname %>% rex_clean()) %>%
                                            mutate(admin2_name_clean = '' ) %>%
                                            rex_admin_function()

forjoining_wikipedia_subnational_south_africa_long <- wikipedia_subnational_south_africa_long %>%   
                                                      dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed) %>% 
                                                      distinct()

dim(forjoining_wikipedia_subnational_south_africa_long) #



```

### Mexico
  
  https://github.com/carranco-sga/Mexico-COVID-19

```{r}

#https://raw.githubusercontent.com/carranco-sga/Mexico-COVID-19/master/Mexico_COVID19_CTD.csv

```

### Russia

https://github.com/snikitin-de/covid19-russia/tree/master/covid_19_time_series

```{r}

#https://raw.githubusercontent.com/snikitin-de/covid19-russia/master/covid_19_time_series/covid19-russia-cases.csv

```

### Pakistan

https://github.com/ShahrozTanveer/covid-19-pakistan

```{r}

#https://raw.githubusercontent.com/ShahrozTanveer/covid-19-pakistan/master/covid-19-pakistan-data.csv

```

### Nigeria

https://github.com/Kamparia/nigeria-covid19-data

```{r}

#https://raw.githubusercontent.com/Kamparia/nigeria-covid19-data/master/data/csv/ncdc-covid19-states.csv

```

### Bangladesh

Institute of Epidemiology, Disease Control and Research (IEDCR)
http://www.iedcr.gov.bd/

https://github.com/bluedot-bd/covid-19-bd
https://www.kaggle.com/samratkumardey/bangladesh-covid19-daily-dataset
https://github.com/bluedot-bd/covid-19-bd/tree/master/source-csv

```{r}

#laziest thing we can do is just try a bunch of urls and see if he's posted new stuff
#This will need to be updated next month
df_list <- list()
for(i in 9:30){
  #https://raw.githubusercontent.com/bluedot-bd/covid-19-bd/master/source-csv/09-04-2020.csv
  baseurl <- "https://raw.githubusercontent.com/bluedot-bd/covid-19-bd/master/source-csv/"
  middle <- str_pad(i, 2, pad = "0")
  end <- "-04-2020.csv"
  try({
    df_list[[as.character(i)]] <- read_csv(paste0(baseurl, middle, end))
    df_list[[as.character(i)]]$date <- paste0(middle,"-04-2020")
  })
}

iedcr <- bind_rows(df_list)

iedcr_long <- iedcr %>% 
              filter(!is.na(cases)) %>%
              fill(division) %>% #sometimes they don't provide the division but once at the top of each set
                mutate(date_asdate = dmy(date)) %>%
                mutate(dataset="iedcr") %>% 
                
                mutate(admin0_name_clean= "Bangladesh" %>% rex_clean()) %>%
                mutate(admin1_name_clean=division %>% rex_clean() ) %>%
                mutate(admin2_name_clean=district %>% rex_clean() ) %>%
                rex_admin_function() %>%
                group_by(gid, geonameid,wikidata_id) %>% 
                arrange(date_asdate) %>%
                mutate(
                      confirmed=cumsum(cases)#,
                       #deaths=cumsum(deaths)
                       ) %>%
                ungroup()

forjoining_iedcr <- iedcr_long %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate, confirmed=cases)



```

### Ethiopia

```{r}
#https://github.com/Ethiopia-COVID19/Covid19.ET
```

### Philippines

https://github.com/altcoder/ncov-open-data-ph
https://drive.google.com/drive/u/0/folders/10VkiUA8x7TS2jkibhSZK1gmWxFM-EoZP

The phillipines is only reporting 7k. They're literally providing data of every single case

https://drive.google.com/drive/u/0/folders/1zgEKsGhx-5VIz3lHdThWcSDlmo3NI7Gr
"Data is encoded and submitted daily and weekly by hospitals through the DOH
DataCollect BedTracker App. As of April 12, 2020, the response rates for the DOH
DataCollect app are currently only at 30.6% for hospitals and 40.7% for infirmaries."

```{r}

#https://drive.google.com/drive/u/0/folders/10VkiUA8x7TS2jkibhSZK1gmWxFM-EoZP
#https://drive.google.com/open?id=1MLit3Nd8ROrHgGrY3uJ9LpD2tSQ6fGcu
##pdoh <- read_csv("") #this is the 420 drop, it's great but you have to go grab each one sep

#here's it in json form
#https://covid19ph-tracker.herokuapp.com/
#https://raw.githubusercontent.com/raemounz/ph_covid19_tracker/master/src/data/case.json

#These guys update their json pretty regularly so rely on them instead

pdoh <- fromJSON("https://raw.githubusercontent.com/raemounz/ph_covid19_tracker/master/src/data/case.json")



pdoh_confirmed <- pdoh %>% 
                  mutate(confirmed=1) %>%
                  group_by(RegionRes,  DateRepConf) %>% #city was too far, we can't geocode it correctly so we'll take it out and agg at the region ProvCityRes
                  summarize(confirmed = sum(confirmed)) %>%            
                  rename(date=DateRepConf)  %>% 
              ungroup()
 
pdoh_deaths<- pdoh %>% 
              mutate(deaths=1) %>%
              filter(!is.na(DateDied)) %>%
              group_by(RegionRes,  DateDied) %>% #ProvCityRes  #city was too far, we can't geocode it correctly so we'll take it out and agg at the region ProvCityRes
              summarize(deaths = sum(deaths)) %>%            
              rename(date=DateDied) %>% 
              ungroup()

pdoh_long <- pdoh_confirmed %>% 
             full_join(pdoh_deaths) %>%
             mutate(date_asdate = mdy(date)) %>% 
             mutate(admin0_name_original="philippines") %>%
             mutate(admin1_name_original=RegionRes)  %>%
             mutate(admin2_name_original='')  %>% # ProvCityRes #city was too far, we can't geocode it correctly so we'll take it out and agg at the region ProvCityRes
             mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
             mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
             mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
             rex_admin_function() %>%
             mutate(dataset="pdoh") %>% 
             group_by(gid, geonameid, wikidata_id) %>% 
             arrange(date_asdate) %>%
             mutate(confirmed= confirmed %>% replace_na(0) %>% cumsum()) %>%
             mutate(deaths= deaths %>% replace_na(0) %>% cumsum()) %>%
             ungroup() %>%
             filter(wikidata_id!="Q928") #exclude ones that can't be assigned a region because those really aren't the country as a whole

forjoining_pdoh <-pdoh_long %>% 
                     mutate(dataset="TnCovid") %>%
                     dplyr::select(dataset,gid, geonameid, wikidata_id, date_asdate, confirmed=confirmed, deaths=deaths)

```

### Egypt

Gov data is apparently especially sketchy here
https://www.shorouknews.com/news/view.aspx?cdate=03042020&id=924e6078-02ee-4e81-a414-0f4a0066df72
https://www.al-monitor.com/pulse/originals/2020/04/egypt-lockdown-coronavirus-economic-irregular-workers.html
https://www.al-monitor.com/pulse/originals/2020/03/egyptian-superstitions-jokes-on-coronavirus.html

gov figures are like 2k 
http://english.ahram.org.eg/NewsContent/1/64/367128/Egypt/Politics-/UPDATED-Egypt-sees--new-coronavirus-cases,-bringin.aspx

Outside estimate of 6k to 19k
https://www.theguardian.com/world/2020/mar/15/egypt-rate-coronavirus-cases-higher-than-figures-suggest

These leaked figures April 2, of 870
https://www.shorouknews.com/news/view.aspx?cdate=03042020&id=924e6078-02ee-4e81-a414-0f4a0066df72

"Egypt has not conducted PCR tests for all citizens or even for a large number of citizens — as the United States has, for example — so the announced numbers will be much less than the actual cases.
Meanwhile, the government does not announce the number of PCR tests being conducted daily."

### Vietnam

https://www.npr.org/sections/coronavirus-live-updates/2020/04/16/835748673/in-vietnam-there-have-been-fewer-than-300-covid-19-cases-and-no-deaths-heres-why
https://www.reuters.com/article/us-health-coronavirus-vietnam-quarantine/vietnam-quarantines-tens-of-thousands-in-camps-amid-vigorous-attack-on-coronavirus-idUSKBN21D0ZU

"Tens of thousands have been put in quarantine camps. By the end of March, Vietnam had banned all international and domestic flights. The government locked down the country on April 1. State-run media say the current social distancing and stay-at-home orders are to be extended for at least another week.

Streets normally buzzing with motorcycles and cars are almost empty in most large cities. As the economic toll of the lockdown becomes apparent, some entrepreneurs are stepping up to help. One has provided "rice ATMs" to dispense free rice to those who are out of work.

Some may still be skeptical of Vietnam's relatively low COVID-19 case numbers. The CDC's MacArthur is not.

"Our team up in Hanoi is working very, very closely with their Ministry of Health counterparts," he says. "The communications I've had with my Vietnam team is that at this point in time, [they] don't have any indication that those numbers are false.""

Vietnam reporting appears to be rough, no deaths!?!
https://github.com/vncovid19/vncovid19.github.io

https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Vietnam

```{r}

# Vietnam scraping
# First case table was seen on 11th March . We have the data till 24th April for the same.

library(rvest)
library(stringr)
library(dplyr)

month_start=1
month_end=4
dates_to_grab <- apply(expand.grid(str_pad(month_start:month_end, 2, pad = "0") , str_pad(1:31, 2, pad = "0") ), 1, paste, collapse="")

histor_links_list <- list()
for(q in dates_to_grab){
  # Scraping from main wikipedia page , since the actual table does not have a V.T.E format. 
  url <- paste0("https://en.wikipedia.org/w/index.php?title=2020_coronavirus_pandemic_in_Vietnam&action=history&offset=2020",q,"000000&limit=1&action=history")
  print(url)
  wikipedia_history_page <- read_html(url)
  links <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('href'))
  titles <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('title'))
  text <- wikipedia_history_page %>% html_nodes("a") %>% html_text()
  wikipedia_history_page_links <- data.frame(link=paste0("https://en.wikipedia.org/",links) ,
                                             title=titles,
                                             text=text) %>%
    filter(str_detect(link,"oldid") & text!="cur" & text!="prev")
  histor_links_list[[q]] <- wikipedia_history_page_links
}

history_links_df <- bind_rows(histor_links_list) %>% arrange(link) %>% filter(!duplicated(link))


# Adding function to filter out only wikitables from the tables that are fetched. Wikitables are data tables.
filter_wikitables <- function(list_of_nodes)
{
  temp_list = list()
  
  for (i in 1:length(list_of_nodes)){
    s = html_attr(list_of_nodes[i],name='class')
    if(is.na(s)){
      s = ''
    }
    
    if(str_detect(s,'wikitable') == TRUE)
    {
      temp_list = append(temp_list,list_of_nodes[i])
    }
  }
  
  return(temp_list)
}


tables_list <- list()
for(i in 1:nrow(history_links_df) ){
  q <- history_links_df$link[i]
  edit_date <- history_links_df$text[i]
  print(q)

  temp_nodes <- read_html(q)  %>% html_nodes("table") 

  filtered = filter_wikitables(temp_nodes)
  temp_tables = list()
  for(i in 1:length(filtered)) 
  {
    # Added try catch for better control on rvest scraper.
    tryCatch({
      t <- html_table(filtered[[i]],fill = TRUE, header=T)
      temp_tables[[i]] <- t
    },error=function(cond) {
      print("Error in one of the tables. Skipping that")
    })
  }
  
  try({
    for(h in temp_tables){
      if(
        (names(h) %>% tolower() %>% str_detect(pattern=c("city / province")) %>% any(na.rm = TRUE)) && dim(h)[[2]] < 5 
        # Makes sure we have filter out tables with higher dimensions
      ){
        tables_list[[q]] <- h
        tables_list[[q]]$edit_date <- edit_date
        print("Table Found")
        break
      }
    }
  })
}
length(tables_list)
lapply(tables_list, names) 

library(janitor)

vietnam_subnation_cases <- bind_rows(lapply(tables_list, clean_names))
dim(vietnam_subnation_cases)


vietnam_subnation_cases_long <- vietnam_subnation_cases %>% 
                                mutate_if(is.factor, as.character) %>% #factors also break coalesce maybe?
                                mutate_if(is.character, list(~na_if(., ""))) %>% #dplyr::coalesce may not handle '' correctly

                                mutate(dataset="wikipedia") %>%
                                
                                mutate(date = dplyr::coalesce(edit_date ) ) %>%

                                mutate(admin0_name_original = "Vietnam"  ) %>%
                                mutate(admin1_name_original = city_province ) %>%
  
                                mutate(date_asdate = edit_date %>% str_replace(".*?, ","") %>% dmy()) %>% 
  
                                mutate(admin0_name_clean = admin0_name_original %>% rex_clean()) %>%
                                mutate(admin1_name_clean = admin1_name_original %>% rex_clean()) %>%
                                mutate(admin2_name_clean = '' %>% rex_clean()) %>%
                                rex_admin_function()

forjoining_wikipedia_vietnam_subnation_cases <- vietnam_subnation_cases_long %>%   
                                      dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=number_of_cases) %>% 
                                      mutate(confirmed = str_replace_all(confirmed,",|>| ","") %>% as.numeric()) %>%
                                      distinct()

dim(forjoining_wikipedia_vietnam_subnation_cases) #


```

### Congo

Super rough, 143 confirmed cases supposedly
https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_the_Republic_of_the_Congo

```{r}

```

### Turkey

https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Turkey
#### Wikipedia

```{r}

#Example of time varying that you have to scrape the same table over and over again.

#https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/Turkey_medical_cases_by_province

library(rvest)

#https://en.wikipedia.org/wiki/Template:COVID-19_testing_by_country_subdivision
#https://en.wikipedia.org/w/index.php?title=Template:2019%E2%80%9320_coronavirus_pandemic_data/Turkey_medical_cases_by_province&action=history

month_start=4 #starts in april
month_end=4
dates_to_grab <- apply(expand.grid(str_pad(month_start:month_end, 2, pad = "0") , str_pad(1:31, 2, pad = "0") ), 1, paste, collapse="")
histor_links_list <- list()
for(q in dates_to_grab){ 
  url <- paste0("https://en.wikipedia.org/w/index.php?title=Template:2019%E2%80%9320_coronavirus_pandemic_data/Turkey_medical_cases_by_province&action=history&offset=2020",q,"000000&limit=1&action=history")
  print(url)
  wikipedia_history_page <- read_html(url)
  links <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('href'))
  titles <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('title')) 
  text <- wikipedia_history_page %>% html_nodes("a") %>% html_text() 
  wikipedia_history_page_links <- data.frame(link=paste0("https://en.wikipedia.org/",links) , 
                                             title=titles,
                                             text=text) %>%
                                  filter(str_detect(link,"oldid") & text!="cur" & text!="prev")
  histor_links_list[[q]] <- wikipedia_history_page_links
}

history_links_df <- bind_rows(histor_links_list) %>% arrange(link) %>% filter(!duplicated(link))

#Ah it's a fixed table that just gets included so scraping the whole page doesn't show any variation

tables_list <- list()
for(i in 1:nrow(history_links_df) ){
  q <- history_links_df$link[i]
  edit_date <- history_links_df$text[i]
  print(q)
  thepage = readLines(q)
  #temp_html <- read_html(q)
  #temp_tables <- temp_html  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=F) 

  library(htmltab) ; #install.packages('htmltab')
  temp_tables <- read_html(q)  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=NA) 
  try({
    for(h in temp_tables){
        #table_temp <- htmltab(doc = thepage, which = i, complementary=F, headerSep="_") #,header=1 htmltab doesn't just fail it crashes R which is sad because I liked it better
        if( 
          names(h) %>% tolower() %>% str_detect( pattern=c("countr|deaths|case")) %>% any() #it has a stupid extra header that hangs it up, so added search term for the header
          ){
          tables_list[[q]] <- h
          tables_list[[q]]$edit_date <- edit_date
          print("Table Found")
          break
        }
    }
  })
}
length(tables_list)
lapply(tables_list, names) #

library(janitor) #install.packages("janitor")

wikipedia_subnational_turkey <- bind_rows(lapply(tables_list, clean_names))
dim(wikipedia_subnational_turkey)

wikipedia_subnational_turkey_long <- wikipedia_subnational_turkey %>%
                                      mutate_if(is.character, list(~na_if(., ""))) %>% #dplyr::coalesce may not handle '' correctly
                                      mutate(dataset="wikipedia") %>%
                                      rowwise() %>% 

                                      mutate(confirmed = dplyr::coalesce(show_all_vte_covid_19_cases_in_turkey_by_province_2) %>% str_replace_all("\\[.*?\\]|\\(.*?\\)|,","") %>% as.numeric() ) %>%
                                      mutate(tested_people = dplyr::coalesce(show_all_vte_covid_19_cases_in_turkey_by_province_3) %>% str_replace_all("\\[.*?\\]|\\(.*?\\)|,","") %>% as.numeric() ) %>%
    
                                      mutate(place0 = "Turkey" ) %>%
                                      mutate(place1 = dplyr::coalesce(show_all_vte_covid_19_cases_in_turkey_by_province) ) %>%
      
                                      mutate(date = dplyr::coalesce(edit_date) ) %>%
                                      mutate(date_asdate = date %>% str_replace(".*, ","") %>% dmy() ) %>% 
      
                                      mutate(admin0_name_clean =  place0 %>% rex_clean()) %>%
                                      mutate(admin1_name_clean =  place1 %>% rex_clean()) %>%
                                      mutate(admin2_name_clean = '' ) %>%
                                      rex_admin_function()

forjoining_wikipedia_subnational_turkey_long <- wikipedia_subnational_turkey_long %>%   
                                          dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed, tested_people) %>% 
                                          distinct()

dim(forjoining_wikipedia_subnational_turkey_long) #

```

### Iran

#### Wikipedia

```{r}

library(rvest)

#https://en.wikipedia.org/wiki/Template:COVID-19_testing_by_country_subdivision
#https://en.wikipedia.org/w/index.php?title=Template:2019%E2%80%9320_coronavirus_pandemic_data/Turkey_medical_cases_by_province&action=history

q ="https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/Iran_medical_cases"

library(htmltab) ; #install.packages('htmltab')
temp_tables <- read_html(q)  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=NA) 
tables_list <- NULL
try({
  for(h in temp_tables){
      #table_temp <- htmltab(doc = thepage, which = i, complementary=F, headerSep="_") #,header=1 htmltab doesn't just fail it crashes R which is sad because I liked it better
      if( 
        names(h) %>% tolower() %>% str_detect( pattern=c("region")) %>% any() #it has a stupid extra header that hangs it up, so added search term for the header
        ){
        tables_list[[q]] <- h
        tables_list[[q]]$edit_date <- edit_date
        print("Table Found")
        break
      }
  }
})

wikipedia_subnational_iran <-  t(tables_list[[1]])  
regions <- rownames(wikipedia_subnational_iran)
provinces <- wikipedia_subnational_iran[,1]
wikipedia_subnational_iran <- wikipedia_subnational_iran %>%  as.data.frame()
wikipedia_subnational_iran$Date <- NULL
colnames(wikipedia_subnational_iran) <- wikipedia_subnational_iran[1,] %>% unlist() %>% as.character()
wikipedia_subnational_iran$regions <- regions
wikipedia_subnational_iran$provinces <- provinces
wikipedia_subnational_iran$Date <- NULL
wikipedia_subnational_iran$`Notes:` <- NULL

df=bind_rows(
  data.frame(regions="Region 1", province_full=c("Alborz Province", "Golestan Province", "Mazandaran Province", "Qazvin Province", "Qom Province", "Semnan Province", "Tehran Province")),
  data.frame(regions="Region 2", province_full=c("Bushehr Province", "Chaharmahal and Bakhtiari Province", "Fars Province", "Hormozgan Province", "Isfahan Province", "Kohgiluyeh and Boyer-Ahmad Province")),
  data.frame(regions="Region 3", province_full=c("Ardabil Province", "East Azerbaijan Province", "Gilan Province", "Kordestan Province", "West Azerbaijan Province", "Zanjan Province")),
  data.frame(regions="Region 4", province_full=c("Hamadan Province", "Ilam Province", "Kermanshah Province", "Khuzestan Province", "Lorestan Province", "Markazi Province")),
  data.frame(regions="Region 5", province_full=c("Kerman Province", "North Khorasan Province", "Razavi Khorasan Province", "Sistan and Baluchestan Province", "South Khorasan Province", "Yazd Province"))
)
df$province <- substr(df$province_full,1,3)

wikipedia_subnational_iran_long <- pivot_longer(wikipedia_subnational_iran, names_to = "date", cols = starts_with("20"), values_to = "confirmed") %>% 
                                   mutate(confirmed=confirmed %>% as.character() %>% as.numeric()) %>%
                                   filter(provinces!="Date") %>%
                                   left_join(df) %>%
                                    mutate_if(is.character, list(~na_if(., ""))) %>% #dplyr::coalesce may not handle '' correctly
                                    mutate(dataset="wikipedia") %>%
                                    mutate(date_asdate = date %>% ymd() ) %>% 
    
                                    mutate(admin0_name_clean = "Iran"  %>% rex_clean()) %>%
                                    mutate(admin1_name_clean =  province_full %>% rex_clean()) %>%
                                    mutate(admin2_name_clean = '' ) %>%
                                    rex_admin_function()

forjoining_wikipedia_subnational_iran_long <- wikipedia_subnational_iran_long %>%   
                                            dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed) %>% 
                                            distinct()

dim(forjoining_wikipedia_subnational_iran_long) #


```


### Thailand

https://github.com/pluz85/Covid19TH-DailyData

#### Wikipedia 

https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Thailand

```{r}
#https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/Thailand_medical_cases_summary



#https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data/Turkey_medical_cases_by_province

library(rvest)

#https://en.wikipedia.org/wiki/Template:COVID-19_testing_by_country_subdivision
#https://en.wikipedia.org/w/index.php?title=Template:2019%E2%80%9320_coronavirus_pandemic_data/Turkey_medical_cases_by_province&action=history

month_start=3 #starts in april
month_end=4
dates_to_grab <- apply(expand.grid(str_pad(month_start:month_end, 2, pad = "0") , str_pad(1:31, 2, pad = "0") ), 1, paste, collapse="")
histor_links_list <- list()
for(q in dates_to_grab){ 
  url <- paste0("https://en.wikipedia.org/w/index.php?title=Template:2019%E2%80%9320_coronavirus_pandemic_data/Thailand_medical_cases_summary&action=history&offset=2020",q,"000000&limit=1&action=history")
  print(url)
  wikipedia_history_page <- read_html(url)
  links <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('href'))
  titles <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('title')) 
  text <- wikipedia_history_page %>% html_nodes("a") %>% html_text() 
  wikipedia_history_page_links <- data.frame(link=paste0("https://en.wikipedia.org/",links) , 
                                             title=titles,
                                             text=text) %>%
                                  filter(str_detect(link,"oldid") & text!="cur" & text!="prev")
  histor_links_list[[q]] <- wikipedia_history_page_links
}

history_links_df <- bind_rows(histor_links_list) %>% arrange(link) %>% filter(!duplicated(link))

#Ah it's a fixed table that just gets included so scraping the whole page doesn't show any variation

tables_list <- list()
for(i in 1:nrow(history_links_df) ){
  q <- history_links_df$link[i]
  edit_date <- history_links_df$text[i]
  print(q)
  thepage = readLines(q)
  #temp_html <- read_html(q)
  #temp_tables <- temp_html  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=F) 

  library(htmltab) ; #install.packages('htmltab')
  temp_tables <- read_html(q)  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=NA) 
  try({
    for(h in temp_tables){
        #table_temp <- htmltab(doc = thepage, which = i, complementary=F, headerSep="_") #,header=1 htmltab doesn't just fail it crashes R which is sad because I liked it better
        if( 
          names(h) %>% tolower() %>% str_detect( pattern=c("Province|province")) %>% any() #it has a stupid extra header that hangs it up, so added search term for the header
          ){
          tables_list[[q]] <- h
          tables_list[[q]]$edit_date <- edit_date
          print("Table Found")
          break
        }
    }
  })
}
length(tables_list)
lapply(tables_list, names) #

library(janitor) #install.packages("janitor")

wikipedia_subnational_thailand <- bind_rows(lapply(tables_list, clean_names))
dim(wikipedia_subnational_thailand)

wikipedia_subnational_thailand_long <- wikipedia_subnational_thailand %>%
                                      mutate_if(is.character, list(~na_if(., ""))) %>% #dplyr::coalesce may not handle '' correctly
                                      mutate(dataset="wikipedia") %>%

                                      mutate(confirmed = dplyr::coalesce(cases,total) %>% str_replace_all("\\[.*?\\]|\\(.*?\\)|,","") %>% as.numeric() ) %>%

                                      mutate(place0 = "Thailand" ) %>%
                                      mutate(place1 = dplyr::coalesce(province) ) %>%
      
                                      mutate(date = dplyr::coalesce(edit_date) ) %>%
                                      mutate(date_asdate = edit_date %>% str_replace(".*, ","") %>% dmy() ) %>% 
      
                                      mutate(admin0_name_clean =  place0 %>% rex_clean()) %>%
                                      mutate(admin1_name_clean =  place1 %>% rex_clean()) %>%
                                      mutate(admin2_name_clean = '' ) %>%
                                      rex_admin_function()

forjoining_wikipedia_subnational_thailand_long <- wikipedia_subnational_thailand_long %>%   
                                              dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed) %>% 
                                              distinct()

dim(forjoining_wikipedia_subnational_thailand_long) #


```

### Myanmar

https://github.com/theananda/myanmar-covid19-data
https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Myanmar

#### Wikipedia

```{r}

myanmar_testing_wikipedia <- read_csv("https://raw.githubusercontent.com/theananda/myanmar-covid19-data/master/MOHS_Dashboard_Data/MOHS%20Dashboard%20Data-Summary.csv")

myanmar_testing_wikipedia_long <- myanmar_testing_wikipedia %>%
                mutate(date_asdate = dmy(Date)) %>% 
                mutate(admin0_name_original="Myanmar") %>%
                mutate(admin1_name_original='')  %>%
                mutate(admin2_name_original='')  %>%
                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                rex_admin_function()

forjoining_myanmar_testing_wikipedia <-myanmar_testing_wikipedia_long %>% 
                     mutate(dataset="wikipedia") %>%
                     dplyr::select(dataset,gid, geonameid, wikidata_id, date_asdate, confirmed=Confirmed, deaths=Death, tested_people=Tested)


```


### Kenya

https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Kenya

```{r}

```

### Colombia

https://github.com/sandrarairan/Covid_Colombia

```{r}



columbia <- read_csv("https://raw.githubusercontent.com/sandrarairan/Covid_Colombia/master/Casos_positivos_de_COVID-19_en_Colombia.csv")

openZH_long <- openZH %>%
                mutate(date_asdate = ymd(date)) %>% 
                 mutate(admin0_name_original="Switzerland") %>%
                 mutate(admin1_name_original=abbreviation_canton_and_fl)  %>%
                 mutate(admin2_name_original='')  %>%
                 mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                 mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                 mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                 rex_admin_function()

forjoining_openZH <-openZH_long %>% 
                     mutate(dataset="openZH") %>%
                     dplyr::select(dataset,gid, geonameid, wikidata_id, date_asdate, confirmed=ncumul_conf, deaths=ncumul_deceased, tested_people=ncumul_tested)



```

## Africa

### 

### Uganda

https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Uganda

```{r}
```


## Multinational

### trackcorona.live

https://www.trackcorona.live/api

### finddx

```{r}

#had to do this manually but I'm pretty confident we can figure out how to pull from the javascript link directly
#temp <- readLines("https://finddx.shinyapps.io/FIND_Cov_19_Tracker/")
#"<base href=\"_w_9f0ff3c4/\">" this is the w in that url
#finddx_tests <- read_csv("https://finddx.shinyapps.io/FIND_Cov_19_Tracker/_w_ae49f9f4/session/fa13bb507e577e305c9e76fd956f24c1/download/downloadData?w=ae49f9f4")
#write_csv(finddx_tests,"/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/finddx_tests.csv") #save a copy locally just to be safe

finddx_tests <- read_csv("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/finddx_tests.csv")

#China literally has no variance it just keeps repeating the same number over and over again
finddx_tests_long <- finddx_tests %>% 
                      mutate_if(is.factor, as.character) %>% #factors also break coalesce maybe?
                      mutate_if(is.character, list(~na_if(., ""))) %>% #dplyr::coalesce may not handle '' correctly
                      mutate(dataset="finddx") %>%
                      mutate(date_asdate = date %>% ymd() ) %>%

  
                      #separate(place, c("admin0_name_original", "admin1_name_original"),sep=":|  |  |, ")  %>% #seperated by either a colon or a weird double space
                      mutate(admin0_name_original = country   ) %>%
                      mutate(admin1_name_original = ''  ) %>%
                      mutate(admin2_name_original = ''  ) %>%
  
                      mutate(admin0_name_clean = admin0_name_original %>% rex_clean() %>% str_replace("mainland","")  ) %>%
                      mutate(admin1_name_clean = admin1_name_original %>% rex_clean()) %>%
                      mutate(admin2_name_clean = admin2_name_original %>% rex_clean()) %>%
                      rex_admin_function()

forjoining_finddx_tests_long <- finddx_tests_long %>%   
                                      dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, tested_people=tests_cumulative) %>% 
                                      distinct()

```

### worldometers.info

https://www.worldometers.info/coronavirus/#countries

Politely ask internet archive for its best snapshot each day
https://archive.readme.io/docs/website-snapshots

https://archive.org/wayback/available?url=https%3A%2F%2Fwww.worldometers.info%2Fcoronavirus%2F%23countries&timestamp=20200101

```{r}

wayback_base_url <- "https://archive.org/wayback/available?url=https%3A%2F%2Fwww.worldometers.info%2Fcoronavirus%2F%23countries&timestamp=" #add timestamp to the end 20200101

library(rvest)

month_start=1
month_end=4
dates_to_grab <- apply(expand.grid(str_pad(month_start:month_end, 2, pad = "0") , str_pad(1:31, 2, pad = "0") ), 1, paste, collapse="") %>% sort()
dates_to_grab_year <- paste0("2020",dates_to_grab) %>% as.numeric()
wayback_history_page_oldest <- read_json(paste0(wayback_base_url, "20200101")) #search an old date to see what the most recent one is
oldest_date <- wayback_history_page_oldest$archived_snapshots$closest$timestamp %>% substring(1,8) %>% as.numeric()

dates_to_grab_year <- dates_to_grab_year[dates_to_grab_year>=oldest_date] #cuts it down to 96
  
histor_links_list <- list()
for(q in dates_to_grab_year){
  try({
    url <- paste0(wayback_base_url, q)
    print(url)
    wayback_history_page <- read_json(url)
    histor_links_list[[q]] <- wayback_history_page %>% as.data.frame() %>% jsonlite::flatten()
  })
}

history_links_df <- bind_rows(histor_links_list)
nrow(history_links_df)

#Ah it's a fixed table that just gets included so scraping the whole page doesn't show any variation

tables_list <- list()
for(q in history_links_df$archived_snapshots.closest.url %>% unique() %>% as.character() %>% sort() ){
  print(q)
  thepage = readLines(q)
  #temp_html <- read_html(q)
  #temp_tables <- temp_html  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=F) 

  library(htmltab) ; #install.packages('htmltab')
  temp_tables <- read_html(q)  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=T) 
  try({
    for(h in temp_tables){
        #table_temp <- htmltab(doc = thepage, which = i, complementary=F, headerSep="_") #,header=1 htmltab doesn't just fail it crashes R which is sad because I liked it better
        if( 
          names(h) %>% tolower() %>% str_detect( pattern=c("country|cases")) %>% any() #had to take out date because it triggers on "update"
          ){
          tables_list[[q]] <- h
          tables_list[[q]]$url <- q
          print("Table Found")
          break
        }
    }
  })
}
length(tables_list)
#lapply(tables_list, names) #

library(janitor) #install.packages("janitor")

worldometers <- bind_rows(lapply(tables_list, FUN=function(x) x %>% clean_names %>% mutate_if(is.numeric, as.character) ))

worldometers_long <- worldometers %>% 
                    mutate_if(is.factor, as.character) %>% #factors also break coalesce maybe?
                    mutate_if(is.character, list(~na_if(., ""))) %>% #dplyr::coalesce may not handle '' correctly
                    mutate(dataset="worldometers") %>%
                    mutate(date_asdate = url %>% str_replace("http://web.archive.org/web/","") %>% str_replace("/https://www.worldometers.info/coronavirus/","")  %>% substring(1,8) %>% ymd() ) %>%
  
                    mutate(place = dplyr::coalesce( country , country_other , country_territory) ) %>% 
                    mutate(place = str_replace_all(place,"Mainland|mainland","") %>% trimws()  ) %>%  

                    mutate(confirmed = dplyr::coalesce(cases,feb_10_11_cases,feb_11_cases,feb_4_cases,feb_5_cases,feb_6_cases,feb_6_feb_7_cases,feb_8_cases,total_cases) ) %>%
                    mutate(deaths = dplyr::coalesce(deaths, feb_10_11_deaths,feb_11_deaths,feb_4_deaths,feb_5_deaths,feb_6_deaths,feb_6_feb_7_deaths,feb_8_deaths,todays_deaths,total_deaths) ) %>%
                    mutate(tested_people = dplyr::coalesce(total_tests) ) %>%

                    #separate(place, c("admin0_name_original", "admin1_name_original"),sep=":|  |  |, ")  %>% #seperated by either a colon or a weird double space
                    mutate(admin0_name_original = place  ) %>%
                    mutate(admin1_name_original = ''  ) %>%
                    mutate(admin2_name_original = ''  ) %>%

                    mutate(admin0_name_clean = admin0_name_original %>% rex_clean()) %>%
                    mutate(admin1_name_clean = admin1_name_original %>% rex_clean()) %>%
                    mutate(admin2_name_clean = '' %>% rex_clean()) %>%
                    rex_admin_function()

forjoining_worldometers_long <- worldometers_long %>%   
                                      dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed, deaths , tested_people) %>% 
                                      mutate(tested_people = str_replace_all(tested_people,",|>| ","") %>% as.numeric()) %>%
                                      mutate(confirmed = str_replace_all(confirmed,",|>| ","") %>% as.numeric())  %>% #intentionally let +x fail
                                      mutate(deaths = str_replace_all(deaths,",|>| ","") %>% as.numeric()) %>% #intentionally let +x fail
                                      distinct()

dim(forjoining_worldometers_long) #



```

### bnonews
https://bnonews.com/index.php/2020/04/the-latest-coronavirus-cases/

```{r}

```


### ECDC
https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide

Country level

```{r}

#these libraries need to be loaded
library(utils)
#read the Dataset sheet into “R”. The dataset will be called "data".
ecdc <- read.csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv", na.strings = "", fileEncoding = "UTF-8-BOM")

ecdc_log <- ecdc %>% 
  mutate(date_asdate = dmy(dateRep)) %>%
  mutate(dataset="ecdc") %>% 
  
  mutate(admin0_name_clean=countriesAndTerritories %>% rex_clean()) %>%
  mutate(admin1_name_clean='') %>%
  mutate(admin2_name_clean='') %>%
  rex_admin_function() %>%
  group_by(gid, geonameid,wikidata_id) %>% arrange(date_asdate) %>%
  mutate(confirmed=cumsum(cases),
         deaths=cumsum(deaths)) %>%
  ungroup()

forjoining_ecdc <- ecdc_log %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate, confirmed, deaths)


```



### Bing

Bing has moved from their api to this github repo
https://github.com/microsoft/Bing-COVID-19-Data

```{r}

#csv is broken so have to read it in as lines first and then seperate it
temp <- readLines("https://raw.githubusercontent.com/microsoft/Bing-COVID-19-Data/master/data/2020-4-25.csv") #i opened an issue ticket asking them to put latest but for now we'll have to change this every day
temp_df <- as.data.frame(temp[2:length(temp)]) #drop the first line
names(temp_df) <- "V1"
bing <- temp_df %>% 
       separate(col=V1,
                into=c('ID','Updated','Confirmed','ConfirmedChange','Deaths','DeathsChange','Recovered','RecoveredChange','Latitude','Longitude','ISO2','ISO3','Country_Region','AdminRegion1','AdminRegion2'),
                sep=",",
                remove = TRUE
                )
dim(bing)

#bing <- read_delim(temp,delim=",") #Warning: 54257 parsing failures.
#problems(bing)

#library(jsonlite)
#library(rjson)
#tmp2 <- tempfile()
#curl_download("https://bing.com/covid/data", tmp2)
#bing1 <- fromJSON("https://web.archive.org/web/20200407032518if_/https://bing.com/covid/data")
#bing2 <-  fromJSON("https://web.archive.org/web/20200414040859if_/https://bing.com/covid/data")
#bing3 <- fromJSON("https://web.archive.org/web/20200331030201if_/https://bing.com/covid/data")
#bing_latest <- fromJSON("https://bing.com/covid/data")
#saveRDS(bing_latest, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/bing_latest_" %>% paste0(Sys.Date(),".Rds")   ) #start saving dayly snapshots that we can add to the pile
#areas <- bind_rows(
#  bing1$areas,
#  bing2$areas,
#  bing3$areas,
#  bing_latest$areas,
#)

#temp1 <- bing1$areas %>% dplyr::select(-areas)
#temp2 <- bing2$areas %>% dplyr::select(-areas)
#temp3 <- bing3$areas %>% dplyr::select(-areas)
#temp4 <- bing_latest$areas %>% dplyr::select(-areas)

#bing_df0 <- bind_rows(areas) %>% mutate(admin0_name_original=id)
#dim(bing_df0) #835 14

#bing_df1 <- bind_rows(bing_df0$areas) %>% separate(id, c("admin1_name_original", "admin0_name_original")) 
#dim(bing_df1) #753  13

#bing_df2 <- bind_rows(bing_df1$areas) %>% separate(id, c("admin2_name_original","admin1_name_original", "admin0_name_original")) 
#dim(bing_df2) #2872   13

#bing_long <- bind_rows(
#          bing_df0 %>% dplyr::select(-areas),
#          bing_df1 %>% dplyr::select(-areas),
#          bing_df2 %>% dplyr::select(-areas)
#) %>% 

bing_long <- bing %>% clean_names() %>%
  mutate(date_asdate = mdy(updated)) %>%
  mutate(dataset="bing") %>% 
  
  mutate(admin0_name_clean=country_region %>% rex_clean()) %>%
  mutate(admin0_name_clean=str_replace(admin0_name_clean,"chinamainland","china")) %>%
  mutate(admin1_name_clean=admin_region1 %>% rex_clean()) %>%
  mutate(admin2_name_clean=admin_region2 %>% rex_clean()) %>%
  rex_admin_function()

forjoining_bing <- bing_long %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths) %>%
                   mutate(confirmed= confirmed %>% as.numeric() ) %>%
                   mutate(deaths= deaths %>% as.numeric() ) 
  
dim(forjoining_bing)

#Some of these are wrong
#russia khanty mansi  is a second level administrative division, it and a bunch of other ones from russia are listed as first order

#other are ambigious like
#Russia Voronezh might either be the city or the Oblast


```

### Our World In Data

```{r}

owid <- read_csv(url("https://covid.ourworldindata.org/data/owid-covid-data.csv")) 
owid$tests_units %>% table()
owid_long <- owid %>%
              mutate(dataset="owid") %>%
              mutate(admin0_name_original=location) %>%
              mutate(date_asdate = ymd(date)) %>% 

              mutate(tested_people=ifelse(tests_units %in% c('cases tested','people tested'),total_tests, NA))       %>%                         
              mutate(tested_samples=ifelse(!tests_units %in% c('cases tested','people tested'),total_tests, NA))      %>%                           
  
              mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
              mutate(admin1_name_clean='') %>%
              mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
              rex_admin_function()

forjoining_owid <- owid_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=total_cases, deaths=total_deaths, tested_people, tested_samples)

```

### WHO

We're going with WHO's data typed up and cleaned by our world in data though they don't trust them anymore and switched over
Our World in Data
Type of the WHO charts as well as others
https://github.com/owid/covid-19-data/tree/master/public/data

"who: data from the World Health Organization, related to confirmed cases and deaths—we have stopped using and updating this data since 18 March 2020."

```{r}

who_owid <- read_csv(url("https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/who/full_data.csv"))  #

who_owid_long <- who_owid %>%
            mutate(dataset="who_owid") %>%
            mutate(admin0_name_original=location) %>%
            mutate(date_asdate = ymd(date)) %>% 

            mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
            mutate(admin1_name_clean='') %>%
            mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
            rex_admin_function()

forjoining_who_owid <- who_owid_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=total_cases, deaths=total_deaths)

```

### Who from the WHO Dashboard

https://who.sprinklr.com/
You have to manually click on the download map data link because it's a leaflet javascript link. Trying to figure out how to do it auto.

https://www.cdc.gov/nchs/nvss/vsrr/COVID19/index.htm

state level data

https://www.cdc.gov/6d30bf37-caf8-4906-8eb1-e848d064534f

https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/cases-in-us.html
"As of April 14, 2020, CDC case counts and death counts include both confirmed and probable cases and deaths. This change was made to reflect an interim COVID-19 position statementpdf iconexternal icon issued by the Council for State and Territorial Epidemiologists on April 5, 2020. The position statement included a case definition and made COVID-19 a nationally notifiable disease."
*Total cases includes 1,696 probable cases and total deaths includes 4,752 probable deaths. #not a large number though


```{r}

#https://data.humdata.org/dataset/coronavirus-covid-19-cases-and-deaths
who <- read_csv(url("https://docs.google.com/spreadsheets/d/e/2PACX-1vSe-8lf6l_ShJHvd126J-jGti992SUbNLu-kmJfx1IRkvma_r4DHi0bwEW89opArs8ZkSY5G2-Bc1yT/pub?gid=0&single=true&output=csv"))  #

who_long <- who %>%
            mutate(dataset="who") %>%
            mutate(admin0_name_original=ADM0_NAME) %>%
            mutate(date_asdate = ymd(date_epicrv)) %>% 

            mutate(admin0_name_clean=ADM0_NAME %>% rex_clean()) %>%
            mutate(admin1_name_clean='') %>%
            mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
            rex_admin_function()

forjoining_who <- who_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=CumCase, deaths=CumDeath)

```

National Center for Health and Statistics
https://www.cdc.gov/nchs/nvss/covid-19.htm

Have state by state pages
https://www.worldometers.info/coronavirus/country/us/

### CSSEGISandData

Johns Hopkins
https://bnonews.com/index.php/2020/04/the-latest-coronavirus-cases/

```{r, echo=F, message=FALSE, results = FALSE, warning=FALSE}

#https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/
CSSE_confirmed_us      <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv"))
CSSE_confirmed_global  <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"))

CSSE_deaths_us      <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv"))
CSSE_deaths_global  <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv"))

CSSE_confirmed_us_long <- pivot_longer(CSSE_confirmed_us, names_to = "date", cols = ends_with("20"), values_to = "confirmed")
CSSE_confirmed_global_long <- pivot_longer(CSSE_confirmed_global, names_to = "date", cols = ends_with("20"), values_to = "confirmed")

CSSE_deaths_us_long <- pivot_longer(CSSE_deaths_us, names_to = "date", cols = ends_with("20"), values_to = "deaths")
CSSE_deaths_global_long <- pivot_longer(CSSE_deaths_global, names_to = "date", cols = ends_with("20"), values_to = "deaths")

CSSE_us_long <- CSSE_confirmed_us_long %>% full_join(CSSE_deaths_us_long) %>% mutate(admin0_name_original="United States") %>% mutate(admin1_name_original=Province_State) %>% mutate(admin2_name_original=Admin2)
CSSE_global_long <- CSSE_confirmed_global_long %>% full_join(CSSE_deaths_global_long) %>% mutate(admin0_name_original=`Country/Region`) %>% mutate(admin1_name_original=`Province/State`)

CSSE_long <- bind_rows(CSSE_us_long, CSSE_global_long) %>% 
              mutate(dataset="CSSE") %>%
              mutate(date_asdate = mdy(str_replace(date,"20$","2020"))) %>%

              mutate(admin0_name_clean=admin0_name_original %>% rex_clean())  %>%
              mutate(admin1_name_clean=admin1_name_original %>% rex_clean())  %>%
              mutate(admin2_name_clean=admin2_name_original %>% rex_clean())  %>%
              rex_admin_function() %>%
              distinct()

test <- CSSE_long %>% group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>% mutate(n=n()) %>% ungroup() #the dupes are coming either from NA location/date info or deaths and confirmed appearing as two seperate records


forjoining_CSSE <- CSSE_long %>% 
                   dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths) %>%
                   group_by(dataset, gid, geonameid, wikidata_id,date_asdate) %>% summarize(confirmed=max(confirmed, na.rm=T), deaths=max(deaths, na.rm=T))

test <- forjoining_CSSE %>% group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>% mutate(n=n()) %>% ungroup() #the dupes are coming either from NA location/date info or deaths and confirmed appearing as two seperate records


```


### metabiota.com

```{r}

metabiota <- read_csv("/home/skynet2/Downloads/data_ncov2019.csv")
dim(metabiota) #605848

table(metabiota$OUTCOME)
table(metabiota$CUMULATIVE_FLAG)

#cities look broken with just a flat number of 1 or 2 "Ho Chi Minh City"

metabiota_confirmed <- metabiota %>% filter(CUMULATIVE_FLAG==T) %>% filter(OUTCOME=="CASE") %>% filter(CONFIRM_STATUS=="CONFIRMED") %>% 
                       filter( DATE_LOW==DATE_HIGH) %>%
                       mutate(composite_source = SOURCE=="Metabiota Composite Source") %>%
                       distinct() %>%
                       arrange(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW, -composite_source) %>%
                       group_by(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW) %>%
                       filter(row_number()==1) %>%
                       ungroup() %>%
                       dplyr::select(date=DATE_REPORT,
                                          AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME,LOCALITY_NAME,confirmed=VALUE) 

#test <- metabiota_confirmed %>% group_by(AL0_NAME,AL1_CODE, AL1_NAME, AL2_NAME, AL3_NAME, LOCALITY_NAME, DATE_LOW) %>% mutate(n=n()) %>% ungroup() #the entire problem is that we're not handling cities correctly.


metabiota_deaths <- metabiota %>% filter(CUMULATIVE_FLAG==T) %>% filter(OUTCOME=="DEATH") %>% filter(CONFIRM_STATUS=="CONFIRMED") %>% 
                       filter( DATE_LOW==DATE_HIGH) %>%
                       mutate(composite_source = SOURCE=="Metabiota Composite Source") %>%
                       distinct() %>%
                       arrange(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW, -composite_source) %>%
                       group_by(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW) %>%
                       filter(row_number()==1) %>%
                       ungroup() %>%
                       dplyr::select(date=DATE_REPORT, #DATE_LOW #changing this to date of the report to align it with other 
                                     AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME,LOCALITY_NAME,deaths=VALUE) 

metabiota_long <- metabiota_confirmed %>% 
                  full_join(metabiota_deaths) %>%
                  group_by(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME) %>%
                  filter(min(confirmed)!=max(confirmed)) %>% #require some variation
                  ungroup() %>%
                  rename(admin0_name_original=AL0_NAME,
                         admin1_name_original=AL1_NAME,
                         admin2_name_original=AL2_NAME,
                         admin3_name_original=LOCALITY_NAME
                         ) %>%
                  dplyr::select(-AL3_NAME) %>%
                  mutate(dataset="metabiota") %>%
                  mutate(date_asdate = ymd(date)) %>% 
                  mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                  mutate(admin0_name_clean=str_replace(admin0_name_clean,"chinamainland","china")) %>%
                
                  mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%

                  mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%

                  mutate(admin3_name_clean=admin3_name_original %>% rex_clean()) %>%
                  rex_admin_function()

forjoining_metabiota <- metabiota_long %>% 
                        filter(is.na(admin3_name_original) | admin3_name_original=='') %>% #we don't handle cities correctly so we have to exclude them here for now %>%
                        dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths) %>% 
                        distinct()

test <- forjoining_metabiota %>% group_by(dataset,gid, geonameid, wikidata_id,date_asdate) %>% mutate(n=n()) %>% ungroup() #the entire problem is that we're not handling cities correctly.

#The problem is ISO31662 is merging up multiple times
#ISO31662


#Drop any with no variation

#https://data.humdata.org/dataset/2019-novel-coronavirus-cases
#Spatiotemporal data for 2019-Novel Coronavirus Covid-19 Cases and deaths
#This dataset is part of COVID-19 Pandemic
#Data Overview
#This repository contains spatiotemporal data from many official sources for 2019-Novel Coronavirus beginning 2019 in Hubei, China ("nCoV_2019")
#You may not use this data for commercial purposes. If there is a need for commercial use of the data, please contact Metabiota at info@metabiota.com to obtain a commercial use license.

```

All the relevant data used in this paper are publicy available and accessible at
https://lab.gedidigital.it/gedi-visual/2020/coronavirus-i-contagi-in-italia/

### Wikipedia 

#### National Testing Table

```{r}

#https://webapps.stackexchange.com/questions/35822/get-the-version-of-a-wikipedia-page-from-a-specific-date
#Go to the history page of your desired page, and then add &YYYYMMDDHHMMSS to the URL.
#http://en.wikipedia.org/w/index.php?title=COVID-19_testing&action=history&offset=20200415000000
#&limit=1&action=history  #limits it further to just one link
library(rvest)
url <- "http://en.wikipedia.org/w/index.php?title=COVID-19_testing&action=history&offset=20200415000000&limit=1&action=history"
wikipedia_history_page <- read_html(url)

#https://en.wikipedia.org/w/index.php?title=COVID-19_testing&oldid=950235147

#Aim at the template not the page
#https://en.wikipedia.org/w/index.php?title=Template:COVID-19_testing&action=history

#The first appearence is feb 25 2020
month_start=3
month_end=4
dates_to_grab <- apply(expand.grid(str_pad(month_start:month_end, 2, pad = "0") , str_pad(1:31, 2, pad = "0") ), 1, paste, collapse="") %>% sort()
histor_links_list <- list()
for(q in dates_to_grab){
  url <- paste0("https://en.wikipedia.org/w/index.php?title=Template:COVID-19_testing_by_country&action=history&offset=2020",q,"000000&limit=1")
  print(url)
  wikipedia_history_page <- read_html(url)
  links <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('href'))
  titles <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('title')) 
  text <- wikipedia_history_page %>% html_nodes("a") %>% html_text() 
  wikipedia_history_page_links <- data.frame(link=paste0("https://en.wikipedia.org/",links) , 
                                             title=titles,
                                             text=text) %>%
                                  filter(str_detect(link,"oldid") & text!="cur" & text!="prev")
  histor_links_list[[q]] <- wikipedia_history_page_links
}

history_links_df <- bind_rows(histor_links_list)

#Ah it's a fixed table that just gets included so scraping the whole page doesn't show any variation

tables_list <- list()
for(q in history_links_df$link %>% unique() ){
  print(q)
  thepage = readLines(q)
  #temp_html <- read_html(q)
  #temp_tables <- temp_html  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=F) 

  library(htmltab) ; #install.packages('htmltab')
  temp_tables <- read_html(q)  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=T) 
  try({
    for(h in temp_tables){
        #table_temp <- htmltab(doc = thepage, which = i, complementary=F, headerSep="_") #,header=1 htmltab doesn't just fail it crashes R which is sad because I liked it better
        if( 
          names(h) %>% tolower() %>% str_detect( pattern=c("country|test|positive")) %>% any() #had to take out date because it triggers on "update"
          ){
          tables_list[[q]] <- h
          tables_list[[q]]$url <- q
          print("Table Found")
          break
        }
    }
  })
}
length(tables_list)
lapply(tables_list, names) #

library(janitor) #install.packages("janitor")

wikipedia_national_test <- bind_rows(lapply(tables_list, FUN=function(x) x %>% clean_names %>% mutate_if(is.numeric, as.character) ))

wikipedia_national_test_long <- wikipedia_national_test %>% 
                                mutate_if(is.factor, as.character) %>% #factors also break coalesce maybe?
                                mutate_if(is.character, list(~na_if(., ""))) %>% #dplyr::coalesce may not handle '' correctly
                                #rowwise() %>% 
                                mutate(dataset="wikipedia") %>%
                                mutate(tested_people = dplyr::coalesce(tests, total_tests,totaltests_a, totaltests, vte_volume_of_testing_2, vte_volume_of_testing, vte_covid_19_testing_volume_2) ) %>%
  
                                mutate(date = dplyr::coalesce(date,as_of,as_of_2, vte_covid_19_testing_volume_3, ) ) %>%
                                mutate(place = dplyr::coalesce(country_or_region,country, vte_covid_19_testing_volume, country_or_territory, vte_volume_of_testing) ) %>% 
                                mutate(place = str_replace_all(place,"Mainland|mainland","") %>% trimws()  ) %>%
                                separate(place, c("admin0_name_original", "admin1_name_original"),sep=":|  |  |, ")  %>% #seperated by either a colon or a weird double space
                                mutate(admin0_name_original = str_replace_all(admin0_name_original,"\\[.*?\\]|\\(.*?\\)","")  ) %>%
                                mutate(admin1_name_original = str_replace_all(admin1_name_original,"\\[.*?\\]|\\(.*?\\)","")  ) %>%
  
                                mutate(date_asdate1 = dmy(paste0(date,", 2020"))) %>% 
                                mutate(date_asdate2 = mdy(paste0(date,", 2020"))) %>% 
                                mutate(date_asdate = dplyr::coalesce(date_asdate1,date_asdate2)  ) %>% 
  
                                mutate(admin0_name_clean = admin0_name_original %>% rex_clean()) %>%
                                mutate(admin1_name_clean = admin1_name_original %>% rex_clean()) %>%
                                mutate(admin2_name_clean = '' %>% rex_clean()) %>%
                                rex_admin_function()

forjoining_wikipedia_national_test <- wikipedia_national_test_long %>%   
                                      dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, tested_people) %>% 
                                      mutate(tested_people = str_replace_all(tested_people,",|>| ","") %>% as.numeric()) %>%
                                      distinct()

dim(forjoining_wikipedia_national_test) #

```



#### Wikipedia Subnational Testing

https://en.wikipedia.org/wiki/Template:COVID-19_testing_by_country_subdivision

```{r}


library(rvest)

#https://en.wikipedia.org/wiki/Template:COVID-19_testing_by_country_subdivision


month_start=4 #starts in april
month_end=4
dates_to_grab <- apply(expand.grid(str_pad(month_start:month_end, 2, pad = "0") , str_pad(1:31, 2, pad = "0") ), 1, paste, collapse="")
histor_links_list <- list()
for(q in dates_to_grab){ 
  url <- paste0("https://en.wikipedia.org/w/index.php?title=Template:COVID-19_testing_by_country_subdivision&action=history&offset=2020",q,"000000&limit=1&action=history")
  print(url)
  wikipedia_history_page <- read_html(url)
  links <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('href'))
  titles <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('title')) 
  text <- wikipedia_history_page %>% html_nodes("a") %>% html_text() 
  wikipedia_history_page_links <- data.frame(link=paste0("https://en.wikipedia.org/",links) , 
                                             title=titles,
                                             text=text) %>%
                                  filter(str_detect(link,"oldid") & text!="cur" & text!="prev")
  histor_links_list[[q]] <- wikipedia_history_page_links
}

history_links_df <- bind_rows(histor_links_list) %>% arrange(link) %>% filter(!duplicated(link))

#Ah it's a fixed table that just gets included so scraping the whole page doesn't show any variation

tables_list <- list()
for(i in 1:nrow(history_links_df) ){
  q <- history_links_df$link[i]
  edit_date <- history_links_df$text[i]
  print(q)
  thepage = readLines(q)
  #temp_html <- read_html(q)
  #temp_tables <- temp_html  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=F) 

  library(htmltab) ; #install.packages('htmltab')
  temp_tables <- read_html(q)  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=T) 
  try({
    for(h in temp_tables){
        #table_temp <- htmltab(doc = thepage, which = i, complementary=F, headerSep="_") #,header=1 htmltab doesn't just fail it crashes R which is sad because I liked it better
        if( 
          names(h) %>% tolower() %>% str_detect( pattern=c("countr|deaths")) %>% any() #had to take out date because it triggers on "update"
          ){
          tables_list[[q]] <- h
          tables_list[[q]]$edit_date <- edit_date
          print("Table Found")
          break
        }
    }
  })
}
length(tables_list)
lapply(tables_list, names) #41 snapshots all with the same names god bless them

library(janitor) #install.packages("janitor")

wikipedia_subnational_tests <- bind_rows(lapply(tables_list, clean_names))
dim(wikipedia_subnational_tests)

wikipedia_subnational_tests_long <- wikipedia_subnational_tests %>%
                                  mutate_if(is.character, list(~na_if(., ""))) %>% #dplyr::coalesce may not handle '' correctly
                                  rowwise() %>% 
                                  mutate(dataset="wikipedia") %>%
  
                                  mutate(confirmed = dplyr::coalesce(positive) %>% as.character() %>% as.numeric() ) %>%
                                  mutate(tested_people = dplyr::coalesce(tests) %>% as.character() %>% as.numeric()  ) %>%

                                  mutate(place0 = dplyr::coalesce(country_a) ) %>%
                                  mutate(place1 = dplyr::coalesce(subdivision) ) %>%
  
                                  mutate(date = dplyr::coalesce(date) ) %>%
                                  mutate(date_asdate = date %>% paste0("., 2020") %>% dmy() ) %>% 
  
                                  mutate(admin0_name_clean = place0 %>% rex_clean()) %>%
                                  mutate(admin1_name_clean = place1 %>% rex_clean()) %>%
                                  mutate(admin2_name_clean = '' ) %>%
                                  rex_admin_function()

forjoining_wikipedia_subnational_tests <- wikipedia_subnational_tests_long %>%   
                                          dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed, tested_people) %>% 
                                          mutate(confirmed = str_replace_all(confirmed,",|>| ","") %>% as.numeric()) %>%
                                          mutate(tested_people = str_replace_all(tested_people,",|>| ","") %>% as.numeric()) %>%
                                          distinct()

dim(forjoining_wikipedia_subnational_tests) #

```


#### Wikipedia National Cases

```{r}

#https://webapps.stackexchange.com/questions/35822/get-the-version-of-a-wikipedia-page-from-a-specific-date
#Go to the history page of your desired page, and then add &YYYYMMDDHHMMSS to the URL.
#http://en.wikipedia.org/w/index.php?title=COVID-19_testing&action=history&offset=20200415000000
#&limit=1&action=history  #limits it further to just one link
library(rvest)

#https://en.wikipedia.org/wiki/Template:2019%E2%80%9320_coronavirus_pandemic_data

#The first appearence is feb 25 2020
month_start=1
month_end=4
dates_to_grab <- apply(expand.grid(str_pad(month_start:month_end, 2, pad = "0") , str_pad(1:31, 2, pad = "0") ), 1, paste, collapse="")
histor_links_list <- list()
for(q in dates_to_grab){ 
  url <- paste0("https://en.wikipedia.org/w/index.php?title=Template:2019%E2%80%9320_coronavirus_pandemic_data&dir=prev&action=history&offset=2020",q,"000000&limit=1&action=history")
  print(url)
  wikipedia_history_page <- read_html(url)
  links <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('href'))
  titles <- wikipedia_history_page %>% html_nodes("a") %>% html_attr(c('title')) 
  text <- wikipedia_history_page %>% html_nodes("a") %>% html_text() 
  wikipedia_history_page_links <- data.frame(link=paste0("https://en.wikipedia.org/",links) , 
                                             title=titles,
                                             text=text) %>%
                                  filter(str_detect(link,"oldid") & text!="cur" & text!="prev")
  histor_links_list[[q]] <- wikipedia_history_page_links
}

history_links_df <- bind_rows(histor_links_list) %>% arrange(link) %>% filter(!duplicated(link))

#Ah it's a fixed table that just gets included so scraping the whole page doesn't show any variation

tables_list <- list()
for(i in 1:nrow(history_links_df) ){
  q <- history_links_df$link[i]
  edit_date <- history_links_df$text[i]
  print(q)
  thepage = readLines(q)
  #temp_html <- read_html(q)
  #temp_tables <- temp_html  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=F) 

  library(htmltab) ; #install.packages('htmltab')
  temp_tables <- read_html(q)  %>% html_nodes("table") %>%  html_table(fill = TRUE, header=T) 
  try({
    for(h in temp_tables){
        #table_temp <- htmltab(doc = thepage, which = i, complementary=F, headerSep="_") #,header=1 htmltab doesn't just fail it crashes R which is sad because I liked it better
        if( 
          names(h) %>% tolower() %>% str_detect( pattern=c("countr|deaths")) %>% any() #had to take out date because it triggers on "update"
          ){
          tables_list[[q]] <- h
          tables_list[[q]]$edit_date <- edit_date
          print("Table Found")
          break
        }
    }
  })
}
length(tables_list)
lapply(tables_list, names) #41 snapshots all with the same names god bless them

library(janitor) #install.packages("janitor")

wikipedia_national_cases <- bind_rows(lapply(tables_list, clean_names))
dim(wikipedia_national_cases)



wikipedia_national_cases_long <- wikipedia_national_cases %>%
                                  mutate_if(is.character, list(~na_if(., ""))) %>% #dplyr::coalesce may not handle '' correctly
                                  #rowwise() %>% 
                                  mutate(dataset="wikipedia") %>%
  
                                  mutate(confirmed = dplyr::coalesce(confirmedcases,confirmed,cases,cases_b,vte2019_20_coronavirus_outbreak_by_country_and_territory_2) ) %>%
                                  mutate(deaths = dplyr::coalesce(deaths_c,deaths,deaths_b,vte2019_20_coronavirus_outbreak_by_country_and_territory_3) ) %>%

                                  mutate(confirmed = confirmed %>% str_replace("[^0-9]","") %>% as.numeric() ) %>%
                                  mutate(deaths = deaths %>% str_replace("[^0-9]","") %>% as.numeric()) %>%
  
                                  mutate(place = dplyr::coalesce(country_region,country_or_territory_a,locations_a,locations_a_2,country_or_region,countries_and_territories_a,country_or_territory,
                                                                         country_or_territory,countries_and_territories_a_2) ) %>%
  
                                  mutate(date = dplyr::coalesce(edit_date) ) %>%
                                  mutate(date_asdate = date %>% str_replace("^.*?,","") %>% dmy()) %>%
  
                                  mutate(admin0_name_original = place %>% str_replace("\\[.*?\\]|\\(.*?\\)","") ) %>%
                                  mutate(admin0_name_original = admin0_name_original %>% tolower() %>% str_replace("mainland","") ) %>%
  
                                  mutate(admin0_name_clean = admin0_name_original %>% rex_clean()) %>%
                                  mutate(admin1_name_clean = '' ) %>%
                                  mutate(admin2_name_clean = '' ) %>%
                                  rex_admin_function()

forjoining_wikipedia_national_cases <- wikipedia_national_cases_long %>%   
                                      dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed, deaths) %>% 
                                      distinct()

dim(forjoining_wikipedia_national_cases) #

```

# Mung It all Together

```{r}

library(tsibble)

lhs_long <- bind_rows(
  
  #North America
  
    #U.S.
    forjoining_nytimes,
    forjoining_usafacts,
    forjoining_covidtracking,
  
  #South America
  
    #Brazil
    forjoining_bhm,
    forjoining_covid19br_long,
    
  ## Europe
  
    #United Kingdom
    forjoining_covid_19_uk,
    
    #Germany
    forjoining_rki_covid19,
    
    #italy
    forjoining_dpc, 
  
    #Switzerland
    forjoining_openZH,
  
  ## South Asia
    #India
    forjoining_covid19india,
    forjoining_imhfw,

  ## Asia
    
    #South Korea
    forjoining_ds4c,
    
    #Vietnam
    forjoining_wikipedia_vietnam_subnation_cases,
  
    #Japan,
    forjoining_covid19japan,
  

  # Africa
  
    #Tunesia
    forjoining_TnCovid,
  
  #Bangladesh
  forjoining_iedcr,
  
  #phillipines
  forjoining_pdoh,
  
  #Congo
  
  #Turkey
  forjoining_wikipedia_subnational_turkey_long,
  
  #Iran
  forjoining_wikipedia_subnational_iran_long,
  
  #Thailand
  forjoining_wikipedia_subnational_thailand_long,
  
  #South Africa
  forjoining_wikipedia_subnational_south_africa_long,
  
  #Myanmar
  forjoining_myanmar_testing_wikipedia,

  
  #Multinational
  forjoining_finddx_tests_long,
  forjoining_worldometers_long,
  forjoining_ecdc,
  forjoining_CSSE,
  forjoining_who_owid,
  forjoining_who,
  forjoining_bing,
  forjoining_metabiota,

  forjoining_wikipedia_national_test,
  forjoining_wikipedia_subnational_tests,
  forjoining_wikipedia_national_cases,
  
)   %>% 
  mutate(confirmed=confirmed %>% as.character() %>% as.numeric() ) %>%
  mutate(deaths=deaths %>% as.character() %>% as.numeric() ) %>%
  
  mutate(confirmed=ifelse(confirmed==0,NA,confirmed)) %>% #we don't believe zeros
  mutate(deaths=ifelse(deaths==0,NA,deaths)) %>%  #we don't believe zeros
  filter(!(is.na(confirmed & is.na(deaths) & is.na(tested_people)))) %>%
  distinct() %>%
  arrange(dataset,gid ,  geonameid ,wikidata_id, date_asdate)

dim(lhs_long) #416,897 #409957 #398999 #310169 #304,376

lhs_long$dataset %>% table() %>% sort()

saveRDS(lhs_long, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long.Rds")

```



```{r}

library(benford.analysis) #install.packages('benford.analysis')

cp <- benford(lhs_long$confirmed, 1, sign="both") #generates benford object
cp #prints
plot(cp) #plots

cp <- benford(lhs_long$deaths, 1, sign="both") #generates benford object
cp #prints
plot(cp) #plots

cp <- benford(lhs_long$tested_people, 2, sign="both") #generates benford object
cp #prints
plot(cp) #plots

cp <- lhs_long %>% pull(tested_people) %>% benford(2, sign="both") #generates benford object
cp #prints
plot(cp) #plots

cp <- lhs_long %>% filter(gid=="USA") %>% pull(tested_people) %>% benford(2, sign="both") #generates benford object
cp #prints
plot(cp) #plots

cp <- lhs_long %>% filter(gid %>% str_detect("CHN")) %>% pull(tested_people) %>% benford(2, sign="both") #generates benford object
cp #prints
plot(cp) #plots


data(corporate.payment) #gets data
cp <- benford(corporate.payment$Amount, 2, sign="both") #generates benford object
cp #prints
plot(cp) #plots
head(suspectsTable(cp),10) #prints the digits by decreasing order of discrepancies
#gets observations of the 2 most suspicious groups
suspects <- getSuspects(cp, corporate.payment, how.many=2)
duplicatesTable(cp) #prints the duplicates by decreasing order
#gets the observations of the 2 values with most duplicates
duplicates <- getDuplicates(cp, corporate.payment,how.many=2)
MAD(cp) #gets the Mean Absolute Deviation
chisq(cp) #gets the Chi-squared test
#gets observations starting with 50 or 99
digits_50_and_99 <- getDigits(cp, corporate.payment, digits=c(50, 99))


```
