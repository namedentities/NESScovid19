---
title: "How to be Curious instead of Contrarian About Covid 19 Part 2: Cases, Tests, and Deaths"
output:
  html_notebook:
    toc: yes
date: 
author: 
affiliation: Director, Machine Learning for Social Science Lab, Center for Peace and Security Studies, University of California San Diego
editor_options: 
  chunk_output_type: inline
---

Rex W. Douglass

<style type="text/css">
blockquote {
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 14px;
    border-left: 5px solid #eee;
}
</style>

The good news brigade is back at it, looking for angles with which to interpret count data that support a happy or underdog story.

```{r}
#libraries
library(lubridate)
library(tidyverse)

#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(scales)
library(gghighlight)
library(lubridate)
library(R0)  # consider moving all library commands to top -- this one was in a loop below

library(WikidataR)
library(countrycode)

library(usmap) ; # install.packages('usmap')
data(statepop)
#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(sf)

```

Can start back from here

```{r}

admin0 <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/admin0.Rds")
admin1 <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/admin1.Rds")
admin2 <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/admin2.Rds")

rex_clean <- function(x){ x %>% stringi::stri_trans_general("latin-ascii") %>% stringi::stri_replace_all(regex="[^A-Za-z0-9]","") %>% tolower() } #so individually, each string should be devoid of spaces,noncharacters, and nonlatin


rex_admin_function <- function(x) {
  x %>% 
  left_join(admin0 ) %>%
  left_join(admin1 )  %>%
  left_join(admin2 ) %>%
  mutate(gid=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',gid2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', gid1, gid0))) %>%
  mutate(wikidata_id=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',wikidata_id2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', wikidata_id1, wikidata_id0))) %>%
  mutate(geonameid=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',geonameid2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', geonameid1, geonameid0))) %>%
  mutate(admin_level=ifelse(!is.na(admin2_name_clean) & admin2_name_clean!='',2,ifelse(!is.na(admin1_name_clean) & admin1_name_clean!='', 1, 0)))
}

```

## Google maps

```{r}
#install.packages("mapsapi")
#library(mapsapi)
#doc = mp_geocode(addresses = "Tel-Aviv", key=) #couldn't get this to work with my api key. intentionally leaving it out here so I don't push it accidentally

```

# LHS

## Mexico

https://www.reddit.com/r/datasets/comments/fykr5h/json_dataset_about_covid19_in_mexico/

## California Hosptilizations

https://github.com/CALmatters/covid-19-california-hospitalizations-data
https://public.tableau.com/profile/ca.open.data#!/vizhome/COVID-19PublicDashboard/Covid-19Hospitals

## ILILNet

#Can download state-week data here
https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html

https://wwwn.cdc.gov/ILINet/

https://www.medrxiv.org/content/10.1101/2020.04.01.20050542v1

https://www.economist.com/graphic-detail/2020/04/11/why-a-study-showing-that-covid-19-is-everywhere-is-good-news?fsrc=scn%2Ftw%2Fte%2Fbl%2Fed%2Ffootprintsoftheinvisibleenemywhyastudyshowingthatcovid19iseverywhereisgoodnewsgraphicdetail&%3Ffsrc%3Dscn%2F=tw%2Fdc

## ECDC
https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide

Country level

```{r}

#these libraries need to be loaded
library(utils)
#read the Dataset sheet into “R”. The dataset will be called "data".
ecdc <- read.csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv", na.strings = "", fileEncoding = "UTF-8-BOM")

ecdc_log <- ecdc %>% 
  mutate(date_asdate = dmy(dateRep)) %>%
  mutate(dataset="ecdc") %>% 
  
  mutate(admin0_name_clean=countriesAndTerritories %>% rex_clean()) %>%
  mutate(admin1_name_clean='') %>%
  mutate(admin2_name_clean='') %>%
  rex_admin_function() %>%
  group_by(gid, geonameid,wikidata_id) %>% arrange(date_asdate) %>%
  mutate(confirmed=cumsum(cases),
         deaths=cumsum(deaths)) %>%
  ungroup()

forjoining_ecdc <- ecdc_log %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate, confirmed, deaths)


```


## India
https://api.covid19india.org/
https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#
https://github.com/covid19india/api/blob/master/state_test_data.json


https://github.com/amodm/api-covid19-in


```{r}
#Couldn't get any of this to work
#library(googlesheets4)
#temp <- read_csv("http://api.covid19india.org/states_daily_csv/confirmed.csv")
#temp <- read_sheet("https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#")
#install.packages('gsheet')
#library(gsheet)

#The test data is pushed as a json file here but it's not historical
#https://github.com/covid19india/api/blob/master/state_test_data.json
covid19india_test <- fromJSON("https://raw.githubusercontent.com/covid19india/api/master/state_test_data.json")$states_tested_data
saveRDS(covid19india_test, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/covid19india_test_latest_" %>% paste0(Sys.Date(),".Rds")   ) #start saving dayly snapshots that we can add to the pile

covid19india_test_long <- covid19india_test %>% 
                          mutate(date_asdate = dmy(updatedon )) %>%
                          mutate(admin0_name_clean="India" %>% rex_clean()) %>%
                          mutate(admin1_name_clean= state %>% rex_clean()) %>%
                          mutate(admin2_name_clean= "" %>% rex_clean()) %>%
                          mutate(totaltested=as.numeric(totaltested)) %>%
                          dplyr::select(date_asdate, admin0_name_clean, admin1_name_clean, admin2_name_clean, tested_people=totaltested)
                          
  
#construct_download_url(url="https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#", format = "csv", sheetid = 1)
#temp < gsheet2tbl(url="https://docs.google.com/spreadsheets/d/e/2PACX-1vSc_2y5N0I67wDU38DjDh35IZSIS30rQf7_NYZhtYYGU1jJYT6_kDx4YpF-qw0LSlGsBYP8pqM_a1Pd/pubhtml#", sheetid = 1)
library(jsonlite)
rootnet_case_json <- fromJSON("https://api.rootnet.in/covid19-in/stats/history")
for(i in 1:length(rootnet_case_json$data$regional)){
  rootnet_case_json$data$regional[[i]]$day <- rootnet_case_json$data$day[i]
}
rootnet_cases_regional <- bind_rows(rootnet_case_json$data$regional)
rootnet_cases_regional <- rootnet_cases_regional  %>%
                           mutate(admin0_name_clean="India" %>% rex_clean()) %>%
                           mutate(admin1_name_clean= loc %>% rex_clean()) %>%
                           mutate(admin2_name_clean= "" %>% rex_clean())

rootnet_cases_national <- rootnet_case_json$data$summary
rootnet_cases_national$day <- rootnet_case_json$data$day

#This is just national testing
rootnet_testing <- fromJSON("https://api.rootnet.in/covid19-in/stats/testing/history")$data

rootnet_national <- rootnet_testing %>%
                     full_join(rootnet_cases_national) %>%
                     mutate(admin0_name_clean="India" %>% rex_clean()) %>%
                     mutate(admin1_name_clean= "" %>% rex_clean()) %>%
                     mutate(admin2_name_clean= "" %>% rex_clean()) 

covid19india_long <- bind_rows(
                rootnet_national %>% rename(tested_samples=totalSamplesTested, tested_people=totalIndividualsTested) %>% mutate(date_asdate = ymd(day )),
                rootnet_cases_regional %>% mutate(date_asdate = ymd(day )) %>% full_join(covid19india_test_long %>% distinct()) 
                ) %>%
                mutate(dataset="covid19india") %>%
                rex_admin_function()

forjoining_covid19india <- rootnet_long %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate,
                                                     tested_samples,
                                                     tested_people,
                                                     confirmed=total,
                                                     deaths=deaths
                                                     )

dim(forjoining_covid19india) #946


```


## Bing

```{r}

library(jsonlite)
#library(rjson)
#tmp2 <- tempfile()
#curl_download("https://bing.com/covid/data", tmp2)
bing1 <- fromJSON("https://web.archive.org/web/20200407032518if_/https://bing.com/covid/data")
bing2 <-  fromJSON("https://web.archive.org/web/20200414040859if_/https://bing.com/covid/data")
bing3 <- fromJSON("https://web.archive.org/web/20200331030201if_/https://bing.com/covid/data")
bing_latest <- fromJSON("https://bing.com/covid/data")
saveRDS(bing_latest, "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/bing_latest_" %>% paste0(Sys.Date(),".Rds")   ) #start saving dayly snapshots that we can add to the pile
areas <- bind_rows(
  bing1$areas,
  bing2$areas,
  bing3$areas,
  bing_latest$areas,
)

temp1 <- bing1$areas %>% dplyr::select(-areas)
temp2 <- bing2$areas %>% dplyr::select(-areas)
temp3 <- bing3$areas %>% dplyr::select(-areas)
temp4 <- bing_latest$areas %>% dplyr::select(-areas)

bing_df0 <- bind_rows(areas) %>% mutate(admin0_name_original=id)
dim(bing_df0) #835 14

bing_df1 <- bind_rows(bing_df0$areas) %>% separate(id, c("admin1_name_original", "admin0_name_original")) 
dim(bing_df1) #753  13

bing_df2 <- bind_rows(bing_df1$areas) %>% separate(id, c("admin2_name_original","admin1_name_original", "admin0_name_original")) 
dim(bing_df2) #2872   13

bing_long <- bind_rows(
          bing_df0 %>% dplyr::select(-areas),
          bing_df1 %>% dplyr::select(-areas),
          bing_df2 %>% dplyr::select(-areas)
) %>% 
  mutate(date_asdate = ymd(substring(lastUpdated,1,10))) %>%
  mutate(date_asdate = date_asdate-1 ) %>% #BING data are one day behind other data, subtract a day off
  
  mutate(dataset="bing") %>% 
  
  mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
  mutate(admin0_name_clean=str_replace(admin0_name_clean,"chinamainland","china")) %>%
  mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
  mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
  rex_admin_function()

forjoining_bing <- bing_long %>% dplyr::select(dataset,gid, geonameid,wikidata_id,date_asdate, confirmed=totalConfirmed, deaths=totalDeaths)
dim(forjoining_bing)

#Some of these are wrong
#russia khanty mansi  is a second level administrative division, it and a bunch of other ones from russia are listed as first order

#other are ambigious like
#Russia Voronezh might either be the city or the Oblast

```

## WHO

```{r}
WHO <- read_csv(url("https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv")) 

who_long <- WHO %>%
            mutate(dataset="who") %>%
            mutate(admin0_name_original=Country) %>%
            mutate(date_asdate = ymd(Date)) %>% 
            rename(confirmed=Confirmed  , deaths=Deaths)  %>%
  
             mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
             mutate(admin1_name_clean='') %>%
             mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
             rex_admin_function()

forjoining_who <- who_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths)

```

## CSSEGISandData
Johns Hopkins

```{r, echo=F, message=FALSE, results = FALSE, warning=FALSE}

#https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/
CSSE_confirmed_us      <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv"))
CSSE_confirmed_global  <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"))

CSSE_deaths_us      <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv"))
CSSE_deaths_global  <- read_csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv"))

CSSE_confirmed_us_long <- pivot_longer(CSSE_confirmed_us, names_to = "date", cols = ends_with("20"), values_to = "confirmed")
CSSE_confirmed_global_long <- pivot_longer(CSSE_confirmed_global, names_to = "date", cols = ends_with("20"), values_to = "confirmed")

CSSE_deaths_us_long <- pivot_longer(CSSE_deaths_us, names_to = "date", cols = ends_with("20"), values_to = "deaths")
CSSE_deaths_global_long <- pivot_longer(CSSE_deaths_global, names_to = "date", cols = ends_with("20"), values_to = "deaths")

CSSE_us_long <- CSSE_confirmed_us_long %>% full_join(CSSE_deaths_us_long) %>% mutate(admin0_name_original="United States") %>% mutate(admin1_name_original=Province_State) %>% mutate(admin2_name_original=Admin2)
CSSE_global_long <- CSSE_confirmed_global_long %>% full_join(CSSE_deaths_global_long) %>% mutate(admin0_name_original=`Country/Region`) %>% mutate(admin1_name_original=`Province/State`)

CSSE_long <- bind_rows(CSSE_us_long, CSSE_global_long) %>% 
              mutate(dataset="CSSE") %>%
              mutate(date_asdate = mdy(str_replace(date,"20$","2020"))) %>%

              mutate(admin0_name_clean=admin0_name_original %>% rex_clean())  %>%
              mutate(admin1_name_clean=admin1_name_original %>% rex_clean())  %>%
              mutate(admin2_name_clean=admin2_name_original %>% rex_clean())  %>%
              rex_admin_function() %>%
              distinct()

test <- CSSE_long %>% group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>% mutate(n=n()) %>% ungroup() #the dupes are coming either from NA location/date info or deaths and confirmed appearing as two seperate records


forjoining_CSSE <- CSSE_long %>% 
                   dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths) %>%
                   group_by(dataset, gid, geonameid, wikidata_id,date_asdate) %>% summarize(confirmed=max(confirmed, na.rm=T), deaths=max(deaths, na.rm=T))

test <- forjoining_CSSE %>% group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>% mutate(n=n()) %>% ungroup() #the dupes are coming either from NA location/date info or deaths and confirmed appearing as two seperate records


```


## NYT


```{r}
#https://www.nytimes.com/article/coronavirus-county-data-us.html
#https://github.com/nytimes/covid-19-data
nytimes <- read_csv(url("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")) #

nytimes_long <- nytimes %>%
                mutate(date_asdate = ymd(date)) %>% 
                rename(confirmed=cases) %>%
                mutate(dataset="nyt") %>%
                mutate(admin0_name_original="United States") %>%
                mutate(admin1_name_original=state) %>%
                mutate(admin2_name_original=county)  %>%

                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                mutate(admin0_name_clean=str_replace(admin0_name_clean,"chinamainland","china")) %>%
              
                mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%

                mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
               rex_admin_function()

forjoining_nytimes <- nytimes_long %>% dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths)

```

### Covidtracking.com

```{r}
#https://covidtracking.com/about-data
"If you are calculating positive rates, it should only be with states that have an A grade. And be careful going back in time because almost all the states have changed their level of reporting at different times."

#The data contain the u.s. and the states too
#FIPS the first two digits are the state
#All 0 is the U.S.
#
#https://covidtracking.com/api
covidtracking <- read_csv(url("https://covidtracking.com/api/states/daily.csv"))
dim(covidtracking) #1653   25
covidtracking_t <- t(covidtracking)

covidtracking_long <- covidtracking %>%

                      mutate(date_asdate = ymd(date)) %>% 
                      rename(state_abbr=state) %>%
                      left_join(state_codes) %>% 
                      dplyr::select(date_asdate, state_name, state_abbr, positive, negative, death) %>%
                      mutate(dataset="covidtracking") %>%
                      mutate(admin0_name_original="United States") %>%
                      mutate(admin1_name_original=state_name)  %>%
                      mutate(admin2_name_original='')  %>%
  
                      mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                      mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                      mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                      rex_admin_function()

forjoining_covidtracking <- covidtracking_long %>%
                            mutate(tested_people=negative+positive) %>%
                            dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=positive, deaths=death, tested_people)

```

metabiota.com

```{r}

metabiota <- read_csv("/home/skynet2/Downloads/data_ncov2019.csv")
dim(metabiota) #605848

table(metabiota$OUTCOME)
table(metabiota$CUMULATIVE_FLAG)

#cities look broken with just a flat number of 1 or 2 "Ho Chi Minh City"

metabiota_confirmed <- metabiota %>% filter(CUMULATIVE_FLAG==T) %>% filter(OUTCOME=="CASE") %>% filter(CONFIRM_STATUS=="CONFIRMED") %>% 
                       filter( DATE_LOW==DATE_HIGH) %>%
                       mutate(composite_source = SOURCE=="Metabiota Composite Source") %>%
                       distinct() %>%
                       arrange(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW, -composite_source) %>%
                       group_by(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW) %>%
                       filter(row_number()==1) %>%
                       ungroup() %>%
                       dplyr::select(date=DATE_REPORT,
                                          AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME,LOCALITY_NAME,confirmed=VALUE) 

#test <- metabiota_confirmed %>% group_by(AL0_NAME,AL1_CODE, AL1_NAME, AL2_NAME, AL3_NAME, LOCALITY_NAME, DATE_LOW) %>% mutate(n=n()) %>% ungroup() #the entire problem is that we're not handling cities correctly.


metabiota_deaths <- metabiota %>% filter(CUMULATIVE_FLAG==T) %>% filter(OUTCOME=="DEATH") %>% filter(CONFIRM_STATUS=="CONFIRMED") %>% 
                       filter( DATE_LOW==DATE_HIGH) %>%
                       mutate(composite_source = SOURCE=="Metabiota Composite Source") %>%
                       distinct() %>%
                       arrange(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW, -composite_source) %>%
                       group_by(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME, DATE_LOW) %>%
                       filter(row_number()==1) %>%
                       ungroup() %>%
                       dplyr::select(date=DATE_REPORT, #DATE_LOW #changing this to date of the report to align it with other 
                                     AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME,LOCALITY_NAME,deaths=VALUE) 

metabiota_long <- metabiota_confirmed %>% 
                  full_join(metabiota_deaths) %>%
                  group_by(AL0_NAME,AL1_NAME,AL2_NAME ,AL3_NAME, LOCALITY_NAME) %>%
                  filter(min(confirmed)!=max(confirmed)) %>% #require some variation
                  ungroup() %>%
                  rename(admin0_name_original=AL0_NAME,
                         admin1_name_original=AL1_NAME,
                         admin2_name_original=AL2_NAME,
                         admin3_name_original=LOCALITY_NAME
                         ) %>%
                  dplyr::select(-AL3_NAME) %>%
                  mutate(dataset="metabiota") %>%
                  mutate(date_asdate = ymd(date)) %>% 
                  mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                  mutate(admin0_name_clean=str_replace(admin0_name_clean,"chinamainland","china")) %>%
                
                  mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%

                  mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%

                  mutate(admin3_name_clean=admin3_name_original %>% rex_clean()) %>%
                  rex_admin_function()

forjoining_metabiota <- metabiota_long %>% 
                        filter(is.na(admin3_name_original) | admin3_name_original=='') %>% #we don't handle cities correctly so we have to exclude them here for now %>%
                        dplyr::select(dataset,gid, geonameid, wikidata_id,date_asdate, confirmed=confirmed, deaths=deaths) %>% 
                        distinct()

test <- forjoining_metabiota %>% group_by(dataset,gid, geonameid, wikidata_id,date_asdate) %>% mutate(n=n()) %>% ungroup() #the entire problem is that we're not handling cities correctly.

#The problem is ISO31662 is merging up multiple times
#ISO31662


#Drop any with no variation

#https://data.humdata.org/dataset/2019-novel-coronavirus-cases
#Spatiotemporal data for 2019-Novel Coronavirus Covid-19 Cases and deaths
#This dataset is part of COVID-19 Pandemic
#Data Overview
#This repository contains spatiotemporal data from many official sources for 2019-Novel Coronavirus beginning 2019 in Hubei, China ("nCoV_2019")
#You may not use this data for commercial purposes. If there is a need for commercial use of the data, please contact Metabiota at info@metabiota.com to obtain a commercial use license.

```

All the relevant data used in this paper are publicy available and accessible at
https://lab.gedidigital.it/gedi-visual/2020/coronavirus-i-contagi-in-italia/

## Wikipedia national testing page


```{r}
#https://webapps.stackexchange.com/questions/35822/get-the-version-of-a-wikipedia-page-from-a-specific-date
#Go to the history page of your desired page, and then add &YYYYMMDDHHMMSS to the URL.
#http://en.wikipedia.org/w/index.php?title=COVID-19_testing&action=history&offset=20200415000000


```



```{r}

lhs_long <- bind_rows(
  forjoining_CSSE,
  forjoining_who,
  forjoining_bing,
  forjoining_ecdc,
  forjoining_nytimes,
  forjoining_covidtracking,
  forjoining_metabiota,
  forjoining_covid19india
)   %>% 
  mutate(confirmed=ifelse(confirmed==0,NA,confirmed)) %>% #we don't believe zeros
  mutate(deaths=ifelse(deaths==0,NA,deaths)) %>%
  filter(!(is.na(confirmed & is.na(deaths) & is.na(tested_people)))) %>%
  distinct() %>%
  arrange(dataset,gid ,  geonameid ,wikidata_id, date_asdate) %>%
  group_by(dataset,gid ,  geonameid ,wikidata_id) %>% 
    mutate(confirmed_fd=confirmed, deaths_fd=deaths, tested_people_fd=tested_people, tested_samples_fd=tested_samples) %>%
    tidyr::fill( ends_with("_fd")) %>%
    mutate_at(vars(ends_with("_fd")), difference) %>%
  ungroup() %>%
  #About 4k of these observations show a decrease from one time step to the next which suggests either an original error or a joining error
  #We're going to straight drop those observations as a cleaning step
  filter( (is.na(confirmed_fd) | confirmed_fd>=0) & 
          (is.na(deaths_fd) | deaths_fd>=0) & 
          (is.na(tested_people_fd) | tested_people_fd>=0) & 
          (is.na(tested_samples_fd) | tested_samples_fd>=0)  
          )

dim(lhs_long) #174028 #174691  #179,748


#test <- lhs_long %>% group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>% summarize(n=n())

all_na <- function(x) any(!is.na(x)) #https://intellipaat.com/community/12999/remove-columns-from-dataframe-where-all-values-are-na

#bing looks off by a day from the other ones
lhs_wide_qcode <- lhs_long %>% 
                  filter(!is.na(wikidata_id) & !is.na(date_asdate)) %>%
                  group_by(gid, geonameid, wikidata_id, date_asdate) %>%  mutate(confirmed_var=var(confirmed, na.rm=T), deaths_var=var(deaths, na.rm=T)) %>% ungroup() %>%
                  dplyr::select(dataset, gid, geonameid, wikidata_id, date_asdate,confirmed, deaths, tested_samples, tested_people) %>%
                  group_by(dataset, gid, geonameid, wikidata_id, date_asdate) %>%  summarise_if(is.numeric,max,na.rm=T) %>% ungroup() %>% #this is a hack, we should have dupes within datasets
                  pivot_wider(names_from = dataset, values_from = c(confirmed, deaths, tested_samples, tested_people) ) %>% 
                  mutate_if(is.numeric, list(~na_if(abs(.), Inf))) %>% #https://stackoverflow.com/questions/12188509/cleaning-inf-values-from-an-r-dataframe
                  select_if(all_na) 

dim(lhs_wide_qcode) #82,157 82k place/day observations

length(unique(lhs_wide_qcode$wikidata_id)) #3697 #almost 4k 

test <- lhs_wide_qcode %>% dplyr::select(starts_with("confirmed")) %>% distinct() 

cor(test, use="pairwise.complete.obs")

#some of these are screwups. LIke bexar county for bing is not montonic
library(tsibble); #install.packages('tsibble')
lhs_wide_qcode_monotonic <- lhs_wide_qcode %>% arrange(date_asdate) %>%
                            group_by(wikidata_id) %>% 
                              tidyr::fill( starts_with("confirmed|deaths|tested")) %>%
                              mutate_if(is.numeric, difference) %>%
                            ungroup() #I think the way to do this is on the long version. If any of the cumulative counts is a decrease then regeject the whole observation.

lhs_temp <- lhs_long %>% group_by(gid, geonameid, wikidata_id, date_asdate) %>% summarize(n=n())
lhs_temp %>% filter(n<=6) %>% pull(n) %>% quantile() #median of only a single data point

lhs_temp %>% filter(n<=6) %>% ggplot(aes(x=date_asdate,y=n)) + geom_point()

library(lfe) #install.packages('lfe'), quietly=TRUE
xy_all <- lhs_long %>% group_by(gid, geonameid, wikidata_id, date_asdate) %>% mutate(n=n()) %>% filter(n<=6) %>% ungroup() %>% 
             filter(!is.na(wikidata_id) & !is.na(date_asdate)) %>% 
             mutate(date_place=paste(wikidata_id,date_asdate))  %>% 
             dplyr::select(confirmed, dataset, date_asdate, wikidata_id, date_place) %>% arrange(date_place) %>% mutate_if(is.character, as.factor) %>%
             filter(confirmed!=0) 
dim(xy_all)
table(xy_all$dataset)
est <- felm( confirmed ~ 1 + dataset | date_asdate + wikidata_id + date_place, data=xy_all)
summary(est)

#lm1 <- lm(confirmed ~ factor(dataset) + factor(date_asdate) + factor(wikidata_id) , data=lhs_long)


```

# RHS

## Testing

### ourworldindata.org

```{r}

#https://drive.google.com/drive/folders/1HPzXon49teN-kMQlY1ry9rh2eq-TyGQI
#https://ourworldindata.org/covid-testing
#on how to reformat the url for straight downloading
#https://gist.github.com/tanaikech/f0f2d122e05bf5f971611258c22c110f
library(googledrive)
library(curl)
tmp <- tempfile()
#curl_download("https://drive.google.com/open?id=18QE3wScRXL-ARIaBl6yh7xUVNlnitvW_", tmp)


#There's also a github repo
#
worldindata_github <- read_csv("https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/testing/covid-testing-all-observations.csv") 

#curl_download("https://drive.google.com/uc?export=download&id=18QE3wScRXL-ARIaBl6yh7xUVNlnitvW_", tmp)
#worldindata_googledrive <- read_csv(tmp) 

dim(worldindata_github)
#dim(worldindata_googledrive) #The github one is longer

worldindata_long <- worldindata_github %>%
                    mutate(date_asdate = ymd(Date)) %>%  
                    separate(Entity, c("admin0_name_original", "note"), sep="-") %>%
                    mutate(admin0_name_original=trimws(admin0_name_original)) %>%
                    rename(tests=`Cumulative total`) %>%
                    mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                     mutate(admin1_name_clean='') %>%
                     mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
          
                    left_join(admin0 ) %>%
                    left_join(admin1 )  %>%
                    left_join(admin2 ) 

```


## Demographics

### U.S. County Demographics

```{r}

#library(R0)  # consider moving all library commands to top -- this one was in a loop below
#Until the U.S. states one goes live have to pull it from here
#Codebook
#https://github.com/JieYingWu/COVID-19_US_County-level_Summaries/blob/master/data/list_of_columns.md
counties <- read_csv(url("https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv")) #using the archived copy from today because they haven't posted the 
counties_t <- t(counties)

counties_1 <- counties %>%
                      filter(as.numeric(substring(FIPS,3,5))==0 & Area_Name!="United States") %>%
                      mutate(admin0_name_original="United States") %>%
                      mutate(admin1_name_original=Area_Name) 
counties_2 <- counties %>% filter(as.numeric(substring(FIPS,3,5))>0 & Area_Name!="United States") %>% 
                      left_join(state_codes %>% rename(State=state_abbr)) %>%
                      mutate(admin0_name_original="United States") %>%
                      mutate(admin1_name_original=state_name)  %>%
                      mutate(admin2_name_original=Area_Name) 

counties_long <- bind_rows(counties_1,counties_2) %>%
                    mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                    mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                    mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%

                      left_join(admin0 ) %>%
                      left_join(admin1 ) %>%
                      left_join(admin2 ) 



```

## 
The international political economy data resource
https://link.springer.com/article/10.1007/s11558-017-9285-0

```{r}

tmp2 <- tempfile()
temp_url <- "https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/X093TV/XHEWU3"
curl::curl_download(temp_url, tmp2)
load(tmp2) #the object's name is ipe_full
glimpse(ipe_full)
#$ country                     <chr> "United States of America", "United States of America", "United States of America", "United States of America", "United States of America", "United States of America", "United States of America", "United States of America", "United States of…
#$ year                        <dbl> 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1…
#$ gwno                        <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…
#$ ccode                       <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…
#$ ifscode                     <dbl> 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 1…
#$ ifs                         <chr> "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "US…
#$ gwabbrev 

ipe_long <- ipe_full %>% 
  group_by(country) %>% filter(year==max(year)) %>% ungroup()
#the whole dataset ends in 2017 and a lot of these values aren't available that recently which is interesting

#codebook
#https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/X093TV/OIB5GS&version=3.0

ipe_long <- ipe_full %>%
            mutate_if(is.numeric,as.character, is.factor, as.character) %>% #https://stackoverflow.com/questions/58124530/pivot-longer-with-multiple-classes-causes-error-no-common-type
            pivot_longer( names_to = "variable", cols = onset2_AO:english_off_USA_CE, values_to = "value", values_ptypes = list(val = 'character')) %>% filter(!is.na(value))  %>% 
            group_by(country, variable) %>% 
            filter(year==max(year)) %>% 
            ungroup()  %>%
            mutate(admin0_name_clean=country %>% rex_clean()) %>%
            mutate(admin1_name_clean='') %>%
            mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
  
            left_join(admin0 ) %>%
            left_join(admin1 )  %>%
            left_join(admin2 ) 

```


```{r}
#World Development Indicators
#http://datatopics.worldbank.org/world-development-indicators/
#csv bulk zip download
#http://databank.worldbank.org/data/download/WDI_csv.zip


WDIData <- read_csv("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/WDIData.csv")

WDIData_long <- WDIData %>%
                mutate_if(is.numeric,as.character, is.factor, as.character) %>% #https://stackoverflow.com/questions/58124530/pivot-longer-with-multiple-classes-causes-error-no-common-type
                pivot_longer( names_to = "year", cols = `1960`:X65, values_to = "value", values_ptypes = list(val = 'character')) %>% filter(!is.na(value))  %>% 
                group_by(`Country Name`, `Indicator Name`) %>% 
                filter(year==max(year)) %>% 
                ungroup() 

colnames(WDIData_long) <- colnames(WDIData_long) %>% str_replace_all(" ","_") %>% tolower()

WDIData_long <- WDIData_long %>%
                mutate(indicator_name=indicator_name%>% str_replace_all(" ","_") %>% tolower()) %>%
                rename(admin0_name_original=country_name) %>%
                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                 mutate(admin1_name_clean='') %>%
                 mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
      
                left_join(admin0 ) %>%
                left_join(admin1 )  %>%
                left_join(admin2 ) 


temp <- WDIData_long %>% count(indicator_name)

```



## Mobility

### Google Mobility Data

```{r}
#Google mobility daily data from the trend lines
#https://github.com/nacnudus/google-location-coronavirus
#only at the country level though
google_mobility_timelines <- read_tsv("https://raw.githubusercontent.com/nacnudus/google-location-coronavirus/master/2020-03-29.tsv")

```

```{r}

#Mobility datga
google_mobility_us <- read_csv(url("https://raw.githubusercontent.com/ActiveConclusion/COVID19_mobility/master/mobility_report_US.csv")) #
google_mobility_regions <- read_csv(url("https://raw.githubusercontent.com/ActiveConclusion/COVID19_mobility/master/mobility_report_regions.csv")) #

google_mobility_us_long <- google_mobility_us %>%
                   mutate(date_asdate = ymd(Date)) %>%
                    mutate(admin0_name_original="United States") %>%
                    mutate(admin1_name_original=State)  %>%
                    mutate(admin2_name_original=Region) %>%
                    mutate(admin2_name_original=ifelse(admin2_name_original=="Total",NA, admin2_name_original))

google_mobility_regions_long <- google_mobility_regions %>%
                          mutate(date_asdate = ymd(Date)) %>%
                          mutate(admin0_name_original=Country) %>%
                          mutate(admin1_name_original=Region)  %>%
                          mutate(admin1_name_original=ifelse(admin1_name_original=="Total",NA, admin1_name_original))  

google_mobility_long <- 
                  bind_rows(google_mobility_us_long,
                            google_mobility_regions_long) %>%
                  rename(google_retail= `Retail & recreation`,
                         google_grocery=`Grocery & pharmacy`,
                         google_park= Parks,
                         google_transit=`Transit stations`,
                         google_workplace=Workplaces,
                         google_residential=Residential) %>% 
                  distinct() %>%
                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                left_join(admin0 ) %>%
                left_join(admin1 )  %>%
                left_join(admin2 )   

```


NYT state order issued

```{r}
#https://www.nytimes.com/interactive/2020/us/coronavirus-stay-at-home-order.html
```



What areas have more or fewer cases/deaths than we would expect?


```{r}

#has new york city as a single entitiy but not the constituent counties which is frustrating

library(strucchange) ; #install.packages('strucchange')
temp <- nytimes_long %>% arrange(date_asdate) %>% filter(county %in% "Bexar") %>% 
  mutate(confirmed_log=log(confirmed+1)) %>%
  mutate(date_rank= rank(date_asdate))  
bp <- breakpoints(confirmed_log ~ 1, data=temp)
bp <- breakpoints(confirmed_log ~ 1 + date_rank, data=temp)
temp %>% ggplot() + geom_point(aes(x=rank(date_asdate), y=confirmed_log)) + geom_vline(xintercept=bp$breakpoints)
coef(bp)


temp <- nytimes_long %>% arrange(date_asdate) %>% filter(county %in% "New York City") %>% 
  mutate(confirmed_log=log(confirmed+1)) %>%
  mutate(t= rank(date_asdate))  
bp <- breakpoints(confirmed_log ~ 1, data=temp)
bp <- breakpoints(confirmed_log ~ 1 + t, data=temp)

tmin <- min(temp$t)
tmax <- max(temp$t)
c(tmin, bp$breakpoints, tmax)


cdf <- data.frame(
  t= c(1,bp$breakpoints), 
  t_slope= coef(bp)[,2]
)

temp <- temp %>% 
  mutate(y_hat = fitted.values(bp)) %>%
  left_join(cdf) %>% 
  fill(t_slope) %>%
  mutate(t_slope_percent_change = round((exp(t_slope)-1)*100,2))

temp %>% ggplot() +
  geom_point(aes(x=rank(date_asdate), y=confirmed_log)) + 
  geom_line(aes(x=rank(date_asdate), y=y_hat)) + 
  geom_vline(xintercept=bp$breakpoints)
```

```{r}

temp_list <- list()
for(q in places$place){
  print(q)
  temp <- NULL
  temp <- nytimes_long %>% 
    arrange(date_asdate) %>% 
    filter(place %in% q) %>% 
    mutate(confirmed_log=log(confirmed+1)) %>%
    mutate(t= rank(date_asdate))  
  if( nrow(temp)==0 ) {print("error"); break}
  
  #bp <- breakpoints(confirmed_log ~ 1, data=temp)
  bp <- NULL
  lm1 <- NULL
  y_hat <- NA
  cdf <- NULL
  try({
    #if it fails fall back to just a lm
    lm1 <- lm(confirmed_log ~ 1 + t, data=temp)
    
    cdf <- data.frame(
      t= 1, 
      t_slope= coef(lm1)[2],
      t_slope_break=0
    )
    y_hat=fitted.values(lm1)
  })
  
  try({
    bp <- breakpoints(confirmed_log ~ 1 + t, data=temp)
    
    cdf <- data.frame(
      t= c(1,bp$breakpoints), 
      t_slope= coef(bp)[,2],
      t_slope_break=1
    )
    y_hat=fitted.values(bp)
  })
  
  try({
    temp <- temp %>% 
      mutate(y_hat = y_hat) %>%
      left_join(cdf) %>% 
      fill(t_slope) %>%
      mutate(t_slope_percent_change = round((exp(t_slope)-1)*100,2))
    
    temp_list[[as.character(q)]] <- temp
  })
  #if( is.na( temp_list[[as.character(q)]]$y_hat) ) {print("error"); break}
}
#"13055"
temp_df <- bind_rows(temp_list)
dim(temp_df)

temp_list[["New York_New York City_NA"]]


```

```{r}

temp_df_max <- temp_df %>% group_by(fips) %>% filter(date_asdate==max(date_asdate)) %>% ungroup()

hist(temp_df_max$t_slope_percent_change, breaks=50)

temp_df_max %>% ggplot() + geom_density(aes(x=t_slope_percent_change))

```

```{r}
temp_df %>% 
  head(1000) %>%
  ggplot() +
  geom_line(aes(x=date_asdate,
                y=t_slope_percent_change,
                color=fips))  + theme(legend.position = "none")

```

```{r}
temp_df %>%
  filter(place %in% "New York_New York City_NA")  %>% 
  mutate(date_asdate_rank=rank(date_asdate)) %>%
  ggplot() +
  geom_point(aes(x=date_asdate,y=t_slope_percent_change))


temp_df %>%
  filter(place %in% "New York_New York City_NA")  %>% 
  ggplot() +
  geom_point(aes(x=date_asdate,y=confirmed_log)) +
  geom_line(aes(x=date_asdate,y=y_hat)) +
  #geom_point(aes(x=date_asdate,y=t_slope_percent_change))
  
  temp_df %>%
  filter(place %in% "Texas_Bexar_48029")  %>% 
  ggplot() +
  geom_point(aes(x=date_asdate,y=confirmed_log)) +
  geom_line(aes(x=date_asdate,y=y_hat))

library(gghighlight)
temp_df %>%
  filter(place %in% c("New York_New York City_NA","Texas_Bexar_48029","California_San Diego_06073","Florida_St. Lucie_12111") ) %>% 
  ggplot(aes(x=date_asdate, color=place)) +
  geom_point(aes(y=confirmed_log)) +
  geom_line(aes(y=y_hat)) +
  gghighlight(#prefered_label %in% c("Washington, US","Italy","China") , #"New York, US" #, "US"
    label_params =
      list(
        size = 3,
        segment.alpha=0)
  ) +
  theme_bw()

library(gghighlight)
temp_df %>%
  filter(t_slope_percent_change>1 & confirmed>20 ) %>% 
  mutate(y_hat_exp=exp(y_hat) ) %>% 
  
  ggplot(aes(x=date_asdate, color=place)) +
  geom_point(aes(y=confirmed_log)) +
  geom_line(aes(y=y_hat)) +
  gghighlight(place %in% c("New York_New York City_NA","Texas_Bexar_48029","California_San Diego_06073","Florida_St. Lucie_12111") , 
              label_params =
                list(
                  size = 3,
                  segment.alpha=0)
  ) +
  theme_bw() + ylim(3,11.5)

library(scales)
library(gghighlight)
temp_df %>%
  filter(t_slope_percent_change>1 & confirmed>20 ) %>% 
  mutate(y_hat_exp=exp(y_hat) ) %>% 
  
  ggplot(aes(x=date_asdate, color=place)) +
  geom_point(aes(y=confirmed)) +
  geom_line(aes(y=y_hat_exp)) +
  gghighlight(place %in% c("New York_New York City_NA","Texas_Bexar_48029","California_San Diego_06073","Florida_St. Lucie_12111") , 
              label_params =
                list(
                  size = 3,
                  segment.alpha=0)
  ) +
  theme_bw() + scale_y_log10(labels = comma_format()) #+ ylim(10, 100000)


temp_df %>%
  filter(t_slope_percent_change>1 & confirmed>=20 ) %>% 
  mutate(y_hat_exp=exp(y_hat) ) %>% 
  
  ggplot(aes(x=days_since_20_confirmed, color=place)) +
  geom_point(aes(y=confirmed)) +
  geom_line(aes(y=y_hat_exp)) +
  gghighlight(place %in% c("New York_New York City_NA","Texas_Bexar_48029","California_San Diego_06073","Florida_St. Lucie_12111") , 
              label_params =
                list(
                  size = 3,
                  segment.alpha=0)
  ) +
  theme_bw() + scale_y_log10(labels = comma_format()) #+ ylim(10, 100000)




```

Ok so we put slope at days since 20 confirmed on the left hand side, and put other covariates on the rhs


```{r}


lhs_20 <- temp_df %>% 
  filter(t_slope_percent_change>1 & confirmed>=20 ) %>% 
  mutate(y_hat_exp=exp(y_hat) ) %>%
  left_join( state_codes %>% dplyr::select(state=state_name, state_abbr ) )
dim(lhs_20)

dim(covidtracking)

contemporary_us  <- us_counties()
rhs_mobility <- mobility %>% 
  mutate(Region=str_replace(Region," County","")) %>%
  left_join(contemporary_us %>% 
              mutate(fips=paste0(statefp , countyfp))  %>% as.data.frame() %>% 
              dplyr::select(fips, State=state_name , Region=name  ) 
  ) %>%
  rename(google_retail= `Retail & recreation`,
         google_grocery=`Grocery & pharmacy`,
         google_park= Parks,
         google_transit=`Transit stations`,
         google_workplace=Workplaces,
         google_residential=Residential) %>% 
  distinct()


rhs_covidtracking <- covidtracking %>%
  mutate(date_asdate = ymd(date)) %>% 
  dplyr::select(date_asdate, state_abbr=state, state_test_positive=positive, state_test_negative=negative) %>%
  mutate( state_test = state_test_positive + state_test_negative) %>% 
  left_join(statepop  %>% dplyr::select(state_abbr=abbr, pop_2015) ) %>%
  mutate( state_test_percap = state_test/pop_2015) %>% 
  distinct()



rhs_counties <- counties %>% dplyr::select(fips=FIPS, 
                                    TOT_MALE, TOT_FEMALE,
                                    doctors_per_cap=`Active Physicians per 100000 Population 2018 (AAMC)`,
                                    popdensity=`Density per square mile of land area - Population`,
                                    area=`Area in square miles - Total area`,
                                    Median_Household_Income_2018,
                                    Total_age65plus,
                                    WA_MALE,	#White alone male population
                                    WA_FEMALE,	#White alone female population
                                    hospitals=`Total Hospitals (2019)`
) %>% 
  mutate(pop=TOT_MALE+TOT_FEMALE) %>% mutate(pop_perc_male = TOT_MALE/pop) %>% 
  mutate(pop_over65_perc=Total_age65plus/pop) %>%
  mutate(pop_nonwhite_perc= (pop-(WA_MALE+WA_FEMALE))/pop  ) %>%
  
  distinct()

xy_all <- lhs_20 %>% filter(county!="Unknown") %>%
  mutate(date_numeric=as.numeric(date_asdate)) %>% 
  left_join(rhs_covidtracking)  %>% 
  left_join(rhs_counties) %>% 
  left_join(rhs_mobility)
dim(xy_all)
glimpse(xy_all)

xy_all_cross_section <- xy_all %>% group_by(place) %>% filter(date_numeric==max(date_numeric)) %>% filter(!is.na(fips))  %>% ungroup() %>% as.data.frame()
dim(xy_all_cross_section)

xy_all_cross_section$pop_log <- log(xy_all_cross_section$pop)
xy_all_cross_section$popdensity_log <- log(xy_all_cross_section$popdensity)
xy_all_cross_section$state_test_log <- log(xy_all_cross_section$state_test)
xy_all_cross_section$area_log <- log(xy_all_cross_section$area)
xy_all_cross_section$Median_Household_Income_2018_log <- log(xy_all_cross_section$Median_Household_Income_2018)
xy_all_cross_section$doctors_per_cap_log <- log(xy_all_cross_section$doctors_per_cap)
xy_all_cross_section$hospitals_log <- log(xy_all_cross_section$hospitals+1)

```

```{r}

temp <- xy_all_cross_section %>%
  dplyr::select(place,state, county, t_slope_percent_change,days_since_20_confirmed,pop_log,popdensity_log,state_test_percap,google_workplace,Median_Household_Income_2018,area_log,pop_over65_perc,hospitals_log) %>% 
  arrange(place)

library(GGally)
xy_all_cross_section %>%
  dplyr::select(t_slope_percent_change,days_since_20_confirmed,pop_log,popdensity_log,state_test_percap,google_workplace,Median_Household_Income_2018_log,area_log,hospitals) %>%
  ggpairs(title = "Within Psychological Variables")

```

```{r}


library(randomForestSRC); #install.packages('randomForestSRC')
rf <- rfsrc(t_slope_percent_change ~ 
              days_since_20_confirmed + 
              pop_log  + #date_numeric +
              pop_perc_male +
              pop_over65_perc + 
              pop_nonwhite_perc +
              #state_test_log +
              doctors_per_cap_log + 
              hospitals_log +
              popdensity_log +
              state_test_percap + #this is doing a ton of work
              Median_Household_Income_2018_log +
              area_log +
              google_workplace +
              google_grocery   +  google_residential + google_retail #+ google_park + google_transit
            , 
            ntree = 300 , 
            na.action='na.impute',
            data=xy_all_cross_section,
            importance="permute"
) #
rf #% variance explained: 19.6
vp <- vimp(rf)
plot(vp)

plot.variable(rf, partial = TRUE, smooth.lines = TRUE, sorted=T)

```




```{r}  
temp_df %>%
  filter(county %in% "Bexar") %>%
  ggplot() +
  geom_point(aes(x=date_asdate, y=confirmed_log)) +
  geom_line(aes(x=date_asdate, y=y_hat)) 

```

```{r}
#what should the cuttoff be?
temp_df %>% filter(confirmed<100) %>%
  ggplot() +
  geom_point(aes(x=confirmed, y=t_slope_percent_change)) 

#Where should we put the cutoff
temp_df %>% filter(confirmed<100) %>%
  ggplot(aes(x=confirmed , y=t_slope_percent_change)) +
  geom_point() +
  geom_smooth()

summary(temp_df$t_slope_percent_change[temp_df$confirmed>=10])


```

```{r}

temp_df_15 <- temp_df %>% filter(confirmed_max>10) %>% filter(days_since_15_confirmed>=1)

temp_df_15 %>% 
  ggplot(aes(x=days_since_15_confirmed , y=t_slope_percent_change)) +
  geom_point() +
  geom_smooth() + ylim(0,50)

```



```{r}
bing_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>% pull(death_perc_confirmed) %>% hist(breaks=100)
#Median of 3.7 deaths per 100
bing_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>% pull(death_perc_confirmed100) %>% summary()

bing_long$displayName

```

Without any limits we gets lots of weirdness on the low and high end. On the low end we're getting undercounting of confirmed cases. If you die, you get tested, but otherwise you're not getting reported and so the percentage of deaths is unrealistically high. On the high end the more people who are getting it, the greater share who are dying. Either because it's reaching further into communities and catching more vulnerbale people, or it's creating competition for limmitted medical resources, or both.

```{r}
bing_long %>% 
  #filter(totalDeaths>=2) %>%
  #filter(totalConfirmed>=200) %>%
  ggplot(aes(x=log(totalConfirmed),y=death_perc_confirmed100)) + geom_point() + ylim(0,50) + geom_smooth()
```

```{r}
bing_long %>% 
  filter(totalDeaths>=2) %>%
  filter(totalConfirmed>=200) %>%
  ggplot(aes(x=log(totalConfirmed),y=death_perc_confirmed100)) + geom_point() + ylim(0,25) + geom_smooth()

```



```{r}


  mutate(place=paste0(state,"_",county,"_",fips)) %>%
  group_by(place) %>%
  mutate(confirmed_cummax=cummax(confirmed)) %>%
  mutate(days_since_1_confirmed=cumsum(confirmed_cummax>=1)) %>%
  mutate(days_since_10_confirmed=cumsum(confirmed_cummax>=10)) %>%
  mutate(days_since_15_confirmed=cumsum(confirmed_cummax>=15)) %>%
  mutate(days_since_20_confirmed=cumsum(confirmed_cummax>=20)) %>%
  
  mutate(days_since_50_confirmed=cumsum(confirmed_cummax>=50)) %>%
  mutate(days_since_100_confirmed=cumsum(confirmed_cummax>=100)) %>%
  mutate(days_since_500_confirmed=cumsum(confirmed_cummax>=500)) %>%
  
  mutate(deaths_cummax=cummax(deaths)) %>%        
  mutate(days_since_1_deaths=cumsum(deaths_cummax>=1)) %>%
  mutate(days_since_10_deaths=cumsum(deaths_cummax>=10)) %>%
  mutate(days_since_50_deaths=cumsum(deaths_cummax>=50)) %>%
  mutate(days_since_100_deaths=cumsum(deaths_cummax>=100)) %>%
  mutate(days_since_500_deaths=cumsum(deaths_cummax>=500)) %>%
  
  mutate(confirmed_fd=confirmed-lag(confirmed)) %>%
  mutate(deaths_fd=deaths-lag(deaths)) %>%
  ungroup() %>%
  arrange(fips, date_asdate) %>%
  
  #filter(days_since_1_confirmed>0) %>%
  group_by(place) %>%
  mutate(confirmed_max=max(confirmed)) %>%
  ungroup() %>%
  
  mutate(deaths=ifelse(deaths==0, NA,deaths)) %>%
  mutate(confirmed=ifelse(confirmed==0, NA,confirmed)) %>%
  mutate(death_perc_confirmed= deaths / (confirmed )) %>%
  mutate(death_perc_confirmed100 = round( deaths / (confirmed/100 ),1 ) )


nytimes_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>% pull(death_perc_confirmed) %>% hist(breaks=100)
#Median of 40 deaths per 1k confirmed
nytimes_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>%  pull(death_perc_confirmed100) %>% summary()

temp <- nytimes_long %>% dplyr::select(place, confirmed, deaths, death_perc_confirmed100)

nytimes_cross <- nytimes_long %>%
  dplyr::select(state, county, fips, place, date_asdate, confirmed, deaths) %>% 
  group_by(place) %>%
  filter(date_asdate==max(date_asdate)) %>%
  ungroup() %>%
  mutate(deaths_perc_confirmed=deaths/confirmed)

temp <- nytimes_long %>% dplyr::select(fips, confirmed_max) %>% distinct()
table(temp$confirmed_max) #936 have 10 or more cases , 1412 don't

hist(nytimes_cross$deaths_perc_confirmed)

```

```{r}
%>%
                      mutate(confirmed_perc_tests= positive / (positive + negative)) %>%
                      mutate(death_perc_confirmed= death / (positive ))  %>%
                      mutate(death_perc_confirmed100 = round( death / (positive/100 ),1 ) ) %>%
                      arrange(state_name,date_asdate)

covidtracking_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>% pull(death_perc_confirmed) %>% hist(breaks=100)
#Median of 1.9 fatalities per thousand confirmed cases
covidtracking_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>% pull(death_perc_confirmed100) %>% summary()


covidtracking_cross <- covidtracking  %>% 
  
  #dplyr::select(state, county, fips, place, date_asdate, confirmed, deaths) %>% 
  group_by(state) %>%
  filter(date_asdate==max(date_asdate)) %>%
  ungroup()# %>%

```


