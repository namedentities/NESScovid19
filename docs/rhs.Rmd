---
title: "R Notebook"
output: html_notebook
---




```{r}

#some of these are screwups. LIke bexar county for bing is not montonic
library(tsibble); #install.packages('tsibble')
lhs_wide_qcode_monotonic <- lhs_wide_qcode %>% arrange(date_asdate) %>%
                            group_by(wikidata_id) %>% 
                              tidyr::fill( starts_with("confirmed|deaths|tested")) %>%
                              mutate_if(is.numeric, difference) %>%
                            ungroup() #I think the way to do this is on the long version. If any of the cumulative counts is a decrease then regeject the whole observation.

lhs_temp <- lhs_long %>% group_by(gid, geonameid, wikidata_id, date_asdate) %>% summarize(n=n())
lhs_temp %>% filter(n<=6) %>% pull(n) %>% quantile() #median of only a single data point

lhs_temp %>% filter(n<=6) %>% ggplot(aes(x=date_asdate,y=n)) + geom_point()

library(lfe) #install.packages('lfe'), quietly=TRUE
xy_all <- lhs_long %>% group_by(gid, geonameid, wikidata_id, date_asdate) %>% mutate(n=n()) %>% filter(n<=6) %>% ungroup() %>% 
             filter(!is.na(wikidata_id) & !is.na(date_asdate)) %>% 
             mutate(date_place=paste(wikidata_id,date_asdate))  %>% 
             dplyr::select(confirmed, dataset, date_asdate, wikidata_id, date_place) %>% arrange(date_place) %>% mutate_if(is.character, as.factor) %>%
             filter(confirmed!=0) 
dim(xy_all)
table(xy_all$dataset)
est <- felm( confirmed ~ 1 + dataset | date_asdate + wikidata_id + date_place, data=xy_all)
summary(est)

#lm1 <- lm(confirmed ~ factor(dataset) + factor(date_asdate) + factor(wikidata_id) , data=lhs_long)


```

# RHS

## Testing

### ourworldindata.org

```{r}

#https://drive.google.com/drive/folders/1HPzXon49teN-kMQlY1ry9rh2eq-TyGQI
#https://ourworldindata.org/covid-testing
#on how to reformat the url for straight downloading
#https://gist.github.com/tanaikech/f0f2d122e05bf5f971611258c22c110f
library(googledrive)
library(curl)
tmp <- tempfile()
#curl_download("https://drive.google.com/open?id=18QE3wScRXL-ARIaBl6yh7xUVNlnitvW_", tmp)


#There's also a github repo
#
worldindata_github <- read_csv("https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/testing/covid-testing-all-observations.csv") 

#curl_download("https://drive.google.com/uc?export=download&id=18QE3wScRXL-ARIaBl6yh7xUVNlnitvW_", tmp)
#worldindata_googledrive <- read_csv(tmp) 

dim(worldindata_github)
#dim(worldindata_googledrive) #The github one is longer

worldindata_long <- worldindata_github %>%
                    mutate(date_asdate = ymd(Date)) %>%  
                    separate(Entity, c("admin0_name_original", "note"), sep="-") %>%
                    mutate(admin0_name_original=trimws(admin0_name_original)) %>%
                    rename(tests=`Cumulative total`) %>%
                    mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                     mutate(admin1_name_clean='') %>%
                     mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
          
                    left_join(admin0 ) %>%
                    left_join(admin1 )  %>%
                    left_join(admin2 ) 

```


## Demographics

### U.S. County Demographics

```{r}

#library(R0)  # consider moving all library commands to top -- this one was in a loop below
#Until the U.S. states one goes live have to pull it from here
#Codebook
#https://github.com/JieYingWu/COVID-19_US_County-level_Summaries/blob/master/data/list_of_columns.md
counties <- read_csv(url("https://raw.githubusercontent.com/JieYingWu/COVID-19_US_County-level_Summaries/master/data/counties.csv")) #using the archived copy from today because they haven't posted the 
counties_t <- t(counties)

counties_1 <- counties %>%
                      filter(as.numeric(substring(FIPS,3,5))==0 & Area_Name!="United States") %>%
                      mutate(admin0_name_original="United States") %>%
                      mutate(admin1_name_original=Area_Name) 
counties_2 <- counties %>% filter(as.numeric(substring(FIPS,3,5))>0 & Area_Name!="United States") %>% 
                      left_join(state_codes %>% rename(State=state_abbr)) %>%
                      mutate(admin0_name_original="United States") %>%
                      mutate(admin1_name_original=state_name)  %>%
                      mutate(admin2_name_original=Area_Name) 

counties_long <- bind_rows(counties_1,counties_2) %>%
                    mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                    mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                    mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%

                      left_join(admin0 ) %>%
                      left_join(admin1 ) %>%
                      left_join(admin2 ) 



```

## 
The international political economy data resource
https://link.springer.com/article/10.1007/s11558-017-9285-0

```{r}

tmp2 <- tempfile()
temp_url <- "https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/X093TV/XHEWU3"
curl::curl_download(temp_url, tmp2)
load(tmp2) #the object's name is ipe_full
glimpse(ipe_full)
#$ country                     <chr> "United States of America", "United States of America", "United States of America", "United States of America", "United States of America", "United States of America", "United States of America", "United States of America", "United States of…
#$ year                        <dbl> 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1…
#$ gwno                        <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…
#$ ccode                       <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…
#$ ifscode                     <dbl> 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 1…
#$ ifs                         <chr> "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "USA", "US…
#$ gwabbrev 

ipe_long <- ipe_full %>% 
  group_by(country) %>% filter(year==max(year)) %>% ungroup()
#the whole dataset ends in 2017 and a lot of these values aren't available that recently which is interesting

#codebook
#https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/X093TV/OIB5GS&version=3.0

ipe_long <- ipe_full %>%
            mutate_if(is.numeric,as.character, is.factor, as.character) %>% #https://stackoverflow.com/questions/58124530/pivot-longer-with-multiple-classes-causes-error-no-common-type
            pivot_longer( names_to = "variable", cols = onset2_AO:english_off_USA_CE, values_to = "value", values_ptypes = list(val = 'character')) %>% filter(!is.na(value))  %>% 
            group_by(country, variable) %>% 
            filter(year==max(year)) %>% 
            ungroup()  %>%
            mutate(admin0_name_clean=country %>% rex_clean()) %>%
            mutate(admin1_name_clean='') %>%
            mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
  
            left_join(admin0 ) %>%
            left_join(admin1 )  %>%
            left_join(admin2 ) 

```


```{r}
#World Development Indicators
#http://datatopics.worldbank.org/world-development-indicators/
#csv bulk zip download
#http://databank.worldbank.org/data/download/WDI_csv.zip


WDIData <- read_csv("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/WDIData.csv")

WDIData_long <- WDIData %>%
                mutate_if(is.numeric,as.character, is.factor, as.character) %>% #https://stackoverflow.com/questions/58124530/pivot-longer-with-multiple-classes-causes-error-no-common-type
                pivot_longer( names_to = "year", cols = `1960`:X65, values_to = "value", values_ptypes = list(val = 'character')) %>% filter(!is.na(value))  %>% 
                group_by(`Country Name`, `Indicator Name`) %>% 
                filter(year==max(year)) %>% 
                ungroup() 

colnames(WDIData_long) <- colnames(WDIData_long) %>% str_replace_all(" ","_") %>% tolower()

WDIData_long <- WDIData_long %>%
                mutate(indicator_name=indicator_name%>% str_replace_all(" ","_") %>% tolower()) %>%
                rename(admin0_name_original=country_name) %>%
                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                 mutate(admin1_name_clean='') %>%
                 mutate(admin2_name_clean='') %>% #you have to remember to put NAs or you'll accidentally over join
      
                left_join(admin0 ) %>%
                left_join(admin1 )  %>%
                left_join(admin2 ) 


temp <- WDIData_long %>% count(indicator_name)

```



## Mobility

### Google Mobility Data



```{r}
#Here's the new one straight csv direct form them
#https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv

#Google mobility daily data from the trend lines
#https://github.com/nacnudus/google-location-coronavirus
#only at the country level though
google_mobility_timelines <- read_tsv("https://raw.githubusercontent.com/nacnudus/google-location-coronavirus/master/2020-03-29.tsv")

```

```{r}

#Mobility datga
google_mobility_us <- read_csv(url("https://raw.githubusercontent.com/ActiveConclusion/COVID19_mobility/master/mobility_report_US.csv")) #
google_mobility_regions <- read_csv(url("https://raw.githubusercontent.com/ActiveConclusion/COVID19_mobility/master/mobility_report_regions.csv")) #

google_mobility_us_long <- google_mobility_us %>%
                   mutate(date_asdate = ymd(Date)) %>%
                    mutate(admin0_name_original="United States") %>%
                    mutate(admin1_name_original=State)  %>%
                    mutate(admin2_name_original=Region) %>%
                    mutate(admin2_name_original=ifelse(admin2_name_original=="Total",NA, admin2_name_original))

google_mobility_regions_long <- google_mobility_regions %>%
                          mutate(date_asdate = ymd(Date)) %>%
                          mutate(admin0_name_original=Country) %>%
                          mutate(admin1_name_original=Region)  %>%
                          mutate(admin1_name_original=ifelse(admin1_name_original=="Total",NA, admin1_name_original))  

google_mobility_long <- 
                  bind_rows(google_mobility_us_long,
                            google_mobility_regions_long) %>%
                  rename(google_retail= `Retail & recreation`,
                         google_grocery=`Grocery & pharmacy`,
                         google_park= Parks,
                         google_transit=`Transit stations`,
                         google_workplace=Workplaces,
                         google_residential=Residential) %>% 
                  distinct() %>%
                mutate(admin0_name_clean=admin0_name_original %>% rex_clean()) %>%
                mutate(admin1_name_clean=admin1_name_original %>% rex_clean()) %>%
                mutate(admin2_name_clean=admin2_name_original %>% rex_clean()) %>%
                left_join(admin0 ) %>%
                left_join(admin1 )  %>%
                left_join(admin2 )   

```


NYT state order issued

```{r}
#https://www.nytimes.com/interactive/2020/us/coronavirus-stay-at-home-order.html
```



What areas have more or fewer cases/deaths than we would expect?


```{r}

#has new york city as a single entitiy but not the constituent counties which is frustrating

library(strucchange) ; #install.packages('strucchange')
temp <- nytimes_long %>% arrange(date_asdate) %>% filter(county %in% "Bexar") %>% 
  mutate(confirmed_log=log(confirmed+1)) %>%
  mutate(date_rank= rank(date_asdate))  
bp <- breakpoints(confirmed_log ~ 1, data=temp)
bp <- breakpoints(confirmed_log ~ 1 + date_rank, data=temp)
temp %>% ggplot() + geom_point(aes(x=rank(date_asdate), y=confirmed_log)) + geom_vline(xintercept=bp$breakpoints)
coef(bp)


temp <- nytimes_long %>% arrange(date_asdate) %>% filter(county %in% "New York City") %>% 
  mutate(confirmed_log=log(confirmed+1)) %>%
  mutate(t= rank(date_asdate))  
bp <- breakpoints(confirmed_log ~ 1, data=temp)
bp <- breakpoints(confirmed_log ~ 1 + t, data=temp)

tmin <- min(temp$t)
tmax <- max(temp$t)
c(tmin, bp$breakpoints, tmax)


cdf <- data.frame(
  t= c(1,bp$breakpoints), 
  t_slope= coef(bp)[,2]
)

temp <- temp %>% 
  mutate(y_hat = fitted.values(bp)) %>%
  left_join(cdf) %>% 
  fill(t_slope) %>%
  mutate(t_slope_percent_change = round((exp(t_slope)-1)*100,2))

temp %>% ggplot() +
  geom_point(aes(x=rank(date_asdate), y=confirmed_log)) + 
  geom_line(aes(x=rank(date_asdate), y=y_hat)) + 
  geom_vline(xintercept=bp$breakpoints)
```

```{r}

temp_list <- list()
for(q in places$place){
  print(q)
  temp <- NULL
  temp <- nytimes_long %>% 
    arrange(date_asdate) %>% 
    filter(place %in% q) %>% 
    mutate(confirmed_log=log(confirmed+1)) %>%
    mutate(t= rank(date_asdate))  
  if( nrow(temp)==0 ) {print("error"); break}
  
  #bp <- breakpoints(confirmed_log ~ 1, data=temp)
  bp <- NULL
  lm1 <- NULL
  y_hat <- NA
  cdf <- NULL
  try({
    #if it fails fall back to just a lm
    lm1 <- lm(confirmed_log ~ 1 + t, data=temp)
    
    cdf <- data.frame(
      t= 1, 
      t_slope= coef(lm1)[2],
      t_slope_break=0
    )
    y_hat=fitted.values(lm1)
  })
  
  try({
    bp <- breakpoints(confirmed_log ~ 1 + t, data=temp)
    
    cdf <- data.frame(
      t= c(1,bp$breakpoints), 
      t_slope= coef(bp)[,2],
      t_slope_break=1
    )
    y_hat=fitted.values(bp)
  })
  
  try({
    temp <- temp %>% 
      mutate(y_hat = y_hat) %>%
      left_join(cdf) %>% 
      fill(t_slope) %>%
      mutate(t_slope_percent_change = round((exp(t_slope)-1)*100,2))
    
    temp_list[[as.character(q)]] <- temp
  })
  #if( is.na( temp_list[[as.character(q)]]$y_hat) ) {print("error"); break}
}
#"13055"
temp_df <- bind_rows(temp_list)
dim(temp_df)

temp_list[["New York_New York City_NA"]]


```

```{r}

temp_df_max <- temp_df %>% group_by(fips) %>% filter(date_asdate==max(date_asdate)) %>% ungroup()

hist(temp_df_max$t_slope_percent_change, breaks=50)

temp_df_max %>% ggplot() + geom_density(aes(x=t_slope_percent_change))

```

```{r}
temp_df %>% 
  head(1000) %>%
  ggplot() +
  geom_line(aes(x=date_asdate,
                y=t_slope_percent_change,
                color=fips))  + theme(legend.position = "none")

```

```{r}
temp_df %>%
  filter(place %in% "New York_New York City_NA")  %>% 
  mutate(date_asdate_rank=rank(date_asdate)) %>%
  ggplot() +
  geom_point(aes(x=date_asdate,y=t_slope_percent_change))


temp_df %>%
  filter(place %in% "New York_New York City_NA")  %>% 
  ggplot() +
  geom_point(aes(x=date_asdate,y=confirmed_log)) +
  geom_line(aes(x=date_asdate,y=y_hat)) +
  #geom_point(aes(x=date_asdate,y=t_slope_percent_change))
  
  temp_df %>%
  filter(place %in% "Texas_Bexar_48029")  %>% 
  ggplot() +
  geom_point(aes(x=date_asdate,y=confirmed_log)) +
  geom_line(aes(x=date_asdate,y=y_hat))

library(gghighlight)
temp_df %>%
  filter(place %in% c("New York_New York City_NA","Texas_Bexar_48029","California_San Diego_06073","Florida_St. Lucie_12111") ) %>% 
  ggplot(aes(x=date_asdate, color=place)) +
  geom_point(aes(y=confirmed_log)) +
  geom_line(aes(y=y_hat)) +
  gghighlight(#prefered_label %in% c("Washington, US","Italy","China") , #"New York, US" #, "US"
    label_params =
      list(
        size = 3,
        segment.alpha=0)
  ) +
  theme_bw()

library(gghighlight)
temp_df %>%
  filter(t_slope_percent_change>1 & confirmed>20 ) %>% 
  mutate(y_hat_exp=exp(y_hat) ) %>% 
  
  ggplot(aes(x=date_asdate, color=place)) +
  geom_point(aes(y=confirmed_log)) +
  geom_line(aes(y=y_hat)) +
  gghighlight(place %in% c("New York_New York City_NA","Texas_Bexar_48029","California_San Diego_06073","Florida_St. Lucie_12111") , 
              label_params =
                list(
                  size = 3,
                  segment.alpha=0)
  ) +
  theme_bw() + ylim(3,11.5)

library(scales)
library(gghighlight)
temp_df %>%
  filter(t_slope_percent_change>1 & confirmed>20 ) %>% 
  mutate(y_hat_exp=exp(y_hat) ) %>% 
  
  ggplot(aes(x=date_asdate, color=place)) +
  geom_point(aes(y=confirmed)) +
  geom_line(aes(y=y_hat_exp)) +
  gghighlight(place %in% c("New York_New York City_NA","Texas_Bexar_48029","California_San Diego_06073","Florida_St. Lucie_12111") , 
              label_params =
                list(
                  size = 3,
                  segment.alpha=0)
  ) +
  theme_bw() + scale_y_log10(labels = comma_format()) #+ ylim(10, 100000)


temp_df %>%
  filter(t_slope_percent_change>1 & confirmed>=20 ) %>% 
  mutate(y_hat_exp=exp(y_hat) ) %>% 
  
  ggplot(aes(x=days_since_20_confirmed, color=place)) +
  geom_point(aes(y=confirmed)) +
  geom_line(aes(y=y_hat_exp)) +
  gghighlight(place %in% c("New York_New York City_NA","Texas_Bexar_48029","California_San Diego_06073","Florida_St. Lucie_12111") , 
              label_params =
                list(
                  size = 3,
                  segment.alpha=0)
  ) +
  theme_bw() + scale_y_log10(labels = comma_format()) #+ ylim(10, 100000)




```

Ok so we put slope at days since 20 confirmed on the left hand side, and put other covariates on the rhs


```{r}


lhs_20 <- temp_df %>% 
  filter(t_slope_percent_change>1 & confirmed>=20 ) %>% 
  mutate(y_hat_exp=exp(y_hat) ) %>%
  left_join( state_codes %>% dplyr::select(state=state_name, state_abbr ) )
dim(lhs_20)

dim(covidtracking)

contemporary_us  <- us_counties()
rhs_mobility <- mobility %>% 
  mutate(Region=str_replace(Region," County","")) %>%
  left_join(contemporary_us %>% 
              mutate(fips=paste0(statefp , countyfp))  %>% as.data.frame() %>% 
              dplyr::select(fips, State=state_name , Region=name  ) 
  ) %>%
  rename(google_retail= `Retail & recreation`,
         google_grocery=`Grocery & pharmacy`,
         google_park= Parks,
         google_transit=`Transit stations`,
         google_workplace=Workplaces,
         google_residential=Residential) %>% 
  distinct()


rhs_covidtracking <- covidtracking %>%
  mutate(date_asdate = ymd(date)) %>% 
  dplyr::select(date_asdate, state_abbr=state, state_test_positive=positive, state_test_negative=negative) %>%
  mutate( state_test = state_test_positive + state_test_negative) %>% 
  left_join(statepop  %>% dplyr::select(state_abbr=abbr, pop_2015) ) %>%
  mutate( state_test_percap = state_test/pop_2015) %>% 
  distinct()



rhs_counties <- counties %>% dplyr::select(fips=FIPS, 
                                    TOT_MALE, TOT_FEMALE,
                                    doctors_per_cap=`Active Physicians per 100000 Population 2018 (AAMC)`,
                                    popdensity=`Density per square mile of land area - Population`,
                                    area=`Area in square miles - Total area`,
                                    Median_Household_Income_2018,
                                    Total_age65plus,
                                    WA_MALE,	#White alone male population
                                    WA_FEMALE,	#White alone female population
                                    hospitals=`Total Hospitals (2019)`
) %>% 
  mutate(pop=TOT_MALE+TOT_FEMALE) %>% mutate(pop_perc_male = TOT_MALE/pop) %>% 
  mutate(pop_over65_perc=Total_age65plus/pop) %>%
  mutate(pop_nonwhite_perc= (pop-(WA_MALE+WA_FEMALE))/pop  ) %>%
  
  distinct()

xy_all <- lhs_20 %>% filter(county!="Unknown") %>%
  mutate(date_numeric=as.numeric(date_asdate)) %>% 
  left_join(rhs_covidtracking)  %>% 
  left_join(rhs_counties) %>% 
  left_join(rhs_mobility)
dim(xy_all)
glimpse(xy_all)

xy_all_cross_section <- xy_all %>% group_by(place) %>% filter(date_numeric==max(date_numeric)) %>% filter(!is.na(fips))  %>% ungroup() %>% as.data.frame()
dim(xy_all_cross_section)

xy_all_cross_section$pop_log <- log(xy_all_cross_section$pop)
xy_all_cross_section$popdensity_log <- log(xy_all_cross_section$popdensity)
xy_all_cross_section$state_test_log <- log(xy_all_cross_section$state_test)
xy_all_cross_section$area_log <- log(xy_all_cross_section$area)
xy_all_cross_section$Median_Household_Income_2018_log <- log(xy_all_cross_section$Median_Household_Income_2018)
xy_all_cross_section$doctors_per_cap_log <- log(xy_all_cross_section$doctors_per_cap)
xy_all_cross_section$hospitals_log <- log(xy_all_cross_section$hospitals+1)

```

```{r}

temp <- xy_all_cross_section %>%
  dplyr::select(place,state, county, t_slope_percent_change,days_since_20_confirmed,pop_log,popdensity_log,state_test_percap,google_workplace,Median_Household_Income_2018,area_log,pop_over65_perc,hospitals_log) %>% 
  arrange(place)

library(GGally)
xy_all_cross_section %>%
  dplyr::select(t_slope_percent_change,days_since_20_confirmed,pop_log,popdensity_log,state_test_percap,google_workplace,Median_Household_Income_2018_log,area_log,hospitals) %>%
  ggpairs(title = "Within Psychological Variables")

```

```{r}


library(randomForestSRC); #install.packages('randomForestSRC')
rf <- rfsrc(t_slope_percent_change ~ 
              days_since_20_confirmed + 
              pop_log  + #date_numeric +
              pop_perc_male +
              pop_over65_perc + 
              pop_nonwhite_perc +
              #state_test_log +
              doctors_per_cap_log + 
              hospitals_log +
              popdensity_log +
              state_test_percap + #this is doing a ton of work
              Median_Household_Income_2018_log +
              area_log +
              google_workplace +
              google_grocery   +  google_residential + google_retail #+ google_park + google_transit
            , 
            ntree = 300 , 
            na.action='na.impute',
            data=xy_all_cross_section,
            importance="permute"
) #
rf #% variance explained: 19.6
vp <- vimp(rf)
plot(vp)

plot.variable(rf, partial = TRUE, smooth.lines = TRUE, sorted=T)

```




```{r}  
temp_df %>%
  filter(county %in% "Bexar") %>%
  ggplot() +
  geom_point(aes(x=date_asdate, y=confirmed_log)) +
  geom_line(aes(x=date_asdate, y=y_hat)) 

```

```{r}
#what should the cuttoff be?
temp_df %>% filter(confirmed<100) %>%
  ggplot() +
  geom_point(aes(x=confirmed, y=t_slope_percent_change)) 

#Where should we put the cutoff
temp_df %>% filter(confirmed<100) %>%
  ggplot(aes(x=confirmed , y=t_slope_percent_change)) +
  geom_point() +
  geom_smooth()

summary(temp_df$t_slope_percent_change[temp_df$confirmed>=10])


```

```{r}

temp_df_15 <- temp_df %>% filter(confirmed_max>10) %>% filter(days_since_15_confirmed>=1)

temp_df_15 %>% 
  ggplot(aes(x=days_since_15_confirmed , y=t_slope_percent_change)) +
  geom_point() +
  geom_smooth() + ylim(0,50)

```



```{r}
bing_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>% pull(death_perc_confirmed) %>% hist(breaks=100)
#Median of 3.7 deaths per 100
bing_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>% pull(death_perc_confirmed100) %>% summary()

bing_long$displayName

```

Without any limits we gets lots of weirdness on the low and high end. On the low end we're getting undercounting of confirmed cases. If you die, you get tested, but otherwise you're not getting reported and so the percentage of deaths is unrealistically high. On the high end the more people who are getting it, the greater share who are dying. Either because it's reaching further into communities and catching more vulnerbale people, or it's creating competition for limmitted medical resources, or both.

```{r}
bing_long %>% 
  #filter(totalDeaths>=2) %>%
  #filter(totalConfirmed>=200) %>%
  ggplot(aes(x=log(totalConfirmed),y=death_perc_confirmed100)) + geom_point() + ylim(0,50) + geom_smooth()
```

```{r}
bing_long %>% 
  filter(totalDeaths>=2) %>%
  filter(totalConfirmed>=200) %>%
  ggplot(aes(x=log(totalConfirmed),y=death_perc_confirmed100)) + geom_point() + ylim(0,25) + geom_smooth()

```



```{r}


  mutate(place=paste0(state,"_",county,"_",fips)) %>%
  group_by(place) %>%
  mutate(confirmed_cummax=cummax(confirmed)) %>%
  mutate(days_since_1_confirmed=cumsum(confirmed_cummax>=1)) %>%
  mutate(days_since_10_confirmed=cumsum(confirmed_cummax>=10)) %>%
  mutate(days_since_15_confirmed=cumsum(confirmed_cummax>=15)) %>%
  mutate(days_since_20_confirmed=cumsum(confirmed_cummax>=20)) %>%
  
  mutate(days_since_50_confirmed=cumsum(confirmed_cummax>=50)) %>%
  mutate(days_since_100_confirmed=cumsum(confirmed_cummax>=100)) %>%
  mutate(days_since_500_confirmed=cumsum(confirmed_cummax>=500)) %>%
  
  mutate(deaths_cummax=cummax(deaths)) %>%        
  mutate(days_since_1_deaths=cumsum(deaths_cummax>=1)) %>%
  mutate(days_since_10_deaths=cumsum(deaths_cummax>=10)) %>%
  mutate(days_since_50_deaths=cumsum(deaths_cummax>=50)) %>%
  mutate(days_since_100_deaths=cumsum(deaths_cummax>=100)) %>%
  mutate(days_since_500_deaths=cumsum(deaths_cummax>=500)) %>%
  
  mutate(confirmed_fd=confirmed-lag(confirmed)) %>%
  mutate(deaths_fd=deaths-lag(deaths)) %>%
  ungroup() %>%
  arrange(fips, date_asdate) %>%
  
  #filter(days_since_1_confirmed>0) %>%
  group_by(place) %>%
  mutate(confirmed_max=max(confirmed)) %>%
  ungroup() %>%
  
  mutate(deaths=ifelse(deaths==0, NA,deaths)) %>%
  mutate(confirmed=ifelse(confirmed==0, NA,confirmed)) %>%
  mutate(death_perc_confirmed= deaths / (confirmed )) %>%
  mutate(death_perc_confirmed100 = round( deaths / (confirmed/100 ),1 ) )


nytimes_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>% pull(death_perc_confirmed) %>% hist(breaks=100)
#Median of 40 deaths per 1k confirmed
nytimes_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>%  pull(death_perc_confirmed100) %>% summary()

temp <- nytimes_long %>% dplyr::select(place, confirmed, deaths, death_perc_confirmed100)

nytimes_cross <- nytimes_long %>%
  dplyr::select(state, county, fips, place, date_asdate, confirmed, deaths) %>% 
  group_by(place) %>%
  filter(date_asdate==max(date_asdate)) %>%
  ungroup() %>%
  mutate(deaths_perc_confirmed=deaths/confirmed)

temp <- nytimes_long %>% dplyr::select(fips, confirmed_max) %>% distinct()
table(temp$confirmed_max) #936 have 10 or more cases , 1412 don't

hist(nytimes_cross$deaths_perc_confirmed)

```

```{r}
%>%
                      mutate(confirmed_perc_tests= positive / (positive + negative)) %>%
                      mutate(death_perc_confirmed= death / (positive ))  %>%
                      mutate(death_perc_confirmed100 = round( death / (positive/100 ),1 ) ) %>%
                      arrange(state_name,date_asdate)

covidtracking_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>% pull(death_perc_confirmed) %>% hist(breaks=100)
#Median of 1.9 fatalities per thousand confirmed cases
covidtracking_long %>% filter(!is.na(death_perc_confirmed) & death_perc_confirmed!=0) %>% pull(death_perc_confirmed100) %>% summary()


covidtracking_cross <- covidtracking  %>% 
  
  #dplyr::select(state, county, fips, place, date_asdate, confirmed, deaths) %>% 
  group_by(state) %>%
  filter(date_asdate==max(date_asdate)) %>%
  ungroup()# %>%

```
